# DALL·E 1

- Paper: https://arxiv.org/abs/2102.12092
- Blog post: https://openai.com/blog/dall-e/
- Code: https://github.com/openai/dall-e
- Model: Not available
- Alternative code (PyTorch): https://github.com/lucidrains/DALLE-pytorch
- Alternative code (JAX/Flax): https://github.com/borisdayma/dalle-mini

The DALL·E 1 paper proposes a simple approach for text-to-image generation using a transformer model that autoregressively models text and image tokens as a single stream of data. Unlike traditional approaches that require complex architectures and auxiliary losses, DALL·E 1's approach relies on data and scale to be competitive with previous domain-specific models in zero-shot evaluations. This approach eliminates the need for training on a fixed dataset and enables generating images from previously unseen text descriptions.

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1.png
---
width: 70%
name: fig-dalle1
---
DALL·E 1
```

The initial iteration of DALL·E employed a transformer decoder similar to GPT-3 that generated 256×256 images based on text input and an optional image start. Text inputs were encoded as BPE-tokens with a maximum length of 256, while images were encoded using special image tokens generated by a discrete variational autoencoder (dVAE). The dVAE encoded 256×256 images into a 32×32 token grid with an 8192-word vocabulary. However, due to the dVAE's encoding, some fine details and high-frequency features were lost, leading to a characteristic smoothness and blurriness in DALL·E-generated images.

- DALL·E's initial version is a transformer decoder that generates a 256×256 image from text input and an optional beginning of the image.
- Text is encoded by BPE-tokens with a maximum of 256, while images are encoded by special image tokens with 1024 options, using a discrete variational autoencoder (dVAE).
- The dVAE encodes 256×256 images into a grid of 32×32 tokens with an 8192 possible value vocabulary.
- Generated images by DALL·E may have blurriness and smoothness due to the loss of high-frequency features and details resulting from the dVAE encoding process.

## DALL·E 1 Charateristics

### Controlling Attributes

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_attributes.png
---
width: 70%
name: fig-dalle1-attributes
---
Controlling Attributes
```

### Drawing Multiple Objects

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_objects.png
---
width: 70%
name: fig-dalle1-objects
---
Drawing Multiple Objects
```

### Visualizing Perspective and Three-Dimensionality

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_perspective.png
---
width: 70%
name: fig-dalle1-perspective
---
Visualizing Perspective and Three-Dimensionality
```

### Visualizing Internal and External Structure

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_structure.png
---
width: 70%
name: fig-dalle1-structure
---
Visualizing Internal and External Structure
```

### Inferring Contextual Details

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_context.png
---
width: 70%
name: fig-dalle1-context
---
Inferring Contextual Details
```

## DALL·E 1 Architecture

- The transformer is a large model with 12B parameters.
- It consisted of 64 sparse transformer blocks with a complicated set of attention mechanisms inside, consisting of
  - classical text-to-text masked attention,
  - image-to-text attention, and
  - image-to-image sparse attention.
- All three attention types are merged into a single attention operation.
- The model was trained on a dataset of 250M image-text pairs.

### VQ-VAE

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_vqvae.png
---
width: 70%
name: fig-dalle1-vqvae
---
VQ-VAE
```

- VQ-VAE is a type of variational autoencoder that uses vector quantization to obtain a discrete latent representation.
- This is in contrast to the continuous latent space that other variational autoencoders have.
- The objective function of a VQ-VAE, when trained on an image dataset, can be written as:

  $\mathcal{L} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - \beta \cdot D_{KL}[q(z|x) || p(z)]$

  where $p(x)$ is the data distribution, $q$ is the approximate posterior over latent variables and $D_{KL}$ denotes the Kullback-Leibler divergence.

- This objective function encourages the model to learn an efficient codebook that minimises reconstruction error while also matching the prior distribution over codes.

**A latent space**

A latent space is obtained by encoding the input image into the nearest codebook entry. This process is called vector quantization and results in a discrete latent space.

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_latent.png
---
width: 70%
name: fig-dalle1-latent
---
A latent space
```

### Autoencoders

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_autoencoder.png
---
width: 70%
name: fig-dalle1-autoencoder
---
Autoencoders
```

- Autoencoder is a neural network that is trained to predict its input.
- The objective function of an autoencoder can be written as:

  $\mathcal{L} = \mathbb{E}_{p(x)}[\log p(x|z)]$

  where $p(x)$ is the data distribution and $p(x|z)$ is the model distribution.

- This objective function encourages the model to learn a latent space that captures the underlying structure of the data.

- A VQ-VAE can be seen as a type of autoencoder where the latent space is constrained to be discrete.

#### Variational Autoencoders (VAE)

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_vae.png
---
width: 70%
name: fig-dalle1-vae
---
Variational Autoencoders (VAE)
```

- Variational Autoencoders (VAE) is a type of autoencoder where the latent space is continuous.
- The objective function of a VAE can be written as:

  $\mathcal{L} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}[q(z|x) || p(z)]$

  where $p(x)$ is the data distribution, $q$ is the approximate posterior over latent variables and $D_{KL}$ denotes the Kullback-Leibler divergence.

- This objective function encourages the model to learn a latent space that captures the underlying structure of the data while also matching the prior distribution over latent variables.

### Discrete Spaces

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_discrete.png
---
width: 70%
name: fig-dalle1-discrete
---
Discrete Spaces
```

- Discrete spaces are more efficient to represent than continuous spaces.
- This is because a discrete space can be represented with a finite number of bits, whereas a continuous space requires an infinite number of bits.
- In addition, discrete spaces are easier to manipulate and reason about than continuous spaces.
- For these reasons, VQ-VAE is more efficient than VAE at learning latent representations of data.

### Uncertainty in the Posterior

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_uncetainty1.png
---
width: 70%
name: fig-dalle1-uncetainty1
---
Uncertainty in the Posterior
```

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_uncetainty2.png
---
width: 70%
name: fig-dalle1-uncetainty2
---
Uncertainty in the Posterior
```

- Uncertainty in the posterior is added by soft-sampling codebook vectors from the Gumbel-Softmax distribution.
- This results in a softened latent space which can be seen as a continuous approximation of the discrete latent space.

- The Gumbel-Softmax distribution is a type of distribution that allows for sampling from a discrete space while still allowing for gradients to flow through the samples.
- This is useful for training models with discrete latent spaces, such as VQ-VAE.
- The Gumbel-Softmax distribution is defined as:

  $G(z;\mu,\beta) = \frac{\exp((z - \mu)/\beta)}{\sum_{k=1}^K \exp((z_k - \mu)/\beta)}$

  where $\mu$ is the mean, $\beta$ is the temperature and $K$ is the number of classes.

**Comparison of original images (top) and reconstructions from the dVAE (bottom)**

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_dvae.png
---
width: 70%
name: fig-dalle1-dvae
---
Comparison of original images (top) and reconstructions from the dVAE (bottom)
```

### Decoder

A GPT-3 like transformer decoder consumes a sequence of text tokens and (optional) image tokens (here a single image token with id 42) and produces a continuation of an image (here the next image token with id 1369)

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_decoder.png
---
width: 70%
name: fig-dalle1-decoder
---
Decoder
```

### Sampling From a Trained DALL-E

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_sampling.png
---
width: 70%
name: fig-dalle1-sampling
---
Sampling From a Trained DALL-E
```

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_sampling2.png
---
width: 70%
name: fig-dalle1-sampling2
---
Sampling From a Trained DALL-E
```

## DALL·E 1 Results

Several image generation examples from the original paper

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_examples.png
---
width: 70%
name: fig-dalle1-examples
---
Several image generation examples from the original paper
```

The trained model generated several samples (up to 512!) based on the text provided, then all these samples were ranked by a special model called CLIP, and the top-ranked one was chosen as the result of the model.

```{figure} ../figs/aiart/dalle1/aiart_1_dalle1_eval.png
---
width: 70%
name: fig-dalle1-eval
---
Evaluation
```
