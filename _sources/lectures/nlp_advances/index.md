# Advances in AI and NLP

## Course Description:

This course provides an in-depth exploration of recent advances in artificial intelligence and natural language processing, with a focus on influential research papers published after 2016. Students will read and analyze seminal papers on topics such as neural networks, sequence modeling, generative models, and natural language processing. The course will emphasize critical reading and thinking skills, as well as practical implementation skills, through a series of presentations and discussion sessions.

## Learning Objectives:

Upon completing this course, students will be able to:

- Understand and critique the main concepts, techniques, and architectures in artificial intelligence and natural language processing.
- Analyze and apply the latest research in artificial intelligence and natural language processing to real-world problems.
- Develop strong written and oral communication skills through presentations and group discussions.

## Prerequisites:

- Familiarity with programming (preferably Python)
- Basic knowledge of machine learning and deep learning
- Basic knowledge of natural language processing

## Course Outline:

The following is a tentative outline of the topics to be covered in this course. The order of the topics may be adjusted as needed.

### Week 1: Introduction to AI and Natural Language Processing

- Paper: "Sequence to Sequence Learning with Neural Networks" {cite}`sutskever2014sequence`
- Discussion: Introduction to the course, review of prerequisites, and overview of the paper.

### Week 2: Attention Mechanisms in Neural Networks

- Paper: "Attention Is All You Need" {cite}`vaswani2017attention`
- Discussion: Attention mechanisms in neural networks, pros and cons, and applications in natural language processing.

### Week 3: Generative Models and Adversarial Training

- Paper: "Generative Adversarial Networks" {cite}`goodfellow2020generative`
- Discussion: Generative models, their applications in natural language processing, and adversarial training.

### Week 4: Pretrained Language Models

- Paper: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" {cite}`devlin2018bert`
- Discussion: Pretrained language models, their applications in natural language processing, and comparison of different models.

### Week 5: Recent Advances in Transformer Models

- Paper: "The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives" {cite}`voita2019bottom`
- Discussion: Recent advances in Transformer models, their applications in natural language processing, and comparison of different models.

### Week 6: Attention in Vision

- Paper: "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention" {cite}`xu2015show`
- Discussion: Attention mechanisms in vision, pros and cons, and applications in image captioning.

### Week 7: Knowledge Graphs

- Paper: "Knowledge Graphs" {cite}`hogan2021knowledge`
- Discussion: Introduction to knowledge graphs, their applications in natural language processing, and current approaches to knowledge embedding.

### Week 8: Midterm

### Week 9: Language Generation and Dialogue Systems

- Paper: "A Persona-Based Neural Conversation Model" {cite}`li2016persona`
- Discussion: Approaches to language generation and dialogue systems, their applications in natural language processing, and comparison of different models.

### Week 10: Multimodal Learning

- Paper: "Attention-based Multimodal Neural Machine Translation" {cite}`huang2016attention`
- Discussion: Multimodal learning, their applications in natural language processing, and comparison of different models.

### Week 11: Common Sense and Reasoning

- Paper: "ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning" {cite}`sap2019atomic`
- Discussion: Introduction to common sense and reasoning, their applications in natural language processing, and current approaches to real-world visual reasoning and question answering.

### Week 12: Adapting AI Models to Different Tasks

- Paper: "Meta-Learning for Few-Shot Natural Language Processing" {cite}`yin2020meta`
- Discussion: Approaches to adapting AI models to different tasks, their applications in natural language

### Week 13: Few-Shot Learning with Prompt-Based Methods

- Paper: "The Power of Scale for Parameter-Efficient Prompt Tuning" {cite}`lester2021power`
- Discussion: Introduction to prompt-based methods, their applications in natural language processing, and comparison of different models.

### Week 14: Ethical and Social Implications of AI and NLP

- Paper: "Towards a Code of Ethics for Artificial Intelligence" {cite}`boddington2017towards`
- Discussion: Ethical and social implications of AI and NLP, including bias, privacy, and accountability.

### Week 15: Reserved for Make-up classes

### Week 16: Final Project Presentations

## Grading:

- Class participation and attendance: 20%
- Presentations and group discussions: 30%
- Final project: 50%
