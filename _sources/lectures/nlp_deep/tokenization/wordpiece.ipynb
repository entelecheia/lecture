{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b31328f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# WordPiece Step-by-Step Implementation\n",
    "\n",
    "In this lecture, we will walk you through the process of implementing WordPiece tokenization, a subword tokenization algorithm used in many state-of-the-art models like BERT, DistilBERT, and RoBERTa. We will use a dataset of financial news headlines for this purpose.\n",
    "\n",
    "## Dataset Preparation\n",
    "\n",
    "First, we need to load our dataset. We will use the `ashraq/financial-news` dataset from the Hugging Face Hub. We will use the `headline` column as our text data. We will randomly sample 1000 records from this dataset for our tokenization process. Here is how we can do this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456bca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset parquet (/home/yjlee/.cache/huggingface/datasets/ashraq___parquet/ashraq--financial-news-89d6ac597a40e29e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 53.96it/s]\n",
      "Loading cached shuffled indices for dataset at /home/yjlee/.cache/huggingface/datasets/ashraq___parquet/ashraq--financial-news-89d6ac597a40e29e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-79cde8905e45a47f.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ashraq/financial-news\")\n",
    "texts = dataset[\"train\"].shuffle(seed=1234).select(range(1000))[\"headline\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18f0b360",
   "metadata": {},
   "source": [
    "## Pre-tokenization\n",
    "\n",
    "Before we start with the WordPiece tokenization, we need to pre-tokenize our text. Pre-tokenization involves splitting the text into words. This is a necessary step because WordPiece operates on the word level. Here is a simple function to pre-tokenize the text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45502764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def pre_tokenize(text, lowercase=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.split(\" \")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "513d73f2",
   "metadata": {},
   "source": [
    "## Vocabulary Initialization\n",
    "\n",
    "The next step is to initialize our vocabulary. In the case of WordPiece, we start with a vocabulary of individual characters. Here is how we can do this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14a39e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3636\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def initialize_vocab(texts, lowercase=True):\n",
    "    vocab = defaultdict(int)\n",
    "    for text in texts:\n",
    "        words = pre_tokenize(text, lowercase)\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "word_freqs = initialize_vocab(texts)\n",
    "print(\"Number of words: {}\".format(len(word_freqs.keys())))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9805a9d5",
   "metadata": {},
   "source": [
    "The alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by ##:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adad7bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', '#', '##!', '##\"', '##$', '##%', '##&', \"##'\", '##)', '##+', '##,', '##-', '##.', '##/', '##0', '##1', '##2', '##3', '##4', '##5', '##6', '##7', '##8', '##9', '##:', '##;', '##?', '##]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##q', '##r', '##s', '##t', '##u', '##v', '##w', '##x', '##y', '##z', '##|', '##®', '##é', '##–', '##—', '##…', '$', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '[', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '–', '—', '€']\n"
     ]
    }
   ],
   "source": [
    "characters = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in characters:\n",
    "        characters.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in characters:\n",
    "            characters.append(f\"##{letter}\")\n",
    "\n",
    "characters = sorted(characters)\n",
    "print(characters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f35e0277",
   "metadata": {},
   "source": [
    "## Adding Special Tokens\n",
    "\n",
    "BERT and other models that use WordPiece tokenization use special tokens like \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\". We need to add these tokens to our vocabulary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3488dc5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + characters.copy()\n",
    "len(vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fb1326b",
   "metadata": {},
   "source": [
    "## Splitting Words into Characters\n",
    "\n",
    "Next, we split each word into characters. We also add a special prefix \"##\" to all characters except the first one in each word. This prefix is used to indicate that a character is not the start of a new word:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "328e74e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50040ae1",
   "metadata": {},
   "source": [
    "## Computing Pair Scores\n",
    "\n",
    "Now, we compute the scores for all possible pairs of characters. The score of a pair is defined as the frequency of the pair divided by the product of the frequencies of the individual characters. The idea is to find the pair that occurs together more often than separately:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f016267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('t', '##r'): 5.0029720626114525e-05\n",
      "('##r', '##u'): 1.623534759879209e-05\n",
      "('##u', '##d'): 9.79063701800694e-06\n",
      "('##d', '##e'): 4.1215004357264235e-05\n",
      "('##e', '##a'): 2.057878735731085e-05\n",
      "('##a', '##u'): 4.019809621816311e-06\n"
     ]
    }
   ],
   "source": [
    "def compute_pair_scores(splits, word_freqs):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "\n",
    "pair_scores = compute_pair_scores(splits, word_freqs)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eb19f8a",
   "metadata": {},
   "source": [
    "The `compute_pair_scores` function calculates the scores for all possible pairs of characters in the vocabulary. The score of a pair is defined as the frequency of the pair divided by the product of the frequencies of the individual characters. This score is a measure of how often the pair occurs together compared to how often they occur separately.\n",
    "\n",
    "## Finding the Best Pair\n",
    "\n",
    "Next, we find the pair with the highest score. This pair is the best candidate for merging:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61931da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('~', '##$') 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8444f2f8",
   "metadata": {},
   "source": [
    "## Merging the Best Pair\n",
    "\n",
    "Once we have identified the best pair, we merge it and update our splits:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d8aada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits, word_freqs):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "            splits[word] = split\n",
    "    return splits\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "005d0d5e",
   "metadata": {},
   "source": [
    "The `merge_pair` function merges the best pair in all the words in our vocabulary. If the second character of the pair starts with \"##\", we remove the \"##\" before merging. After merging, we replace the pair in the word with the merged character.\n",
    "\n",
    "## Repeating the Process\n",
    "\n",
    "We repeat the process of computing pair scores, finding the best pair, and merging the best pair until we reach our desired vocabulary size:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76996c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 tokens: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '\"', '#', '##!', '##\"', '##$']\n",
      "Last 50 tokens: ['##tangibl', '##ntangibl', '##intangibl', \"'intangibl\", '##rangl', 'wrangl', 'dange', 'danger', '##eng', '##leng', '##lleng', '##alleng', 'challeng', 'challenge', 'challenge:', 'ange', 'anger', 'angers', \"'intangible\", 'wrangle', 'wrangler', 'change', 'changes', 'challenger', 'challenges', 'chi', 'chin', 'chip', 'chic', 'chil', 'chico', \"chico's\", 'china', \"china's\", 'china,', 'chipo', 'chipot', 'chipotl', 'chile', 'chipotle', \"chipotle's\", 'chipotle,', 'chine', 'chines', 'chinese', \"##rsday's\", \"##ursday's\", \"thursday's\", \"##esday's\", \"##uesday's\"]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits, word_freqs)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits, word_freqs)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)\n",
    "\n",
    "print(\"First 10 tokens: {}\".format(vocab[:10]))\n",
    "print(\"Last 50 tokens: {}\".format(vocab[-50:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3243675f",
   "metadata": {},
   "source": [
    "## Encoding Words\n",
    "\n",
    "Now that we have our WordPiece vocabulary, we can use it to encode words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "443c2cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', '##o', '##m', '##p', '##a', '##n', '##y']\n",
      "['c', '##o', '##m', '##p', '##a', '##n', '##i', '##e', '##s']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens\n",
    "\n",
    "\n",
    "print(encode_word(\"company\"))\n",
    "print(encode_word(\"companies\"))\n",
    "print(encode_word(\"회사\"))  # UNK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e167db2",
   "metadata": {},
   "source": [
    "The `encode_word` function takes a word and encodes it using the WordPiece vocabulary. It starts from the end of the word and finds the longest substring that is in the vocabulary. If no such substring is found, it returns the unknown token \"[UNK]\". Otherwise, it adds the substring to the list of tokens and repeats the process with the remaining part of the word.\n",
    "\n",
    "## Tokenizing Text\n",
    "\n",
    "Finally, we can use our WordPiece tokenizer to tokenize text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47ad848e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['investment', 'opportunities', 'in', 'the', 'c', '##o', '##m', '##p', '##a', '##n', '##y']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    words = pre_tokenize(text)\n",
    "    encoded_words = [encode_word(word) for word in words]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "tokenized_text = tokenize(\"Investment opportunities in the company\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87b335d5",
   "metadata": {},
   "source": [
    "The `tokenize` function takes a text, splits it into words using the `pre_tokenize` function, encodes each word using the `encode_word` function, and returns the list of encoded words.\n",
    "\n",
    "That's it! You have now implemented WordPiece tokenization from scratch. You can use this knowledge to understand how subword tokenization works in models like BERT, DistilBERT, and RoBERTa.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46fe7cf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by ##:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "486e42fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##0', '##1', '##2', '##3', '##4', '##5', '##6', '##7', '##8', '##9', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##q', '##r', '##s', '##t', '##u', '##v', '##w', '##x', '##y', '##z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "characters = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in characters:\n",
    "        characters.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in characters:\n",
    "            characters.append(f\"##{letter}\")\n",
    "\n",
    "characters.sort()\n",
    "\n",
    "print(characters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b913df4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Add the special tokens used by the model at the beginning of that vocabulary. In the case of BERT, it’s the list [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20a72d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + characters.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9c4a680",
   "metadata": {},
   "source": [
    "Split each word, with all the letters that are not the first prefixed by ##:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95130df",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3494fddb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A function to compute the score of each pair:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b494b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73134672",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', '##n'): 3.065532997859911e-05\n",
      "('##n', '##v'): 5.217525332521506e-06\n",
      "('##v', '##e'): 2.2967510416892118e-05\n",
      "('##e', '##s'): 6.2108678847586545e-06\n",
      "('##s', '##t'): 7.931201514160114e-06\n",
      "('##t', '##i'): 8.905730802064189e-06\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "150b06f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Find the pair with the highest score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8036f196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('8', '##9') 0.0004752098843655948\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08498fdf",
   "metadata": {},
   "source": [
    "So the first merge to learn is (`8`, `##9`) -> `89`. Add it to the vocabulary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f633bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append(\"89\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe293c5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "To continue, we need to apply that merge in our splits dictionary. A function for this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84dbb5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85d4ebe0",
   "metadata": {},
   "source": [
    "And we can have a look at the result of the first merge:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d624b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['89', '##2', '##0']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(\"8\", \"##9\", splits)\n",
    "splits[\"8920\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "048d0b8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now we have everything we need to loop until we have learned all the merges we want. For example, we can loop until we have a vocabulary of size 1000:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e84fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc976956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 tokens: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##0', '##1', '##2', '##3', '##4']\n",
      "Last 50 tokens: ['thompso', 'accompan', 'accompany', 'accompani', 'thompson', 'compa', 'compan', 'company', 'compani', 'compar', 'compac', 'compas', 'compass', 'compact', 'employm', '##mpl', '##mply', '##imply', 'simply', '##amp', 'camp', 'ramp', 'hamp', 'damp', '##vamp', '##ymp', 'lymp', 'lymph', '##lymp', 'olymp', 'olympi', 'olympic', 'lympho', 'lymphom', 'lymphoma', 'campa', 'campai', 'campaig', 'campaign', '##rump', 'trump', 'bump', 'gump', 'pump', 'pumps', '##lump', 'slump', '##sump', '##sumpt', '##sumpti']\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 tokens: {}\".format(vocab[:10]))\n",
    "print(\"Last 50 tokens: {}\".format(vocab[-50:]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4118aea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Encoding is done by finding the biggest subword in the vocabulary that is in the word, and splitting on it. Iterating on the word until it is empty:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a0f6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e55dbedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company']\n",
      "['compani', '##e', '##s']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"company\"))\n",
    "print(encode_word(\"companies\"))\n",
    "print(encode_word(\"회사\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f400b6c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "To tokenize a sentence, we can apply this function to each word:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "806bbc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = pre_tokenize(text)\n",
    "    encoded_words = [encode_word(word) for word in words]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50a9afe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', '##n', '##v', '##e', '##s', '##t', '##m', '##e', '##n', '##t', 'opportuniti', '##e', '##s', 'i', '##n', 'th', '##e', 'company']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenize(\"Investment opportunities in the company\")\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6806c658",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69e0d190",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Then, we need to initialize our vocabulary to something larger than the vocabulary size we want.\n",
    "- We have to include all the basic characters (otherwise we won’t be able to tokenize every word).\n",
    "- For the bigger substrings, we can use the most frequent substrings in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc11ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in', 6437), ('th', 6241), ('he', 4833), ('er', 4585), ('re', 4475), ('an', 4311), ('the', 3977), ('on', 3842), ('es', 3360), ('ar', 3269)]\n"
     ]
    }
   ],
   "source": [
    "character_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        character_freqs[word[i]] += freq\n",
    "        # Loop through the subwords of length at least 2\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i:j]] += freq\n",
    "\n",
    "# Sort subwords by frequency\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_subwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7558f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60023"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_subwords)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90ed43b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We group the characters with the best subwords to arrive at an initial vocabulary of size 2000:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e388d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs = (\n",
    "    list(character_freqs.items()) + sorted_subwords[: 2000 - len(character_freqs)]\n",
    ")\n",
    "token_freqs = {token: freq for token, freq in token_freqs}\n",
    "len(token_freqs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8093f3c",
   "metadata": {},
   "source": [
    "Next, we compute the sum of all frequencies, to convert the frequencies into probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dac4d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd04301d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The main function is the one that tokenizes words using the Viterbi algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e97350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, model):\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    for start_idx in range(len(word)):\n",
    "        # This should be properly filled by the previous steps of the loop\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                # If we have found a better segmentation ending at end_idx, we update\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        # We did not find a tokenization of the word -> unknown\n",
    "        return [\"<unk>\"], None\n",
    "\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056f330a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['app', 'le'], 16.199784937807312)\n",
      "(['investment'], 9.955290111180942)\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"apple\", model))\n",
    "print(encode_word(\"investment\", model))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41aefe0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Compute the loss:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10c70cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802891.4150846584"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "compute_loss(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53ac4e1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Computing the scores for each token:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a5975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # We always keep tokens of length 1\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = compute_scores(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cc44ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.53267826826777\n",
      "239.44326648849528\n",
      "326.00756032345816\n",
      "113.19295595935546\n",
      "435.93838484131265\n"
     ]
    }
   ],
   "source": [
    "print(scores[\"app\"])\n",
    "print(scores[\"le\"])\n",
    "print(scores[\"investment\"])\n",
    "print(scores[\"invest\"])\n",
    "print(scores[\"ment\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d36b1a55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Iterate until we have the desired vocabulary size:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 1000:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    # Remove percent_to_remove tokens with the lowest scores.\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "053846b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "To tokenize a sentence, we can apply this function to each word:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d001097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, model):\n",
    "    words = pre_tokenize(text)\n",
    "    encoded_words = [encode_word(word, model)[0] for word in words]\n",
    "    return sum(encoded_words, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cdb5b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['investment', 'o', 'pport', 'un', 'ities', 'in', 'the', 'company']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenize(\"investment opportunities in the company\", model)\n",
    "print(tokenized_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
