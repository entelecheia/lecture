{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Tokenization Pipeline\n",
    "\n",
    "```{figure} figs/tokenization_pipeline.png\n",
    "---\n",
    "width: 80%\n",
    "name: fig-tokenization-pipeline\n",
    "---\n",
    "Tokenization pipeline\n",
    "```\n",
    "\n",
    "The tokenization process typically involves the following steps:\n",
    "\n",
    "1. **Normalization**: This is the process of converting text into a standard format. This may involve converting all text to lowercase, removing punctuation or special characters, correcting spelling errors, and so on. The goal of normalization is to reduce the complexity of the text and to focus on the important features.\n",
    "\n",
    "2. **Pre-tokenization**: This involves splitting the text into preliminary tokens. This could be as simple as splitting the text into words based on whitespace, or it could involve more complex processes like sentence segmentation.\n",
    "\n",
    "3. **Model**: This is the core algorithm that determines how the pre-tokenized text is further broken down into tokens. There are various models available, including word-level, character-level, and subword-level tokenization models.\n",
    "\n",
    "4. **Post-processing**: This involves any additional processing that needs to be done after tokenization. This could include removing stop words (common words like 'is', 'the', 'and', etc. that are often filtered out), stemming (reducing words to their root form), or lemmatization (reducing words to their base or dictionary form).\n",
    "\n",
    "Let's now look at how we can implement this pipeline using Python.\n",
    "\n",
    "## Implementing Tokenization in Python\n",
    "\n",
    "We will be using the `transformers` library from Hugging Face, which provides a wide range of pre-trained models for NLP tasks. It also provides various tokenizers that we can use.\n",
    "\n",
    "First, let's import the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76364d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd4b0c86",
   "metadata": {},
   "source": [
    "We can use the `AutoTokenizer` class to automatically infer the correct tokenizer to use based on the pre-trained model we specify.\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Normalization is usually handled internally by the tokenizer. However, we can see the effect of normalization by directly calling the normalizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c40ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b64e0697",
   "metadata": {},
   "source": [
    "In this example, we're using the tokenizer for the `bert-base-uncased` model. The `normalize_str` method applies the normalizer to the input string, converting all characters to lowercase and removing any special characters.\n",
    "\n",
    "### Pre-tokenization\n",
    "\n",
    "Next, we perform pre-tokenization. This involves splitting the text into preliminary tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ac2e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (16, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f47b4ee",
   "metadata": {},
   "source": [
    "Different models may use different pre-tokenization methods. For example, the GPT-2 and T5 models use different pre-tokenizers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0fde111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁Hello,', (0, 6)),\n",
       " ('▁how', (7, 10)),\n",
       " ('▁are', (11, 14)),\n",
       " ('▁you?', (16, 20))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be0b71af",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The model is the core algorithm that determines how the pre-tokenized text is further broken down into tokens. This is usually handled internally by the tokenizer, so we won't go into detail here.\n",
    "\n",
    "### Post-processing\n",
    "\n",
    "Finally, we perform post-processing. This involves any additional processing that needs to be done after tokenization. For example, we might want to convert our tokens into input IDs, which can be inputted into our\n",
    "\n",
    "model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53112a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.46MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('Ġhow', (6, 10)),\n",
       " ('Ġare', (10, 14)),\n",
       " ('Ġ', (14, 15)),\n",
       " ('Ġyou', (15, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62f9262c",
   "metadata": {},
   "source": [
    "In this example, we're using the `pre_tokenize_str` method of the tokenizer's model to tokenize the input string.\n",
    "\n",
    "### Postprocessing\n",
    "\n",
    "Postprocessing involves any additional processing that needs to be done after the text has been tokenized. This can include padding the tokenized text to a certain length, adding special tokens (like start and end tokens), or converting the tokenized text into a format that can be used by a machine learning model.\n",
    "\n",
    "Here's an example of how you can perform postprocessing using the `transformers` library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8532cfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7592, 1010, 2129, 2024, 2017, 1029]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"Hello, how are you?\")\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "609d6eaa",
   "metadata": {},
   "source": [
    "In this example, the `convert_tokens_to_ids` method is used to convert each token into its corresponding input ID.\n",
    "\n",
    "Another common post-processing step is adding special tokens, such as `[CLS]` and `[SEP]`, which are used by models like BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a7acc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'hello', ',', 'how', 'are', 'you', '?', '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"Hello, how are you?\")\n",
    "tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "tokens\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0900daa2",
   "metadata": {},
   "source": [
    "In this example, we manually add the `[CLS]` token at the beginning of our token list and the `[SEP]` token at the end.\n",
    "\n",
    "However, the `transformers` library provides a more convenient way to perform these post-processing steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "309b1b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2129, 2024, 2017, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_input = tokenizer(\"Hello, how are you?\", add_special_tokens=True)\n",
    "encoded_input\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecf1347f",
   "metadata": {},
   "source": [
    "In this example, the `tokenizer` method automatically tokenizes the input text, converts the tokens to input IDs, and adds the special tokens. The `add_special_tokens=True` argument specifies that we want to add the special tokens.\n",
    "\n",
    "The `tokenizer` method returns an `Encoding` object, which contains all the information we need:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b8f7ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7592, 1010, 2129, 2024, 2017, 1029, 102]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_input.input_ids)  # The input IDs\n",
    "print(encoded_input.token_type_ids)  # The token type IDs\n",
    "print(encoded_input.attention_mask)  # The attention mask\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a8a2f12",
   "metadata": {},
   "source": [
    "The `input_ids` attribute contains the input IDs, the `token_type_ids` attribute contains the token type IDs (used for models that have separate segments of input, like BERT), and the `attention_mask` attribute contains the attention mask (used for models that use attention, like BERT and GPT-2).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
