{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab9abd5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Unigram Step-by-Step Implementation\n",
    "\n",
    "Unigram Language Model (ULM) is a subword tokenization algorithm, frequently used for Natural Language Processing (NLP) tasks. It's a statistical language model which is trained to predict the next word in a sentence in the context of the previous words. Here, we will walk you through a detailed implementation of the Unigram algorithm using a financial news headlines dataset.\n",
    "\n",
    "## Dataset Preparation\n",
    "\n",
    "First, we need to load our dataset. We will use the `ashraq/financial-news` dataset from the Hugging Face Hub. We will use the `headline` column as our text data. We will randomly sample 1000 records from this dataset for our tokenization process. Here is how we can do this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37b79f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/yjlee/.cache/huggingface/datasets/ashraq___parquet/ashraq--financial-news-89d6ac597a40e29e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 54.01it/s]\n",
      "Loading cached shuffled indices for dataset at /home/yjlee/.cache/huggingface/datasets/ashraq___parquet/ashraq--financial-news-89d6ac597a40e29e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-79cde8905e45a47f.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ashraq/financial-news\")\n",
    "texts = dataset[\"train\"].shuffle(seed=1234).select(range(1000))[\"headline\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7150dafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3636\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def pre_tokenize(text, lowercase=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.split(\" \")\n",
    "\n",
    "\n",
    "def initialize_vocab(texts, lowercase=True):\n",
    "    vocab = defaultdict(int)\n",
    "    for text in texts:\n",
    "        words = pre_tokenize(text, lowercase)\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "word_freqs = initialize_vocab(texts)\n",
    "print(\"Number of words: {}\".format(len(word_freqs.keys())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45167185",
   "metadata": {},
   "source": [
    "## Vocabulary Initialization\n",
    "\n",
    "For Unigram, we need to initialize our vocabulary to something larger than the vocabulary size we want. We have to include all the basic characters (otherwise we won’t be able to tokenize every word). For the bigger substrings, we can use the most frequent substrings in the corpus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9be49ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        character_freqs[word[i]] += freq\n",
    "        # Loop through the subwords of length at least 2\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i:j]] += freq\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91380086",
   "metadata": {},
   "source": [
    "Then, we sort subwords by frequency:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e721c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in', 1092), ('re', 706), ('er', 690), ('st', 675), ('es', 673), ('an', 629), ('ar', 617), ('al', 610), ('on', 539), ('ng', 504)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33925"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_subwords[:10])\n",
    "len(sorted_subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2bed8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs = (\n",
    "    list(character_freqs.items()) + sorted_subwords[: 2000 - len(character_freqs)]\n",
    ")\n",
    "token_freqs = {token: freq for token, freq in token_freqs}\n",
    "len(token_freqs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b744d71",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Next, we create a Unigram model, where each token is associated with a negative log-likelihood:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2aba06d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n",
    "len(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e637440a",
   "metadata": {},
   "source": [
    "## Word Encoding\n",
    "\n",
    "The `encode_word` function is used to find the best segmentation of a word into subwords, using the Unigram model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7337d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['i', 'ph', 'one'], 21.168929786103597)\n",
      "(['app', 'le'], 16.224036483432627)\n"
     ]
    }
   ],
   "source": [
    "def encode_word(word, model):\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    for start_idx in range(len(word)):\n",
    "        # This should be properly filled by the previous steps of the loop\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                # If we have found a better segmentation ending at end_idx, we update\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        # We did not find a tokenization of the word -> unknown\n",
    "        return [\"<unk>\"], None\n",
    "\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score\n",
    "\n",
    "\n",
    "print(encode_word(\"iphone\", model))\n",
    "print(encode_word(\"apple\", model))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9019429",
   "metadata": {},
   "source": [
    "## Loss Computation\n",
    "\n",
    "We then define a function to compute the loss of our model. The loss is the sum of the negative log-likelihoods of each word in the vocabulary, weighted by its frequency:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e31878cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136832.67116572068"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "compute_loss(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c35665c",
   "metadata": {},
   "source": [
    "## Compute Scores\n",
    "\n",
    "We compute the scores by calculating the change in the loss when a token is removed from the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3ec3fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # We always keep tokens of length 1\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = compute_scores(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "819b4d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.386676873022225\n",
      "45.03959364909679\n",
      "48.10544469070737\n",
      "55.051404965808615\n",
      "45.268116643244866\n"
     ]
    }
   ],
   "source": [
    "print(scores[\"app\"])\n",
    "print(scores[\"le\"])\n",
    "print(scores[\"investment\"])\n",
    "print(scores[\"invest\"])\n",
    "print(scores[\"ment\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7fba9ef",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "\n",
    "We iteratively remove tokens from the model to improve its performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7892bda1",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing ngt\n",
      "Removing gth\n",
      "Removing rodu\n",
      "Removing odu\n",
      "Removing oduc\n",
      "Removing oldings\n",
      "Removing ldings\n",
      "Removing lides\n",
      "Removing artners\n",
      "Removing rtners\n",
      "Removing tners\n",
      "Removing mbe\n",
      "Removing expec\n",
      "Removing xpec\n",
      "Removing xpect\n",
      "Removing resou\n",
      "Removing resour\n",
      "Removing resourc\n",
      "Removing esou\n",
      "Removing esour\n",
      "Removing esourc\n",
      "Removing esource\n",
      "Removing -sta\n",
      "Removing tars\n",
      "Removing adin\n",
      "Removing erna\n",
      "Removing highe\n",
      "Removing ighe\n",
      "Removing igher\n",
      "Removing ghe\n",
      "Removing gher\n",
      "Removing poin\n",
      "Removing oint\n",
      "Removing xt\n",
      "Removing ainers\n",
      "Removing quart\n",
      "Removing quarte\n",
      "Removing uart\n",
      "Removing uarte\n",
      "Removing uarter\n",
      "Removing erage\n",
      "Removing misse\n",
      "Removing isses\n",
      "Removing loser\n",
      "Removing oser\n",
      "Removing secto\n",
      "Removing s&\n",
      "Removing sma\n",
      "Removing tti\n",
      "Removing outl\n",
      "Removing outlo\n",
      "Removing outloo\n",
      "Removing utlo\n",
      "Removing utloo\n",
      "Removing utlook\n",
      "Removing tloo\n",
      "Removing tlook\n",
      "Removing ighlights\n",
      "Removing ghlights\n",
      "Removing hlights\n",
      "Removing lights\n",
      "Removing ervi\n",
      "Removing sday'\n",
      "Removing eh\n",
      "Removing nder\n",
      "Removing itio\n",
      "Removing prod\n",
      "Removing produ\n",
      "Removing bon\n",
      "Removing arkets\n",
      "Removing rkets\n",
      "Removing kets\n",
      "Removing a'\n",
      "Removing rf\n",
      "Removing orati\n",
      "Removing oratio\n",
      "Removing sset\n",
      "Removing downg\n",
      "Removing downgr\n",
      "Removing downgra\n",
      "Removing downgrad\n",
      "Removing downgrade\n",
      "Removing owng\n",
      "Removing owngr\n",
      "Removing owngra\n",
      "Removing owngrad\n",
      "Removing owngrade\n",
      "Removing owngraded\n",
      "Removing wng\n",
      "Removing wngr\n",
      "Removing wngra\n",
      "Removing wngrad\n",
      "Removing wngrade\n",
      "Removing wngraded\n",
      "Removing ngr\n",
      "Removing ngra\n",
      "Removing ngrad\n",
      "Removing ngrade\n",
      "Removing ngraded\n",
      "Removing nor\n",
      "Removing nke\n",
      "Removing erge\n",
      "Removing ya\n",
      "Removing rgi\n",
      "Removing trat\n",
      "Removing omment\n",
      "Removing mment\n",
      "Removing enta\n",
      "Removing isc\n",
      "Removing ovi\n",
      "Removing ema\n",
      "Removing stren\n",
      "Removing streng\n",
      "Removing strengt\n",
      "Removing treng\n",
      "Removing trengt\n",
      "Removing trength\n",
      "Removing reng\n",
      "Removing rengt\n",
      "Removing rength\n",
      "Removing iu\n",
      "Removing osers\n",
      "Removing sers\n",
      "Removing isha\n",
      "Removing ishar\n",
      "Removing ishare\n",
      "Removing ike\n",
      "Removing dg\n",
      "Removing nth\n",
      "Removing ttin\n",
      "Removing servi\n",
      "Removing servic\n",
      "Removing ervic\n",
      "Removing ervice\n",
      "Removing rvic\n",
      "Removing rvice\n",
      "Removing ndustry\n",
      "Removing dustry\n",
      "Removing ustry\n",
      "Removing stry\n",
      "Removing roun\n",
      "Removing emi\n",
      "Removing esd\n",
      "Removing esda\n",
      "Removing sk\n",
      "Removing tfs\n",
      "Removing righ\n",
      "Removing e-\n",
      "Removing corporati\n",
      "Removing corporatio\n",
      "Removing orporati\n",
      "Removing orporatio\n",
      "Removing orporation\n",
      "Removing rporati\n",
      "Removing rporatio\n",
      "Removing rporation\n",
      "Removing porati\n",
      "Removing poratio\n",
      "Removing poration\n",
      "Removing tem\n",
      "Removing trus\n",
      "Removing rust\n",
      "Removing sn\n",
      "Removing orp,\n",
      "Removing rp,\n",
      "Removing firs\n",
      "Removing irs\n",
      "Removing irst\n",
      "Removing 015\n",
      "Removing esources\n",
      "Removing sources\n",
      "Removing ources\n",
      "Removing urces\n",
      "Removing rces\n",
      "Removing vet\n",
      "Removing nom\n",
      "Removing interna\n",
      "Removing internat\n",
      "Removing internati\n",
      "Removing internatio\n",
      "Removing internation\n",
      "Removing internationa\n",
      "Removing nterna\n",
      "Removing nternat\n",
      "Removing ngs\n",
      "Removing toc\n",
      "Removing arn\n",
      "Removing tock\n",
      "Removing rning\n",
      "Removing arni\n",
      "Removing arnin\n",
      "Removing earni\n",
      "Removing earnin\n",
      "Removing nings\n",
      "Removing arnings\n",
      "Removing ..\n",
      "Removing ee\n",
      "Removing aly\n",
      "Removing 01\n"
     ]
    }
   ],
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 1800:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    # Remove percent_to_remove tokens with the lowest scores.\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        print(\"Removing {}\".format(sorted_scores[i][0]))\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac48c714",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Finally, we define a `tokenize` function that splits a given text into subwords according to our trained model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "edab6f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['investment', 'op', 'port', 'un', 'ities', 'in', 'the', 'compan', 'y']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text, model):\n",
    "    words = pre_tokenize(text)\n",
    "    encoded_words = [encode_word(word, model)[0] for word in words]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "tokenized_text = tokenize(\"investment opportunities in the company\", model)\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69e0d190",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "This concludes our detailed step-by-step implementation of the Unigram Language Model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
