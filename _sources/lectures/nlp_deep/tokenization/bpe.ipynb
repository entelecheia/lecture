{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48017f83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# BPE Step-by-Step Implementation\n",
    "\n",
    "In this lecture, we will walk through the implementation of Byte Pair Encoding (BPE), a popular subword tokenization method. We will use a dataset of financial news headlines for this demonstration.\n",
    "\n",
    "## Dataset Preparation\n",
    "\n",
    "First, we need to load our dataset. We will use the `ashraq/financial-news` dataset from the Hugging Face Hub. We will randomly sample 1000 records from this dataset for our demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "813fdadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset parquet (/home/yjlee/.cache/huggingface/datasets/ashraq___parquet/ashraq--financial-news-89d6ac597a40e29e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 48.62it/s]\n",
      "Loading cached shuffled indices for dataset at /home/yjlee/.cache/huggingface/datasets/ashraq___parquet/ashraq--financial-news-89d6ac597a40e29e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-79cde8905e45a47f.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ashraq/financial-news\")\n",
    "texts = dataset[\"train\"].shuffle(seed=1234).select(range(1000))[\"headline\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8485fc0",
   "metadata": {},
   "source": [
    "## BPE Implementation\n",
    "\n",
    "Now, let's dive into the implementation of BPE.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "We start by initializing our vocabulary. We will format each word by separating its characters with spaces and appending a special end-of-word token `</w>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31158d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3636\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "\n",
    "def format_word(text, space_token=\"▁\"):\n",
    "    return \" \".join(list(text)) + \" \" + space_token\n",
    "\n",
    "\n",
    "def initialize_vocab(texts, lowercase=True):\n",
    "    vocab = {}\n",
    "\n",
    "    for text in texts:\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        all_words = text.split()\n",
    "        for word in all_words:\n",
    "            word = format_word(word)\n",
    "            vocab[word] = vocab.get(word, 0) + 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = initialize_vocab(texts)\n",
    "print(f\"Number of words: {len(vocab)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b73fe7c4",
   "metadata": {},
   "source": [
    "### Token Extraction\n",
    "\n",
    "Next, we extract all unique tokens from our vocabulary and count their frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dab07179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 64\n"
     ]
    }
   ],
   "source": [
    "def get_tokens_from_vocab(vocab):\n",
    "    tokens = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens[token] += freq\n",
    "        vocab_tokenization[\"\".join(word_tokens)] = word_tokens\n",
    "    return tokens, vocab_tokenization\n",
    "\n",
    "\n",
    "tokens, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "936eeb86",
   "metadata": {},
   "source": [
    "### Bigram Counts\n",
    "\n",
    "We then count the frequency of each bigram (pair of consecutive tokens) in our vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2327e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_counts(vocab):\n",
    "    pairs = {}\n",
    "    for word, count in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            pairs[pair] = pairs.get(pair, 0) + count\n",
    "    return pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74443b3e",
   "metadata": {},
   "source": [
    "### Merge Operations\n",
    "\n",
    "The core of BPE is a series of merge operations. In each operation, we find the most frequent bigram and merge it into a single token. We repeat this process for a specified number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce722936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 0: ('s', '▁') with count 1639\n",
      "All tokens: dict_keys(['t', 'r', 'u', 'd', 'e', 'a', '▁', 'w', 'i', 'l', 'c', 'o', 'n', 'g', 'v', 'm', ',', 'h', '.', 'k', \"'\", 's▁', 'j', 's', 'f', 'y', ':', 'p', '(', ')', '?', 'b', ';', 'z', '2', '0', '1', '8', '&', '5', '-', 'q', '3', 'x', '4', '7', '%', '$', '9', '/', '6', '—', '…', '–', '|', '!', '#', '[', ']', '~', 'é', '+', '®', '\"', '€'])\n",
      "Number of tokens: 65\n",
      "Merge 1: ('i', 'n') with count 1092\n",
      "All tokens: dict_keys(['t', 'r', 'u', 'd', 'e', 'a', '▁', 'w', 'i', 'l', 'c', 'o', 'n', 'g', 'v', 'm', 'in', ',', 'h', '.', 'k', \"'\", 's▁', 'j', 's', 'f', 'y', ':', 'p', '(', ')', '?', 'b', ';', 'z', '2', '0', '1', '8', '&', '5', '-', 'q', '3', 'x', '4', '7', '%', '$', '9', '/', '6', '—', '…', '–', '|', '!', '#', '[', ']', '~', 'é', '+', '®', '\"', '€'])\n",
      "Number of tokens: 66\n",
      "Merge 2: ('e', '▁') with count 900\n",
      "All tokens: dict_keys(['t', 'r', 'u', 'd', 'e', 'a', '▁', 'w', 'i', 'l', 'c', 'o', 'n', 'g', 'v', 'm', 'in', ',', 'h', 'e▁', '.', 'k', \"'\", 's▁', 'j', 's', 'f', 'y', ':', 'p', '(', ')', '?', 'b', ';', 'z', '2', '0', '1', '8', '&', '5', '-', 'q', '3', 'x', '4', '7', '%', '$', '9', '/', '6', '—', '…', '–', '|', '!', '#', '[', ']', '~', 'é', '+', '®', '\"', '€'])\n",
      "Number of tokens: 67\n",
      "Merge 999: ('cor', 'p.▁') with count 6\n",
      "All tokens: dict_keys(['tru', 'de', 'au', '▁', 'will▁', 'le', 'ad▁', 'a▁', 'co', 'al', 'iti', 'on▁', 'go', 'ver', 'n', 'ment▁', 'in▁', 'canad', 'a,▁', 'wh', 'il', 'e▁', 'the▁', 'u.', 'k', '.', \"'s▁\", 'jo', 'h', 'son▁', 'fi', 'gh', 'ts▁', 'an', 'o', 'ther▁', 'day▁', 'week▁', 'ahe', 'ad', ':▁', 'fo', 'm', 'c▁', 'me', 'et', 'ing', ',▁', 'g', 'd', 'p▁', 'and▁', 'more▁', 'earnings▁', 'is▁', 'l', 'um', 'in', '(', 'n)▁', 'great▁', 'growth▁', 'stock', '?▁', 'hu', 'ge▁', 'min', 'er▁', 'bank', 'r', 'up', 't', 'ci', 'es▁', 'po', 's', 'si', 'ble▁', 'so', 'on', ';▁', 'news▁', 'for▁', 'gold▁', 'sil', 'b', 'stre', 'am', 'ing▁', 'compani', 'e', 's?▁', 'ni', 'z', 'ant▁', 'en', 'th', 's▁', 'buy', 'back▁', 'pro', 'gr', 'am▁', 'ite▁', 'hou', 'se▁', 'sing▁', 'cu', 'op', 'i', 'id▁', 'produc', 'tion▁', 'global▁', 'bond▁', 'markets▁', 'to▁', 'ter▁', 'new▁', 'ph', 'ase▁', '2018▁', 'mar', 'sh▁', '&▁', 'cl', 'an▁', '(m', 'c)▁', 'hits▁', '5', '2', '-week▁', 'high▁', 'li', 'd▁', 'q3▁', 'results▁', 'tracking▁', 'pre', 'm▁', 'wat', \"a's▁\", 'fa', 'ir', 'x▁', 'financial▁', 'holdings▁', 'portfolio▁', '-▁', 'q4▁', '2017▁', 'update▁', 'bi', 'corporation▁', 'q2▁', 'call▁', 'slides▁', 'ver▁', 'ay▁', 'old▁', 'tri', 'c', 'cap', 'it', 'up▁', '20', '%▁', 'after▁', 'our', 'retail▁', 'sales▁', 'y▁', 'gain▁', 'stocks▁', 'fall▁', 'ahead▁', 'of▁', 'f', 'ed▁', 'gn', 'inc.▁', 'ters▁', 'over', 'ter', 'or', 'tale▁', 'tape▁', 'tu', 'ck', 'retirement▁', 'systems▁', 'insur', 'ance▁', 'trust▁', 'fund▁', 'buys▁', 'w', 't▁', 'dis', 'ne', 'he', 'ss▁', 'corp,▁', 'first▁', '...▁', 'ar', 'ry▁', 'ro', \"s'▁\", 'view▁', 'capital▁', 'management▁', '2015▁', 'tran', 'sc', 'ra', '$', 'b▁', 'ex', 'pan', 'sion▁', 'pl', 'no', 'v', 'gas▁', 'syste', 'ares▁', 'sol', 'ar▁', 'partn', 'er', 'shi', 'invest▁', 're', 'siden', 'tial▁', 'asse', 'by▁', 'ber', 's:▁', 'technology▁', 'with▁', 'rising▁', 'expect', 'ations▁', 'canadian▁', 'natural▁', 'resources▁', 'declares▁', 'ca', '0.', '3', '7', '5▁', 'dividend▁', 'vetr▁', 'community▁', 'has▁', 'downgraded▁', 'u', '-stars▁', 'im', 'mun', 'medic', 'high', 'ly▁', 'invest', 'p', 'du', 'inter', 'nation', '201', 'set▁', 'cor', 's,▁', 'fu', 'll▁', 'upg', 'raded▁', 'sis▁', '-star', 's.▁', 'higher▁', 'mb', 'p,▁', 'revenues▁', 'a', ')▁', 'earning', 'u.s.▁', 'back', 'bro', 'nor', 'th▁', 'anc', 'tions▁', 'how▁', 'rea', 'ch', '1', 'cr', 'amer', 'ma', 'mone', '9', '/', '4', '/1', '8', 'st', 'one▁', 'llc▁', 'point▁', 'se', 'ct▁', 'banc', 'inc,▁', 'at', 'y', 'k▁', 'ic', 'ip', 'al▁', 'income▁', '$0.0', '3▁', 'la', 'ru', 'sh', 'earnings:▁', 'why▁', 'as', \"'t▁\", 'stock▁', 'follow', 'su', 'wi', 'ceo▁', 'el▁', 'l▁', 'jon', 'sells▁', '10', ',', '0', '00▁', 'shares▁', 'top▁', 'ranked▁', 'mo', 'men', 'buy▁', 'march▁', 'ation▁', 'alty▁', 'ed', 'ran', 'dy▁', 'ur', 'che', '2016▁', 'transcript▁', 'mon', 'z▁', 'international▁', 'estimates▁', 'at▁', 'its▁', 'next▁', 'report', 'her', 'age▁', 'com', 'mer', 'ce▁', 'beats▁', '1,▁', 'revenue▁', 'in-', 'line▁', 'ag', 'ri', 'tur', 'market▁', 'report▁', 'thur', 'da', 'y,▁', '.▁', '6▁', 'ke', 'w▁', 'hold', '.5%▁', 'siti', 'portfolio', 'ol', 'ging▁', 'china▁', 'merger▁', 'ors▁', 'contin', 'ue▁', 'ry', 'sl', 'group▁', '1▁', 'auto▁', 'acqui', 'res▁', 'for', 'ers▁', 'analyst▁', 'blog▁', 'lif', 'est', 'ta', 'kes▁', 'ac', 'ate▁', 'investor▁', 'con', 'den', 'ous▁', '9▁', 'ali', 'o▁', 'continues▁', 'ti', 've▁', 'ff', 'st▁', 'cont', \"'\", 'ey', \"'▁\", 'goo', 'le▁', 'sho', 'cks▁', 'sch', 'ste', 'reit▁', 'sen', 'trad', 'n▁', 'pharma▁', 'ata▁', 'gainers▁', 'financ', \"e's▁\", '(h', '2019▁', 'all', 'scrip', 'el', 'ps▁', 'ho', 'sp', 'ital▁', 'chi', 'ear', 'lo', 'ws▁', 'mobil', 'vi', 'en▁', 'out▁', 'merg', 'strate', 'gy▁', 'quar', 'commentary▁', 'bank▁', 'les▁', 'be▁', 'arm', 'marke', 'economi', 'value▁', 'company▁', 'home▁', 'ak▁', 'ak', 's)▁', 'por', 'wor', 'ks▁', 'int', 'industri', 'strong▁', 'data▁', 'ds▁', 'cen', 'us▁', 'energy▁', 've', 'als▁', 'rise▁', '4▁', 'oil▁', 'out', 'pu', 'br', 'pac', 'consumer▁', 'di', 'tion', 'ary▁', 'pe', 'this▁', 'hn▁', 'bu', 'health▁', 'care▁', 'be', '-', '2▁', 'or▁', 'mor', ')', 'moving▁', 'average▁', 'ros', 'pot', 'stru', 'ments▁', 'beat', 'miss▁', 'should▁', 'investors▁', 'their▁', 'to', 'ward▁', 'ct', 'x)▁', 'time▁', 'star', 'heal', 'the', 'qu', 'ity▁', 'announ', 'quarter▁', 'year▁', '—…–', 'j', 'ab', 'il▁', 'it▁', 'ear▁', 'key▁', 'sup', 'je', 'x', 'off', 'er,▁', 'but▁', 'rema', 'ins▁', 'open▁', 'per', 'foo', 'te', 'sts▁', \"d's▁\", \"y's▁\", 'equ', 'misses▁', 'medi', 'wing▁', 'strength▁', 'among▁', 'energ', 'y/', 'mat', 'eri', 'gain', 's;▁', 'um▁', 'yiel', '10▁', 'bio', 'sci', 'ence▁', 'losers▁', 'mi', 'fil', 'da▁', 'ine▁', 'can', 'big▁', 'cap▁', 'fit', 'pi', 'july▁', '6', 'impro', 'fol', 'low▁', 'bl', 'ack', 'ck▁', 'into▁', 'inve', 'january▁', 'vo', 'oun', 'announces▁', 'earn', 'fun', 'ding▁', 'd-', '–', 'f▁', 'we', 'van', 'gu', 'ard▁', 's&p▁', '…', 'etf', 'ishares▁', 'are▁', '&', 't,▁', 'national▁', 'car', 'ate', 'etf▁', 'ba', 'sed▁', 'llion▁', 'ping▁', 'seas', 'close▁', 'xed▁', 'despite▁', 'reta', 'sm', 'g▁', 'products▁', 'inc▁', 'p)▁', 'ce', 'mber▁', 'do▁', 'sal', 'es,▁', 'y:▁', 'gen', 'move▁', 'ire▁', 'ould▁', 'fir', 'fe', 'your▁', 'from▁', 'ates▁', 'down▁', '7▁', 'par', 'ann', 'dividen', 'd,▁', 'valu', 'part▁', 'ap', 'vol', 'analy', 'e,▁', 'ren', 'lead', 'american▁', 'earns▁', 'rating▁', 'update', 'ati', 'dig', 'deal▁', 'loss▁', 'than▁', 'reti', 'un', 'res', 'ves▁', 'co▁', 'ju', 'ating▁', 'brea', 'ven', 'prices▁', 'met', 'vie', 'q', 'outlook▁', 'ha', 'tic', 'upgrad', 'ng', 'sur', 'ges▁', 'as▁', 'beat▁', 'eu', 'v▁', 'asset▁', 'technologies▁', 'come▁', 'notable▁', \"day's▁\", 'man', 'now▁', 'lu', 'manage', 'ment', 'mit', 'contrac', 'ss', 'rate▁', 'get', 'ting▁', 'los', 'tech', 'bar', 'str', 'inde', 'ssion▁', 'zacks▁', 'highlights:▁', 'char', 'commun', 'ation', 'sou', 'ther', 'ch▁', 'ic▁', '—…', '|', 'group', 'c,▁', '.5', '$0.', 'indi', 'ces▁', 'king▁', 'me▁', '0▁', 'pen', 'grow', '(f', 'wee', 'q1▁', 'dow▁', 'bo', 'zack', 's.com▁', 'red▁', 'highligh', 'inc', '?', 'i▁', 'dup', 'de▁', 'win', 'dow', 'shor', 'ke▁', '!', 'gic▁', 'expected▁', 'what▁', 'week', 'rele', 'decl', 'month', 'distri', 'relative▁', 'trade▁', 'ears▁', 'goes▁', 'ex-dividend▁', 'book▁', 'cha', 'stor', 'tr', 'oo', 'now?▁', 'ari', 'pay', 'ou', 'mic', 'port', 'ities▁', 'bon', 'fac', 'lay', 'uni', 'servi', 'industry▁', '—', 'war', 'trading▁', '&p▁', '#', 'chin', 'ound▁', 'equity▁', 'ral', 'wall▁', 'street▁', 'break', 'rat', 'super', 'pharm', '4%▁', 'ince▁', 'last▁', 'ly', 'high-yield▁', 'll', '5.', '3%▁', 'partners▁', 'mu', '[', 'view:▁', ']', 'financi', 'ty▁', 'mist', 'yield▁', 'ay', 'winn', 'gains▁', 'august▁', 'jum', 'you▁', 'industr', 'can▁', 'pri', 'bil', 'get▁', 'inf', 'struct', 'ure▁', 'fer', 'trac', 'gener', 'hi', 'incre', 'ases▁', 'reports▁', 'operating▁', '(10-q)▁', 'ght▁', 'diam', 'rough▁', 'ban', 'bet', 't-', 'tting▁', 'before▁', \"thursday's▁\", 'sta', 'estim', 'verage▁', 'look▁', 'investment▁', 'loo', 'ons▁', 'wedne', \"sday's▁\", 'best▁', 'way▁', 'ger▁', 'all▁', 'spe', 'sector▁', 'ble', 'compan', 'atur', 'ir▁', 'erg', 'vis', 'ine', 'gold', 'pa', 'cut▁', 'net', 'ctor▁', 'resul', 'ts:▁', 'tech▁', 'x,▁', 'ill▁', '~', 'g)▁', 'my▁', 'sum', 'man▁', 'fact', 'low', 'divi', 'tap', '8▁', 'ib', 'boo', 'debt▁', 'tg', 'gi', 'corp.▁', 't)▁', 'sli', 'ving▁', 'poin', 'ga', 'rai', 'gui', 'small▁', 'etfs▁', 'upd', 'et▁', 'ld', 'investing▁', 'har', 'sharehold', 'weekly▁', 'insi', 'highli', 'net▁', 'funds▁', 'neutral▁', 'mid', 'these▁', 'util', 'view', 'ge', 'com▁', 'sto', 'shar', 'ris', 'roun', 'ings▁', 'americ', 'lli', 'econo', '%', 'eps▁', 'right▁', 'do', 'expe', 'ff▁', 'é', 'aly', 'ong▁', 'h▁', 'michael▁', 'cy', ':', 'gre', 'hal', 'now', '+', 'qui', 'ative▁', 'ms▁', 'tn', 'uti', 'af', 'cono', 'under', 'rade', 'clo', 'end▁', 'reven', 'p.▁', 'sy', 'oper', 'streng', 'tw', 'fin', 'ree▁', 'nam', 'dri', 'val', 'corpor', 'age', 'tal', 'jan', 'ser', 'sof', 'ked▁', 'ld▁', 'resour', 'des▁', 'produ', 'der', 'oc', 'ber▁', 'rip', 'form', 'plan▁', 'track', 'miss', 'q)▁', 'nex', 'm,▁', 'neutr', 'perform', 'resu', 'yi', 'old', 'lls▁', 'technologi', '®', 'rom▁', 'indu', 'rou', 'you', 'gh▁', 'our▁', 'grad', 'blo', 'high-', 'stri', 'consum', 'rel', '\"', '€'])\n",
      "Number of tokens: 1031\n"
     ]
    }
   ],
   "source": [
    "def merge_vocab(pair, vocab_in):\n",
    "    vocab_out = {}\n",
    "    bigram = re.escape(\" \".join(pair))\n",
    "    p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
    "    bytepair = \"\".join(pair)\n",
    "    for word in vocab_in:\n",
    "        w_out = p.sub(bytepair, word)\n",
    "        vocab_out[w_out] = vocab_in[word]\n",
    "    return vocab_out, (bigram, bytepair)\n",
    "\n",
    "\n",
    "def find_merges(vocab, tokens, num_merges, indices_to_print=[0, 1, 2]):\n",
    "    merges = []\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_bigram_counts(vocab)\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        best_count = pairs[best_pair]\n",
    "\n",
    "        vocab, (bigram, bytepair) = merge_vocab(best_pair, vocab)\n",
    "        merges.append((r\"(?<!\\S)\" + bigram + r\"(?!\\S)\", bytepair))\n",
    "        tokens, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "        if i in indices_to_print:\n",
    "            print(f\"Merge {i}: {best_pair} with count {best_count}\")\n",
    "            print(\"All tokens: {}\".format(tokens.keys()))\n",
    "            print(\"Number of tokens: {}\".format(len(tokens.keys())))\n",
    "\n",
    "    return vocab, tokens, merges, vocab_tokenization\n",
    "\n",
    "\n",
    "num_merges = 1000\n",
    "indices_to_print = [0, 1, 2, num_merges - 1]\n",
    "\n",
    "vocab, tokens, merges, vocab_tokenization = find_merges(\n",
    "    vocab, tokens, num_merges, indices_to_print\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d32d7197",
   "metadata": {},
   "source": [
    "## Encoding and Decoding\n",
    "\n",
    "### Decoding\n",
    "\n",
    "Decoding is straightforward. We simply concatenate all the tokens together and remove the stop token `</w>`. For example, if the encoded sequence is [`the</w>`, `high`, `est</w>`, `moun`, `tain</w>`], the decoded sequence is `the highest mountain`.\n",
    "\n",
    "### Encoding\n",
    "\n",
    "Encoding is a bit more complex. For a given sentence, we need to find the longest token in our vocabulary that is a subword of each word in the sentence. If no such token exists, we replace the word with an unknown token `</u>`. This process is computationally expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7951c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_token_length(token, space_token=\"▁\"):\n",
    "    space_token_len = len(space_token)\n",
    "    if token[-space_token_len:] == space_token:\n",
    "        return len(token) - space_token_len + 1\n",
    "    else:\n",
    "        return len(token)\n",
    "\n",
    "\n",
    "def encode_word(string, sorted_tokens, unknown_token=\"</u>\"):\n",
    "    if string == \"\":\n",
    "        return []\n",
    "    sorted_tokens = sorted_tokens.copy()\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    string_tokens = []\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        token = sorted_tokens[i]\n",
    "        token_reg = re.escape(token.replace(\".\", \"[.]\"))\n",
    "\n",
    "        matched_positions = [\n",
    "            (m.start(0), m.end(0)) for m in re.finditer(token_reg, string)\n",
    "        ]\n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "        substring_end_positions = [\n",
    "            matched_position[0] for matched_position in matched_positions\n",
    "        ]\n",
    "\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            string_tokens += encode_word(\n",
    "                string=substring,\n",
    "                sorted_tokens=sorted_tokens[i + 1 :],\n",
    "                unknown_token=unknown_token,\n",
    "            )\n",
    "            string_tokens += [token]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += encode_word(\n",
    "            string=remaining_substring,\n",
    "            sorted_tokens=sorted_tokens[i + 1 :],\n",
    "            unknown_token=unknown_token,\n",
    "        )\n",
    "        break\n",
    "    return string_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "774d4ed9",
   "metadata": {},
   "source": [
    "We can now use this function to encode a given word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb751111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokenization(word_given, sorted_tokens, vocab_tokenization):\n",
    "    print(\"Tokenizing word: {}...\".format(word_given))\n",
    "    if word_given in vocab_tokenization:\n",
    "        print(\"Tokenization of the known word:\")\n",
    "        print(vocab_tokenization[word_given])\n",
    "        print(\"Tokenization treating the known word as unknown:\")\n",
    "        print(\n",
    "            encode_word(\n",
    "                string=word_given, sorted_tokens=sorted_tokens, unknown_token=\"</u>\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"Tokenizating of the unknown word:\")\n",
    "        print(\n",
    "            encode_word(\n",
    "                string=word_given, sorted_tokens=sorted_tokens, unknown_token=\"</u>\"\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d64ab0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing word: investors▁...\n",
      "Tokenization of the known word:\n",
      "['investors▁']\n",
      "Tokenization treating the known word as unknown:\n",
      "['investors▁']\n",
      "Tokenizing word: dogecoin▁...\n",
      "Tokenizating of the unknown word:\n",
      "['do', 'ge', 'co', 'in▁']\n"
     ]
    }
   ],
   "source": [
    "# Sort tokens by length in descending order\n",
    "sorted_tokens = sorted(tokens.keys(), key=len, reverse=True)\n",
    "\n",
    "word_given_known = \"investors▁\"\n",
    "\n",
    "print_tokenization(word_given_known, sorted_tokens, vocab_tokenization)\n",
    "\n",
    "word_given_unknown = \"dogecoin▁\"\n",
    "\n",
    "print_tokenization(word_given_unknown, sorted_tokens, vocab_tokenization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d316f27f",
   "metadata": {},
   "source": [
    "Finally, we can use our encoding function to tokenize an entire sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8726dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['investment▁', 'op', 'port', 'un', 'ities▁', 'in▁', 'the▁', 'company▁']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text, space_token=\"▁\"):\n",
    "    text = re.sub(\"\\s+\", \" \", text.lower())\n",
    "    words = [word + space_token for word in text.split(\" \")]\n",
    "    encoded_words = [\n",
    "        encode_word(word, sorted_tokens, unknown_token=\"</u>\") for word in words\n",
    "    ]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "tokenized_text = tokenize(\"Investment opportunities in the company\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f68216a",
   "metadata": {},
   "source": [
    "That's it! You have now implemented BPE from scratch. This should give you a good understanding of how subword tokenization works in practice.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93ad889c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
