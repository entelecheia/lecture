{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48017f83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# BPE Step-by-Step Implementation\n",
    "\n",
    "In this lecture, we will walk through the implementation of Byte Pair Encoding (BPE), a popular subword tokenization method. We will use a dataset of financial news headlines for this demonstration.\n",
    "\n",
    "## Dataset Preparation\n",
    "\n",
    "First, we need to load our dataset. We will use the `ashraq/financial-news` dataset from the Hugging Face Hub. We will randomly sample 100 records from this dataset for our demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813fdadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/yjlee/.cache/huggingface/datasets/ashraq___parquet/ashraq--financial-news-89d6ac597a40e29e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ashraq/financial-news\")\n",
    "texts = dataset[\"train\"].shuffle().select(range(1000))[\"headline\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8485fc0",
   "metadata": {},
   "source": [
    "## BPE Implementation\n",
    "\n",
    "Now, let's dive into the implementation of BPE.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "We start by initializing our vocabulary. We will format each word by separating its characters with spaces and appending a special end-of-word token `</w>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31158d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3713\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "\n",
    "def format_word(text, space_token=\"▁\"):\n",
    "    return \" \".join(list(text)) + \" \" + space_token\n",
    "\n",
    "\n",
    "def initialize_vocab(texts, lowercase=True):\n",
    "    vocab = {}\n",
    "\n",
    "    for text in texts:\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        all_words = text.split()\n",
    "        for word in all_words:\n",
    "            word = format_word(word)\n",
    "            vocab[word] = vocab.get(word, 0) + 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = initialize_vocab(texts)\n",
    "print(f\"Number of words: {len(vocab)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b73fe7c4",
   "metadata": {},
   "source": [
    "### Token Extraction\n",
    "\n",
    "Next, we extract all unique tokens from our vocabulary and count their frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dab07179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 61\n"
     ]
    }
   ],
   "source": [
    "def get_tokens_from_vocab(vocab):\n",
    "    tokens = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens[token] += freq\n",
    "        vocab_tokenization[\"\".join(word_tokens)] = word_tokens\n",
    "    return tokens, vocab_tokenization\n",
    "\n",
    "\n",
    "tokens, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "936eeb86",
   "metadata": {},
   "source": [
    "### Bigram Counts\n",
    "\n",
    "We then count the frequency of each bigram (pair of consecutive tokens) in our vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2327e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_counts(vocab):\n",
    "    pairs = {}\n",
    "    for word, count in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            pairs[pair] = pairs.get(pair, 0) + count\n",
    "    return pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74443b3e",
   "metadata": {},
   "source": [
    "### Merge Operations\n",
    "\n",
    "The core of BPE is a series of merge operations. In each operation, we find the most frequent bigram and merge it into a single token. We repeat this process for a specified number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce722936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 0: ('s', '▁') with count 1671\n",
      "All tokens: dict_keys(['t', 'r', 'a', 'n', 's', 'c', '▁', 'e', 'h', 'b', 's▁', 'y', '$', '0', '.', '7', ',', 'o', 'v', 'u', 'l', 'x', 'f', 'i', 'g', 'd', 'p', '(', ')', 'm', 'w', '8', '9', '-', '2', 'k', 'q', '1', '5', 'j', '3', '4', \"'\", '?', ':', 'z', '6', '%', '&', ';', '/', '#', '—', '…', '–', '|', '~', 'â', '+', '!', '€', '\"'])\n",
      "Number of tokens: 62\n",
      "Merge 1: ('i', 'n') with count 1009\n",
      "All tokens: dict_keys(['t', 'r', 'a', 'n', 's', 'c', '▁', 'e', 'h', 'b', 's▁', 'y', '$', '0', '.', '7', ',', 'o', 'v', 'u', 'l', 'x', 'f', 'i', 'g', 'd', 'p', 'in', '(', ')', 'm', 'w', '8', '9', '-', '2', 'k', 'q', '1', '5', 'j', '3', '4', \"'\", '?', ':', 'z', '6', '%', '&', ';', '/', '#', '—', '…', '–', '|', '~', 'â', '+', '!', '€', '\"'])\n",
      "Number of tokens: 63\n",
      "Merge 2: ('e', '▁') with count 888\n",
      "All tokens: dict_keys(['t', 'r', 'a', 'n', 's', 'c', '▁', 'e', 'h', 'b', 's▁', 'y', '$', '0', '.', '7', ',', 'o', 'v', 'u', 'e▁', 'l', 'x', 'f', 'i', 'g', 'd', 'p', 'in', '(', ')', 'm', 'w', '8', '9', '-', '2', 'k', 'q', '1', '5', 'j', '3', '4', \"'\", '?', ':', 'z', '6', '%', '&', ';', '/', '#', '—', '…', '–', '|', '~', 'â', '+', '!', '€', '\"'])\n",
      "Number of tokens: 64\n",
      "Merge 999: ('shor', 't▁') with count 6\n",
      "All tokens: dict_keys(['trans', 'ac', 't▁', 'tech▁', 'beats▁', 'by▁', '$0.0', '7', ',▁', 'on▁', 'revenue▁', 'col', 'la', 'br', 'x▁', 'f', 'ill', 's▁', 'a▁', 'big▁', 'ne', 'ed', 'has▁', 'po', 'ten', 'tial▁', 'for▁', 'investors▁', '(', 'dg', 'x,▁', 'ev', 'd', 'y,▁', 'cl', 'r', 'a', 'ff', 'x', ')▁', 'etf▁', 'op', 'tions▁', 'g', 'al', 'or', 'e▁', 'commod', 'ities▁', 'en', 'sc', 'o▁', 'b', 'ags▁', 'new▁', '8', '9', '-', 'day▁', 'con', 'trac', 'its▁', '7▁', 'star', 'ting▁', 'fe', 'uary▁', '2', '8▁', 'c', 'tr', 'al▁', 'bank▁', 'ance▁', 'sh', 'e', 'et▁', 'expan', 'si', 'on,▁', 'go', 'ver', 'n', 'ment▁', 'de', 'ic', 'and▁', 'gold▁', 'an▁', 'update▁', 'p', 'ir', 'us▁', 'in▁', 'ligh', 'of▁', 're', 'ent▁', 've', 'lo', 'ments▁', 'american▁', 'van', 'gu', 'ard▁', 'av', 'sur', 'pas', 'ses▁', 'q1▁', 'earnings▁', 'estimates▁', 'company▁', 'news▁', 'april▁', '0', '2,▁', '2015▁', '-▁', 'corpor', 'ate▁', 'sum', 'mar', 'y▁', 'prices▁', 'ste', 'ad', 'ahead▁', 'jo', 'bs▁', 'um', 'ber▁', 'st', 'an', 'ley▁', 'ck▁', 'to▁', 'buy▁', 'min', 'ity▁', 'ak', 'm', 't', 'd▁', 'produc', 'ts▁', '$', '3', '4', 'm▁', 'pro', 'as', 'li', 'ed▁', 'strong▁', 'analyst▁', 'blog▁', 'lin', 'k▁', 'mid', 'stre', 'am', \"'s▁\", 'l', 'c)▁', 'ceo▁', 'ar', 'ry▁', 'dav', 'is▁', 'q2▁', 'results▁', 'call▁', 'transcript▁', 'beat▁', 'stor', 'ex▁', 'v', 'this▁', 'se', 'on', '?▁', 'da▁', 'es▁', 'vol', 'un', 'ary▁', 'vic', 'ction▁', 'port', 'ing▁', \"here's▁\", 'why▁', 'you▁', 'should▁', 'o', 'id▁', 'bet', 'c▁', 'res', 'stock▁', 'now▁', 'k', 's', 'as▁', 'sou', 'thern▁', 'hits▁', '5', '2-', 'week▁', 'high▁', 'u.', 's.', 'ex', 'deal▁', 'natural▁', 'gas▁', 'age▁', 'fore', 'ca', 'st:▁', 'u', 'outloo', 'k,▁', 'but▁', 'inde', 'be', 'low▁', 'market▁', 'expect', 'ations▁', \"what's▁\", 'the▁', 'car', 'ds▁', 'ger▁', 'sk', 't)▁', 'z', 'el', 'le▁', 'ch', 'ks▁', 'up▁', '1', '6', 'pa', 'y', 'q3▁', 'co', 'corporation▁', 'over', 'oug', 'h', 't,▁', 'tale▁', 'tape▁', 'will▁', 'ea', 'st▁', 'we', 'banc', 'p▁', 'w', 'aga', 'joh', 'ch▁', 'dividend▁', 'income▁', 'track', 'er▁', 'reti', 'coun', 'cy', 'le', 's,▁', '..', '.', 'wh', 'er', '.▁', 'some▁', 'the', 'en▁', 'gro', 'w▁', 'be▁', 'bu', 'bl', 'inc.▁', 'ben', 'fit▁', 'from▁', 'mo', 'ile▁', 'growth▁', 'he', 'ph', 'one▁', 'ag', 'il', 'a)▁', '5.', '%▁', 'sinc', 'tic', 'highlights▁', 'b▁', 'opport', 'unity▁', 'sh▁', 'dr', 'ug', 'top▁', 'yield▁', 'stocks▁', 'financial▁', 'industry▁', 'cr', 'ude▁', 'oil▁', 'ase▁', 'ud', 'dis', 'ci', 'pl', 'ine▁', 'risk▁', 'if', 'th▁', 'street▁', 'fin', 'ni', 'i▁', 'in', 'e,▁', 'gr', 'ill▁', 'reports▁', 'ating▁', '10', 'q', 'systems▁', 'come▁', 'amid▁', 'challeng', 'per', 'io', 'pe', 'ak▁', 'energ', 'y:▁', 'game▁', 'wat', 'ers▁', '&▁', 'revenues▁', 'g▁', 'est', 'jan', 'ation▁', 'lead', 'cor', 'res▁', 'ti', 'ble▁', 'deb', 'ie', 'are▁', 'om', 'health▁', 'net▁', 'gains▁', '5▁', 'th', 'ings▁', 'before▁', 'ens▁', 'pr', 'im', ':▁', 'par', '▁', 'up', 'cur', '3▁', 'und', '10▁', 'fu', 'tu', 'arro', 'ly▁', 'high', ';▁', 'ou', 'se▁', 'holdings▁', 'so', 'ar▁', 'comm', 'upgrad', 'z▁', 's.▁', 'sell', 'off', 'how▁', 'ates▁', 'can▁', 'value▁', 'core▁', 'portfolio', 'canad', 'ian▁', 'sol', 'wn▁', 'qu', '2018▁', 'str', 'ct', 'ure▁', 'at', 'invest', 's;▁', 'ck', 'sid', 'ge▁', 'is', 'i', 'pac', 'ic▁', 'bre', 'out▁', 'und▁', 'sal', 'es,▁', 'boo', 'miss▁', 'end▁', 'year▁', 'ear', 'fund▁', 'ke▁', 'earning', 'sales▁', 'j', 'cramer', 'pla', 'tain', 'ul', 'can', 'improv', 'ement▁', 'vie', 'ws▁', 'us', 'one', 'data▁', 'ter▁', 'it▁', 'portfolio▁', 'reta', 'il▁', 'properti', 'america▁', '(r', 'sil', 'cre', '(s', 'focus:▁', '6%▁', 'ses', 'sion▁', 'ma', 'ta', 'ins▁', 'ors▁', 'asset▁', 'management,▁', 'buys▁', 'gre', 'inc,▁', 'bro', 'iel', '...▁', 'tw', 'with▁', 'bullish▁', 'insid', 'buy', 'am▁', 'win', 'ps▁', 'l▁', 'do', \"'\", 'goo', 'fr', 'id', \"'▁\", 'tim', 'year', 'trade▁', 'pu', 'te▁', 'could▁', 'pres', 'emerg', 'ing', 'notable▁', 'after▁', 'es', \"day's▁\", 'close▁', 'ust▁', 'first▁', 'u.s.▁', 'ation', 'lower▁', 'chin', 'ese▁', 'ch:▁', 'ig', 'ap', 'zacks.com▁', 'red▁', 'inc', 'lu', 'da', 'tur', 'turn', 'poin', 'app', 'ent', 'analy', 'arm', 'ho', '2016▁', 'q4▁', 'sl', 'watch▁', 'ur', 'tion', 'ign', 'dom', 's?▁', 'pi', 'ons▁', '2019▁', 'mi', 'contin', 'te', 'china▁', 'weekly▁', 'report▁', '/', '/1', 'air', 'misses▁', 'rev', 'llc▁', 'bri', 'sto', 'ib', 'compan', 'ati', 'ther', 'eu', 'ics▁', 'corp', 'sof', 'up:▁', 'at▁', 'su', 'ess▁', 'mac', 'au', 'gam', 'down▁', 'may▁', 'fol', 'low', 'cu', 'announ', 'ce', 'di', 'fall', 'q▁', 'h▁', 'no', 'ive▁', '#', 'prov', 'ial▁', 'met', 'als▁', 'pos', 'ition▁', 'sp', 'ite▁', 'to', 'line▁', 'ad▁', 'capital▁', 'markets▁', 'ann', 'investor▁', 'fer', 'ence▁', 'ide', 'show▁', 'el▁', 'yn', 'sees▁', 'higher▁', 's:▁', \"s'▁\", 'downgrad', '0▁', 'saf', 'nov', 'em', 's&p▁', '500▁', 'illi', 'companies▁', 'demand▁', 'you', 'highlights:▁', 'jun', 'all', 'and', 'ele', 'ant▁', 'inter', 'national▁', 'wor', '2%▁', 'her', 'trad', 'other▁', 'log', 'it', ')', 'mic', 'du', 'or▁', 'euro', 'equ', 'act', 'valu', 'time▁', 'ab', 'b)▁', 'ant', 'daily▁', 'por', 'laun', 'ches▁', 'ng', 'mov', 'industri', 'nes', 'pre', 'shares▁', 'es:▁', 'money▁', 'ct▁', 'quarter▁', '—…–', 'acqu', 'arge▁', 'um▁', 'global▁', '—…—…▁', '|', 'shar', 'rat', 'ren', 'home▁', '2.', 'd,▁', 'what▁', 'clas', 'action▁', 'sin', 'll', 'investment▁', 'rele', 'ases▁', 'mon', 'sy', 'me', 'rea', 'eric', 'itions▁', '2018', 'key▁', 'nu', 'ment', 'price▁', 'arg', 'r▁', 'rating▁', 'jum', '4▁', 'poised▁', 'ral', \"cramer's▁\", 'mad▁', 'gener', 'cont', 'end', 'az', 'ies▁', 'bi', 'pol', \"y's▁\", 'banks▁', 'co,▁', 'est▁', 'ishares▁', 'msci▁', 'marke', 'sells▁', 'set', '%', 'los', 'man▁', 'ver▁', 'gainers▁', '/▁', 'losers▁', '/2018)▁', 'sup', '8%▁', 'relative▁', 'strength', 'man', 'wall▁', 'et', 'john▁', 'king▁', 'open▁', 'positive▁', 'still▁', 'les▁', 'what', 'kes▁', 'econom', 'out', 'your▁', 'ip', '(p', 'ris', 'beat', 'incre', 'eps▁', 'gen', 'f▁', 'cover', 'ge', 'that▁', 'ys▁', 'fi', 'er,▁', 'ory▁', 'holdings,▁', 'ro', 'healthcare▁', 'ter', 'partners▁', 'duc', 'cast▁', 'sector▁', 'nasda', 'n▁', 'ke', 's)▁', '3%▁', 'ran', 'stock', 'moving▁', 'average▁', 'ros', 'wednes', 'y)▁', '9▁', 'sts▁', 'shor', 'rise▁', 'pharm', 'aceutic', 'jumps▁', 'review:▁', 'mun', 'bond▁', 'ish', 'ther▁', 'blo', 'prof', 'dat', 'a,▁', 'tion▁', 'tops▁', 'sign', 'wi', 'ack', 'fact', 'ra', 'ised▁', 'outlook▁', 'morgan▁', 'io▁', 'oc', 'ber', '2▁', 'all▁', 'e?▁', 'yiel', 'iv', 'ol', 'ate', 't:▁', 'consum', 'll▁', '—…', '……', 'iden', 'mber▁', '–', 'technolog', 'ues▁', 'oo', 'erger▁', 'ives▁', 'lat', 'cap', 'mat', 'form', 'servic', 'sho', 'services▁', 'etf', 'sch', 'wa', 'mod', '6▁', '$0.', 'sen', 'trust▁', '-year▁', 'ix▁', 'ri', 'energy▁', 'sa', 'hol', 'ay▁', 'med', 'ical▁', '1▁', 'gol', '~', 'guidance▁', 'inc▁', 'tre', 'short▁', 'inv', 'zacks▁', 'tal', 'com▁', 'ould▁', 'mor', '2017▁', 'group▁', 'oun', 'for', 'ia▁', 'declares▁', '0.', 'focu', 'back', 'â', 'ty▁', 'bank', 'ing,▁', ':', '00▁', 'com', 'gain', 'co▁', 'business▁', 'back▁', '1,▁', 'val', 'loo', '6.', 'management▁', 'ue▁', 'estimat', 'picks▁', 'equity▁', 'ight▁', 'pric', 'div', 'hold', 'revie', 'break', 'fa', '…', 'grow', 'add', 'bo', 'my▁', 'erg', '(g', 'al,▁', 'among▁', 'son▁', '20', 'iti', 'big', \"it's▁\", 'ion▁', 'tn', 'round', \"e's▁\", 'quar', 'pen', 'expe', 'expected▁', 'loss▁', 'etfs▁', 'war', 'research▁', 'ga', 'ce▁', '201', 'clos', 'decl', 'off▁', 'ld▁', 'anc', 'mis', 'ces▁', 'financ', 'enter', 'rising▁', 'ff▁', 'fro', 'set▁', 'dividen', 'entertain', 'earn', 'streng', 'earnings:▁', 'itive▁', 'igh', 'tran', '+', 'review', 'profit▁', 'resul', 'eutic', 'over▁', 'bio', '—', '&', 'our', 'tap', ',', '!', 'ong▁', 'fo', '€', 'ares▁', 'proper', 'bull', 'momentum▁', 'updat', 'atur', 'relat', 'transcrip', 'wed', 'resour', 'system', 'health', 'americ', 'pan', 'highlight', 'mone', 'ear▁', '\"', 'week', 's.com▁', 'capit', 'bon', 'sect', 'tech', 'af', 'men', 'ands▁', 'glob', 'manag', 'down', 'repor', 'highligh', 'resear', 'cks▁', 'ser', 'glo', 'dow', 'ily▁', 'table▁', 'eng', 'stem', 'ban', 'verage▁', 'ment,▁', 'estim', 's&', 'ings,▁', 'wee', 'indu', 'stri', 'fir', 'nation', 'amer', 'scrip'])\n",
      "Number of tokens: 1029\n"
     ]
    }
   ],
   "source": [
    "def merge_vocab(pair, vocab_in):\n",
    "    vocab_out = {}\n",
    "    bigram = re.escape(\" \".join(pair))\n",
    "    p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
    "    bytepair = \"\".join(pair)\n",
    "    for word in vocab_in:\n",
    "        w_out = p.sub(bytepair, word)\n",
    "        vocab_out[w_out] = vocab_in[word]\n",
    "    return vocab_out, (bigram, bytepair)\n",
    "\n",
    "\n",
    "def find_merges(vocab, tokens, num_merges, indices_to_print=[0, 1, 2]):\n",
    "    merges = []\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_bigram_counts(vocab)\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        best_count = pairs[best_pair]\n",
    "\n",
    "        vocab, (bigram, bytepair) = merge_vocab(best_pair, vocab)\n",
    "        merges.append((r\"(?<!\\S)\" + bigram + r\"(?!\\S)\", bytepair))\n",
    "        tokens, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "        if i in indices_to_print:\n",
    "            print(f\"Merge {i}: {best_pair} with count {best_count}\")\n",
    "            print(\"All tokens: {}\".format(tokens.keys()))\n",
    "            print(\"Number of tokens: {}\".format(len(tokens.keys())))\n",
    "\n",
    "    return vocab, tokens, merges, vocab_tokenization\n",
    "\n",
    "\n",
    "num_merges = 1000\n",
    "indices_to_print = [0, 1, 2, num_merges - 1]\n",
    "\n",
    "vocab, tokens, merges, vocab_tokenization = find_merges(\n",
    "    vocab, tokens, num_merges, indices_to_print\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d32d7197",
   "metadata": {},
   "source": [
    "## Encoding and Decoding\n",
    "\n",
    "### Decoding\n",
    "\n",
    "Decoding is straightforward. We simply concatenate all the tokens together and remove the stop token `</w>`. For example, if the encoded sequence is [`the</w>`, `high`, `est</w>`, `moun`, `tain</w>`], the decoded sequence is `the highest mountain`.\n",
    "\n",
    "### Encoding\n",
    "\n",
    "Encoding is a bit more complex. For a given sentence, we need to find the longest token in our vocabulary that is a subword of each word in the sentence. If no such token exists, we replace the word with an unknown token `</u>`. This process is computationally expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7951c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_token_length(token, space_token=\"▁\"):\n",
    "    space_token_len = len(space_token)\n",
    "    if token[-space_token_len:] == space_token:\n",
    "        return len(token) - space_token_len + 1\n",
    "    else:\n",
    "        return len(token)\n",
    "\n",
    "\n",
    "def encode_word(string, sorted_tokens, unknown_token=\"</u>\"):\n",
    "    if string == \"\":\n",
    "        return []\n",
    "    sorted_tokens = sorted_tokens.copy()\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    string_tokens = []\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        token = sorted_tokens[i]\n",
    "        token_reg = re.escape(token.replace(\".\", \"[.]\"))\n",
    "\n",
    "        matched_positions = [\n",
    "            (m.start(0), m.end(0)) for m in re.finditer(token_reg, string)\n",
    "        ]\n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "        substring_end_positions = [\n",
    "            matched_position[0] for matched_position in matched_positions\n",
    "        ]\n",
    "\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            string_tokens += encode_word(\n",
    "                string=substring,\n",
    "                sorted_tokens=sorted_tokens[i + 1 :],\n",
    "                unknown_token=unknown_token,\n",
    "            )\n",
    "            string_tokens += [token]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += encode_word(\n",
    "            string=remaining_substring,\n",
    "            sorted_tokens=sorted_tokens[i + 1 :],\n",
    "            unknown_token=unknown_token,\n",
    "        )\n",
    "        break\n",
    "    return string_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "774d4ed9",
   "metadata": {},
   "source": [
    "We can now use this function to encode a given word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb751111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokenization(word_given, sorted_tokens, vocab_tokenization):\n",
    "    print(\"Tokenizing word: {}...\".format(word_given))\n",
    "    if word_given in vocab_tokenization:\n",
    "        print(\"Tokenization of the known word:\")\n",
    "        print(vocab_tokenization[word_given])\n",
    "        print(\"Tokenization treating the known word as unknown:\")\n",
    "        print(\n",
    "            encode_word(\n",
    "                string=word_given, sorted_tokens=sorted_tokens, unknown_token=\"</u>\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"Tokenizating of the unknown word:\")\n",
    "        print(\n",
    "            encode_word(\n",
    "                string=word_given, sorted_tokens=sorted_tokens, unknown_token=\"</u>\"\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d64ab0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing word: investors▁...\n",
      "Tokenization of the known word:\n",
      "['investors▁']\n",
      "Tokenization treating the known word as unknown:\n",
      "['investors▁']\n",
      "Tokenizing word: dogecoin▁...\n",
      "Tokenizating of the unknown word:\n",
      "['do', 'ge', 'co', 'in▁']\n"
     ]
    }
   ],
   "source": [
    "# Sort tokens by length in descending order\n",
    "sorted_tokens = sorted(tokens.keys(), key=len, reverse=True)\n",
    "\n",
    "word_given_known = \"investors▁\"\n",
    "\n",
    "print_tokenization(word_given_known, sorted_tokens, vocab_tokenization)\n",
    "\n",
    "word_given_unknown = \"dogecoin▁\"\n",
    "\n",
    "print_tokenization(word_given_unknown, sorted_tokens, vocab_tokenization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d316f27f",
   "metadata": {},
   "source": [
    "Finally, we can use our encoding function to tokenize an entire sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8726dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['investment▁', 'opport', 'un', 'ities▁', 'in▁', 'the▁', 'company▁']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text, space_token=\"▁\"):\n",
    "    text = re.sub(\"\\s+\", \" \", text.lower())\n",
    "    words = [word + space_token for word in text.split(\" \")]\n",
    "    encoded_words = [\n",
    "        encode_word(word, sorted_tokens, unknown_token=\"</u>\") for word in words\n",
    "    ]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "tokenized_text = tokenize(\"Investment opportunities in the company\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f68216a",
   "metadata": {},
   "source": [
    "That's it! You have now implemented BPE from scratch. This should give you a good understanding of how subword tokenization works in practice.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93ad889c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
