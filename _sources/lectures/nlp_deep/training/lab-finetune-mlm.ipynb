{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Finetuining a MLM\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab session, we are going to take the previously pretrained BERT model that we created using the Masked Language Modeling (MLM) objective and finetune it for a downstream task: sentiment analysis. Sentiment analysis is a common Natural Language Processing (NLP) task that involves determining the sentiment expressed in a piece of text, usually positive, negative, or neutral.\n",
    "\n",
    "While pretraining, the model learned to understand the syntax and semantics of the language. Now, during finetuning, we will teach the model how to apply this understanding to a specific task.\n",
    "\n",
    "Here's a brief overview of the procedure we will follow:\n",
    "\n",
    "1. **Preparing the Environment:** We'll start by setting up the necessary libraries and tools in our Google Colab environment.\n",
    "\n",
    "2. **Loading the Pretrained Model and Tokenizer:** Next, we will load our pretrained BERT model and the tokenizer we used during pretraining.\n",
    "\n",
    "3. **Preparing the Dataset:** We will prepare our sentiment analysis dataset by loading it, cleaning the text, and splitting it into training and validation sets.\n",
    "\n",
    "4. **Tokenizing and Formatting the Data:** We'll then tokenize our data using the previously trained tokenizer and format it for the sentiment analysis task.\n",
    "\n",
    "5. **Modifying the Model for Sentiment Analysis:** Before starting the finetuning process, we need to modify our BERT model to make it suitable for sentiment analysis. We'll do this by adding a new classification layer on top of BERT.\n",
    "\n",
    "6. **Training the Model:** Once our model is ready, we'll train it on our sentiment analysis dataset using the Hugging Face's `Trainer` class.\n",
    "\n",
    "7. **Validating the Model:** After training, we will validate our model's performance on the validation dataset.\n",
    "\n",
    "8. **Saving and Loading the Model:** Finally, we'll save our finetuned model for future use and learn how to load it from disk.\n",
    "\n",
    "Remember, this whole process is happening within the Google Colab environment, which provides a convenient and powerful platform for running our model training tasks.\n",
    "\n",
    "Now, let's dive in and start finetuning our BERT model for sentiment analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Environment\n",
    "\n",
    "Before we start, we need to set up our working environment in Google Colab. This involves installing necessary libraries and setting up GPU for our computations.\n",
    "\n",
    "Here are the steps:\n",
    "\n",
    "1. **Check Python Version**\n",
    "\n",
    "Google Colab should come with Python 3.7 or later pre-installed. You can verify this by running:\n",
    "\n",
    "```python\n",
    "!python --version\n",
    "```\n",
    "\n",
    "2. **Installing Hugging Face Transformers Library**\n",
    "\n",
    "We will use the Hugging Face Transformers library for our BERT model and tokenizer. To install it, run:\n",
    "\n",
    "```python\n",
    "!pip install transformers\n",
    "```\n",
    "\n",
    "3. **Installing the Hugging Face Datasets Library**\n",
    "\n",
    "We also need the Hugging Face Datasets library for handling our dataset. You can install it by running:\n",
    "\n",
    "```python\n",
    "!pip install datasets\n",
    "```\n",
    "\n",
    "4. **Setting up the GPU**\n",
    "\n",
    "Google Colab provides access to a free GPU that we can use to train our models faster. To set up the GPU, follow these steps:\n",
    "\n",
    "- Click on the `Runtime` tab in the Google Colab menu.\n",
    "- Select `Change runtime type`.\n",
    "- In the pop-up window, under `Hardware accelerator`, choose `GPU` and click `Save`.\n",
    "\n",
    "After these steps, your Google Colab environment is ready for training and finetuning models.\n",
    "\n",
    "Remember to import necessary libraries before starting the next steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BertForMaskedLM,\n",
    "    BertForSequenceClassification,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries will help us in loading the model, the tokenizer, training the model, loading the dataset and evaluating our model's performance.\n",
    "\n",
    "## Loading the Pretrained Model and Tokenizer\n",
    "\n",
    "Now that our environment is prepared, we can load our pretrained BERT model and the tokenizer we trained along with it. These components have been saved from the previous steps we completed.\n",
    "\n",
    "Here's how we can do it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained BERT model\n",
    "model = BertForMaskedLM.from_pretrained(\"../tmp//bert_base_uncased\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../tmp/bert_base_uncased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, `BertForMaskedLM.from_pretrained` and `BertTokenizerFast.from_pretrained` functions are used to load the model and tokenizer, respectively. We specify the path to the directory where we saved the pretrained model and tokenizer, and these functions handle the rest.\n",
    "\n",
    "Now that we have loaded the pretrained BERT model and tokenizer, we can move forward to prepare our sentiment analysis dataset.\n",
    "\n",
    "## Preparing the Dataset\n",
    "\n",
    "For the task of sentiment analysis, we will be using the IMDb dataset which contains movie reviews along with their sentiment polarity (positive/negative). This dataset is widely used for sentiment analysis tasks and is available through the Hugging Face Datasets library.\n",
    "\n",
    "Here's how to load and prepare the IMDb dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/yj.lee/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 854.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Print out the number of items in the train and test sets\n",
    "print(f\"Number of training examples: {len(dataset['train'])}\")\n",
    "print(f\"Number of testing examples: {len(dataset['test'])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will load the IMDb dataset and split it into a training set and a test set.\n",
    "\n",
    "Next, we split our training data further into training and validation sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into train and validation\n",
    "train_dataset = dataset[\"train\"].train_test_split(test_size=0.1)[\"train\"]\n",
    "valid_dataset = dataset[\"train\"].train_test_split(test_size=0.1)[\"test\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the `train_test_split` method to split the original training data into a training set (90%) and a validation set (10%). The testing dataset will remain the same.\n",
    "\n",
    "In the next section, we'll preprocess and tokenize this dataset, preparing it for training our sentiment analysis model.\n",
    "\n",
    "## Tokenizing and Formatting the Data\n",
    "\n",
    "Now that we have loaded our data, the next step is to preprocess it. This includes tokenizing the text and formatting it so that it can be used as input to our BERT model. As we are performing a sentiment analysis task, we'll also need to format the labels.\n",
    "\n",
    "Here's how we can tokenize and format our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/yj.lee/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-82520e7875fc69ed.arrow\n"
     ]
    }
   ],
   "source": [
    "# Define a function to tokenize the data\n",
    "def tokenize_function(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# Tokenize the data\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataset to output PyTorch tensors\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "valid_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we first define a `tokenize_function` that tokenizes the text, pads or truncates it to a maximum length, and returns the result. We then map this function to our train, validation, and test datasets to tokenize them.\n",
    "\n",
    "Finally, we set the format of our datasets to output PyTorch tensors. We specify the columns that we want in the output: `input_ids`, `attention_mask`, and `label`.\n",
    "\n",
    "Now our data is ready to be used for training our sentiment analysis model. In the next section, we'll modify our BERT model to make it suitable for this task.\n",
    "\n",
    "## Modifying the Model for Sentiment Analysis\n",
    "\n",
    "We've pretrained a BERT model using a Masked Language Model (MLM) objective and loaded it into memory. However, this model isn't suitable for sentiment analysis as it is. We need to add a classifier on top of the base BERT model to classify the sentiments.\n",
    "\n",
    "First, let's resize the token embeddings of our model. This is necessary because the number of tokens in our pretraining and finetuning tasks might be different:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30000, 768, padding_idx=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll modify the model for sentiment analysis. The Hugging Face Transformers library makes this easy with the `BertForSequenceClassification` model. This model is the same as the standard BERT model, but with an added classification layer on top. Here's how to modify the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../tmp/bert_base_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tmp/bert_base_uncased and are newly initialized: ['classifier.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model with a sequence classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"../tmp/bert_base_uncased\", num_labels=2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we're using the `from_pretrained` method to load our pretrained model. We also specify `num_labels=2` because our sentiment analysis task has two classes: positive and negative.\n",
    "\n",
    "Our model is now ready to be trained for sentiment analysis. In the next section, we'll define the training arguments and start the training process.\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "Now that our model is set up for sentiment analysis and our data is prepared, we can start training. For this, we'll use the `Trainer` class from the Hugging Face Transformers library.\n",
    "\n",
    "Before we can start training, we need to define some training arguments using the `TrainingArguments` class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../tmp/results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"../tmp/logs\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `output_dir` is the directory where the training outputs will be saved, `num_train_epochs` is the number of training epochs, `per_device_train_batch_size` and `per_device_eval_batch_size` are the batch sizes for training and evaluation respectively, `warmup_steps` is the number of warm-up steps, and `weight_decay` is the weight decay. We also set a logging directory to store logs.\n",
    "\n",
    "With these training arguments defined, we can now create our `Trainer` and start training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yj.lee/.local/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='704' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/704 00:06 < 15:19, 0.76 it/s, Epoch 0.02/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=704, training_loss=0.45673076672987506, metrics={'train_runtime': 932.9201, 'train_samples_per_second': 48.236, 'train_steps_per_second': 0.755, 'total_flos': 1.18399974912e+16, 'train_loss': 0.45673076672987506, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function for computing the metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we first define a `compute_metrics` function that will be used to compute the accuracy of our model. We then initialize our `Trainer` with our model, the training arguments, the training and validation datasets, and the `compute_metrics` function. Finally, we start training with the `train` method.\n",
    "\n",
    "This completes the training step. Our model is now finetuned for sentiment analysis. In the next section, we'll validate our model's performance on the validation dataset.\n",
    "\n",
    "## Validating the Model\n",
    "\n",
    "Now that our model is trained, we need to evaluate its performance on unseen data. We will use our validation dataset for this purpose. We'll leverage the `Trainer`'s `evaluate` method, which takes care of the entire evaluation process.\n",
    "\n",
    "Here's how you can do it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss: 0.2014\n",
      "eval_accuracy: 0.9272\n",
      "eval_runtime: 19.4237\n",
      "eval_samples_per_second: 128.7090\n",
      "eval_steps_per_second: 2.0590\n",
      "epoch: 2.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluation_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "for key, value in evaluation_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, the `evaluate` method returns a dictionary with the evaluation results. We then print out these results. The dictionary usually contains the loss and any metrics that we specified in the `compute_metrics` function during training.\n",
    "\n",
    "It's always a good idea to inspect these results to understand how well our model is doing. Remember that a model that performs well on the training data but poorly on the validation data is probably overfitting. Conversely, a model that performs poorly on both is likely underfitting.\n",
    "\n",
    "This completes the validation step. In the final section, we'll save our finetuned model for future use and learn how to load it from disk.\n",
    "\n",
    "## Saving and Loading the Model\n",
    "\n",
    "After finetuning our model and validating its performance, we'll want to save it for future use. We might also want to load it back into memory at a later time. In this section, we'll see how to do both of these tasks.\n",
    "\n",
    "Here's how you can save your model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"../tmp/sentiment_analysis_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, `save_pretrained` is a method that saves both the model's weights and its configuration. We specify the directory where we want to save the model.\n",
    "\n",
    "Now, suppose you want to load this model at a later time. You can do this as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(\"../tmp/sentiment_analysis_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we use the `from_pretrained` method, as we did before, to load our model. We specify the path to the directory where we saved our model, and this method takes care of the rest.\n",
    "\n",
    "Remember that when you load a model, you also need to load the corresponding tokenizer. You can do this the same way you loaded the tokenizer before.\n",
    "\n",
    "This completes our guide on finetuning a BERT model for sentiment analysis using Google Colab. You now know how to prepare your environment, load a pretrained model and tokenizer, prepare your dataset, tokenize and format your data, modify your model for sentiment analysis, train your model, validate it, and finally, save and load it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture-_dERj_9R-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
