# Introduction

This lecture aims to provide a comprehensive introduction to Large Language Models (LLMs), focusing on their underlying architecture, mechanics, and broad application scope. Emphasizing the transformer neural network architecture that serves as the backbone for LLMs, the lecture will explore how these models learn the statistical relationships between words and generate coherent text. Use-cases across various sectors like virtual assistance, healthcare, financial services, and education will also be highlighted.

```{slide} https://docs.google.com/presentation/d/1ks3-gBIdiYH_OKI9z1Goc7-XBr8edST7_7QRNTDSaEY

```

## Sections

```{tableofcontents}

```
