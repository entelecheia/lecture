{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## General Concept\n",
    "\n",
    "Language models are computational models that assign probabilities to sequences of words or predict the next word in a sequence. They play a crucial role in various natural language processing tasks, such as:\n",
    "\n",
    "- Speech recognition\n",
    "- Machine translation\n",
    "- Spelling and grammar correction\n",
    "- Text generation\n",
    "\n",
    "Language models capture the structure and patterns within a language, allowing them to estimate how likely a given word or phrase is to appear in a specific context. There are different types of language models, such as:\n",
    "\n",
    "- N-gram models, which rely on sequences of words\n",
    "- Neural language models, which utilize deep learning techniques to understand and generate text\n",
    "\n",
    "Understanding and developing effective language models is essential for improving the performance of natural language processing systems.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59377615",
   "metadata": {},
   "source": [
    "## Why do we need language models?\n",
    "\n",
    "Language models are essential for various reasons in natural language processing tasks:\n",
    "\n",
    "1. **Disambiguation**: Language models help in resolving ambiguities in speech recognition and text processing, as they can assign probabilities to different interpretations based on the context, selecting the most likely one.\n",
    "\n",
    "2. **Machine Translation**: In translating text from one language to another, language models can help choose the most fluent and accurate translations by estimating the likelihood of word sequences in the target language.\n",
    "\n",
    "3. **Text Generation**: Language models can generate coherent and contextually relevant text, which is useful for tasks like summarization, question-answering, and dialogue systems.\n",
    "\n",
    "4. **Spelling and Grammar Correction**: Language models can identify and correct errors in written text by comparing the probabilities of different word sequences and suggesting more likely alternatives.\n",
    "\n",
    "5. **Assistive Technologies**: Language models are crucial for augmentative and alternative communication (AAC) systems, as they can predict and suggest likely words or phrases for users with speech or language impairments, making communication more efficient.\n",
    "\n",
    "Overall, language models play a critical role in improving the performance and accuracy of natural language processing systems by capturing the structure, patterns, and nuances of a language.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8469ff40",
   "metadata": {},
   "source": [
    "## N-gram Language Models\n",
    "\n",
    "N-gram language models are a simple yet powerful approach to language modeling. They predict the next word in a sequence based on the previous n-1 words, where n is the order of the model. The main types of n-grams are:\n",
    "\n",
    "- **Unigram**: Considers only a single word, ignoring context (n=1).\n",
    "- **Bigram**: Considers a sequence of two words (n=2).\n",
    "- **Trigram**: Considers a sequence of three words (n=3).\n",
    "- **Higher-order n-grams**: Considers longer sequences of words (n>3).\n",
    "\n",
    "In n-gram models, the probability of a word depends only on the (n-1) previous words. This simplification is known as the Markov assumption. To estimate the probabilities, n-gram models rely on counting the occurrences of n-grams in a large corpus and normalizing the counts.\n",
    "\n",
    "Advantages of n-gram models:\n",
    "- Relatively simple and easy to implement.\n",
    "- Efficient in terms of computation and memory usage.\n",
    "\n",
    "Limitations of n-gram models:\n",
    "- Unable to capture long-range dependencies between words.\n",
    "- Sensitive to data sparsity issues, as some n-grams may not appear in the training corpus.\n",
    "\n",
    "Despite their limitations, n-gram models serve as a foundational tool for understanding language modeling concepts and are still used in various NLP applications.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6cc96c0",
   "metadata": {},
   "source": [
    "## N-Grams and Probability Estimation\n",
    "\n",
    "N-grams can be used to estimate the probability of a word given a history (P(w|h)). For instance, if the history h is \"I like to eat\", we might want to estimate the probability of the next word being \"pizza\" (P(pizza|I like to eat)).\n",
    "\n",
    "### Estimating Probabilities from Counts\n",
    "We can estimate this probability by counting the occurrences of the history followed by the target word in a large corpus and dividing by the total count of the history:\n",
    "\n",
    "$$\n",
    "P(\\text{pizza}|\\text{I like to eat}) = \\frac{C(\\text{I like to eat pizza})}{C(\\text{I like to eat})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{먹습니다}|\\text{저는 김치를}) = \\frac{C(\\text{저는 김치를 먹습니다})}{C(\\text{저는 김치를})}\n",
    "$$\n",
    "\n",
    "However, even with a large corpus, it's often not sufficient to provide good estimates due to the creative nature of language and the possibility of unseen word sequences.\n",
    "\n",
    "### Bigram Model\n",
    "To tackle this issue, we can use the bigram model, which approximates the probability of a word given its entire history by considering only the preceding word:\n",
    "\n",
    "$$\n",
    "P(\\text{pizza}|\\text{I like to eat}) \\approx P(\\text{pizza}|\\text{eat})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{먹습니다}|\\text{저는 김치를}) \\approx P(\\text{먹습니다}|\\text{김치를})\n",
    "$$\n",
    "\n",
    "This simplification allows us to estimate probabilities more reliably, but it may not capture longer context dependencies. Nevertheless, n-grams serve as a foundational tool for understanding language modeling concepts and can be useful in various NLP applications.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0a11e1f",
   "metadata": {},
   "source": [
    "### Estimating Joint Probabilities of Word Sequences\n",
    "\n",
    "To estimate the joint probability of an entire sequence of words, such as \"The cat sat on the mat\", we can decompose this probability using the chain rule of probability:\n",
    "\n",
    "$$\n",
    "P(w_1:n) = P(w_1)P(w_2|w_1)P(w_3|w_1:2)...P(w_n|w_1:n−1) = \\prod_{k=1}^n P(w_k|w_1:k−1)\n",
    "$$\n",
    "\n",
    "This decomposition shows the link between computing the joint probability of a sequence and computing the conditional probability of a word given previous words. However, it doesn't really seem to help us! Computing the exact probability of a word given a long sequence of preceding words (e.g., P(wn|w1:n−1)) is challenging because language is creative, and any particular context might have never occurred before.\n",
    "\n",
    "### Example\n",
    "\n",
    "For the sequence \"The cat sat on the mat\", we can decompose the joint probability as follows:\n",
    "\n",
    "$$\n",
    "P(\\text{The, cat, sat, on, the, mat}) = P(\\text{The})P(\\text{cat}|\\text{The})P(\\text{sat}|\\text{The, cat})...P(\\text{mat}|\\text{The, cat, sat, on, the})\n",
    "$$\n",
    "\n",
    "Estimating each conditional probability using counts from a large corpus is not feasible since many long sequences might never have occurred before.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59fb5f1c",
   "metadata": {},
   "source": [
    "### N-gram Models and Markov Assumption\n",
    "\n",
    "N-gram models are used to predict the probability of a word given a fixed number of preceding words. The assumption that the probability of a word depends only on a limited number of previous words is called the Markov assumption.\n",
    "\n",
    "For a bigram model (N=2), the approximation is:\n",
    "\n",
    "$$\n",
    "P(w_n|w_1:n−1) ≈ P(w_n|w_{n−1})\n",
    "$$\n",
    "\n",
    "For an n-gram model with size N, the approximation is:\n",
    "\n",
    "$$\n",
    "P(w_n|w_1:n−1) ≈ P(w_n|w_{n−N+1:n−1})\n",
    "$$\n",
    "\n",
    "Given the n-gram assumption for the probability of an individual word, we can compute the probability of a complete word sequence as:\n",
    "\n",
    "$$\n",
    "P(w_1:n) ≈ \\prod_{k=1}^n P(w_k|w_{k−N+1:k−1})\n",
    "$$\n",
    "\n",
    "**Example**\n",
    "\n",
    "For the trigram model (N=3), the approximation for the sequence \"The cat sat on the mat\" is:\n",
    "\n",
    "$$\n",
    "P(\\text{The, cat, sat, on, the, mat}) ≈ P(\\text{The})P(\\text{cat}|\\text{The})P(\\text{sat}|\\text{The, cat})P(\\text{on}|\\text{cat, sat})P(\\text{the}|\\text{sat, on})P(\\text{mat}|\\text{on, the})\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a30e87ee",
   "metadata": {},
   "source": [
    "### Stock Prices and Markov Assumption\n",
    "\n",
    "The Markov assumption can be applied to model stock prices as well. In finance, the Markov assumption is often used to represent the idea that future stock prices depend only on the current price and a limited number of past prices, rather than the entire price history.\n",
    "\n",
    "The relationship between stock prices and the Markov assumption can be understood as follows:\n",
    "\n",
    "1. **Memoryless Property**: The Markov assumption implies that the stock price at a certain time is only influenced by a fixed number of previous time steps. This means that the future price movement doesn't depend on the entire history but only on the recent past. This property is also known as the memoryless property of Markov models.\n",
    "\n",
    "2. **Simplifying Complex Systems**: Stock prices are affected by a vast number of factors, including market trends, company performance, global events, and investor sentiment. By applying the Markov assumption, we can simplify the modeling of stock prices by focusing only on the most recent price changes, which are assumed to capture relevant information.\n",
    "\n",
    "3. **Prediction and Analysis**: Using the Markov assumption in financial models allows us to predict and analyze stock price movements. For example, we can create Markov models to estimate the probabilities of stock price changes, which can be useful for trading strategies, risk management, and portfolio optimization.\n",
    "\n",
    "It's important to note that the Markov assumption is a simplification and may not always accurately represent the complexities of the stock market. However, it serves as a useful tool in modeling and analyzing stock prices.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2c898bb",
   "metadata": {},
   "source": [
    "## Estimating Bigram or N-gram Probabilities using Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "To estimate the probabilities for bigrams or n-grams, we use maximum likelihood estimation (MLE) by counting the occurrences in a corpus and normalizing the counts.\n",
    "\n",
    "MLE, or Maximum Likelihood Estimation, is a statistical method used for estimating the parameters of a probability distribution or a model. It works by finding the parameter values that maximize the likelihood of the observed data under the given model. In other words, MLE selects the parameters that make the observed data most probable.\n",
    "\n",
    "For example, if we have a dataset of coin tosses (heads and tails), and we want to estimate the probability of getting heads, we can use MLE to find the parameter value that makes the observed sequence of coin tosses most likely. This is usually done by calculating the ratio of the number of heads to the total number of tosses.\n",
    "\n",
    "### Bigram Probability Calculation\n",
    "\n",
    "Compute the bigram probability of a word $w_n$ given the previous word $w_{n-1}$:\n",
    "\n",
    "$$ P(w_n|w_{n−1}) = C(w_{n−1}w_n) / C(w_{n−1}) $$\n",
    "\n",
    "where $C(w_{n−1} w_n)$ is the count of the bigram $w_{n−1}w_n$ and $C(w_{n−1})$ is the count of the unigram $w_{n−1}$.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Consider a mini-corpus with three sentences:\n",
    "\n",
    "1. `<s> I am Sam </s>`\n",
    "2. `<s> Sam I am </s>`\n",
    "3. `<s> I do not like green eggs and ham </s>`\n",
    "\n",
    "Here are the bigram probabilities for some pairs in this corpus:\n",
    "\n",
    "| Bigram         | Probability |\n",
    "|----------------|-------------|\n",
    "| P(I\\|`<s>`)       | 2/3         |\n",
    "| P(Sam\\|`<s>`)     | 1/3         |\n",
    "| P(am\\|I)        | 2/3         |\n",
    "| P(`</s>`\\|Sam)    | 1/2         |\n",
    "| P(Sam\\|am)      | 1/2         |\n",
    "| P(do\\|I)        | 1/3         |\n",
    "\n",
    "For general MLE n-gram parameter estimation:\n",
    "\n",
    "$$ P(w_n|w_{n−N+1:n−1}) = C(w_{n−N+1:n−1} w_n) / C(w_{n−N+1:n−1}) $$\n",
    "\n",
    "Bigram probabilities capture various linguistic phenomena, such as syntax, task-specific patterns, and cultural preferences.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af0ed12d",
   "metadata": {},
   "source": [
    "### Practical Issues in N-gram Models\n",
    "\n",
    "1. **Higher-order n-grams**: In practice, trigram (conditioning on the previous two words), 4-gram, or even 5-gram models are more common when there is sufficient training data. For larger n-grams, extra contexts are needed to the left and right of the sentence end (e.g., P(I|`<s><s>`) for trigrams).\n",
    "\n",
    "2. **Log probabilities**: Since multiplying probabilities can lead to numerical underflow, it's better to represent and compute language model probabilities in log format. Adding log probabilities is equivalent to multiplying probabilities in linear space. Convert back to probabilities when needed by taking the exponential of the log probability:\n",
    "\n",
    "$$   p_1 × p_2 × p_3 × p_4 = \\exp(\\log{p_1} + \\log{p_2} + \\log{p_3} + \\log{p_4}) $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5da0611",
   "metadata": {},
   "source": [
    "### Examples of MLE\n",
    "\n",
    "Here's a practical example of MLE using Python. We'll estimate the parameter p of a Bernoulli distribution (coin toss) based on some observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a90560c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE for p (probability of heads): 0.65\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Observed data (1 for heads, 0 for tails)\n",
    "data = np.array([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1])\n",
    "\n",
    "# Calculate MLE for p (probability of heads)\n",
    "mle_p = np.mean(data)\n",
    "\n",
    "print(\"MLE for p (probability of heads):\", mle_p)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "502fba90",
   "metadata": {},
   "source": [
    "In this example, we have a sequence of 20 coin tosses, where 1 represents heads, and 0 represents tails. To estimate the probability of getting heads (p) using MLE, we simply calculate the mean of the observed data, which is the ratio of the number of heads to the total number of tosses.\n",
    "\n",
    "Here's another example, this time estimating the mean (mu) and standard deviation (sigma) of a Gaussian distribution given some observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c3965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE for mu (mean): 3.4899999999999998\n",
      "MLE for sigma (standard deviation): 0.6441273166075167\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Observed data\n",
    "data = np.array([2.5, 3.2, 4.1, 3.7, 2.8, 3.9, 4.5, 3.1, 2.9, 4.2])\n",
    "\n",
    "# Calculate MLE for mu (mean) and sigma (standard deviation)\n",
    "mle_mu = np.mean(data)\n",
    "mle_sigma = np.std(data)\n",
    "\n",
    "print(\"MLE for mu (mean):\", mle_mu)\n",
    "print(\"MLE for sigma (standard deviation):\", mle_sigma)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7f999e0",
   "metadata": {},
   "source": [
    "## How to generate n-grams using NLTK\n",
    "\n",
    "Here's an example of how to generate n-grams using the Natural Language Toolkit (NLTK) library in Python.\n",
    "\n",
    "First, make sure to install NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7861292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk) (2022.10.31)\n",
      "Installing collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.2.0 nltk-3.8.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3d038f5",
   "metadata": {},
   "source": [
    "Now, let's generate bigrams, trigrams, and 4-grams using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27982ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yjlee/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams:\n",
      "[('I', 'love'), ('love', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'and'), ('and', 'machine'), ('machine', 'learning'), ('learning', '.')]\n",
      "\n",
      "Trigrams:\n",
      "[('I', 'love', 'natural'), ('love', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'and'), ('processing', 'and', 'machine'), ('and', 'machine', 'learning'), ('machine', 'learning', '.')]\n",
      "\n",
      "4-grams:\n",
      "[('I', 'love', 'natural', 'language'), ('love', 'natural', 'language', 'processing'), ('natural', 'language', 'processing', 'and'), ('language', 'processing', 'and', 'machine'), ('processing', 'and', 'machine', 'learning'), ('and', 'machine', 'learning', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Download the punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"I love natural language processing and machine learning.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(\"Bigrams:\")\n",
    "print(bigrams)\n",
    "\n",
    "# Generate trigrams\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "print(\"\\nTrigrams:\")\n",
    "print(trigrams)\n",
    "\n",
    "# Generate 4-grams (quadgrams)\n",
    "quadgrams = list(ngrams(tokens, 4))\n",
    "print(\"\\n4-grams:\")\n",
    "print(quadgrams)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "256c6463",
   "metadata": {},
   "source": [
    "## Evaluating Language Models\n",
    "\n",
    "Evaluating language models can be done in two main ways: extrinsic evaluation and intrinsic evaluation.\n",
    "\n",
    "_Extrinsic evaluation_ is the process of embedding a language model in an application and measuring its performance improvement in that application. For instance, in speech recognition, we can compare two language models by running a speech recognizer with each model and determining which one produces more accurate transcriptions.\n",
    "\n",
    "However, running large NLP systems end-to-end can be resource-intensive. As an alternative, we can use _intrinsic evaluation_ metrics to quickly evaluate potential improvements in a language model. Intrinsic evaluation measures the quality of a model independent of any application.\n",
    "\n",
    "To perform an intrinsic evaluation, we need a test set and a training set. The training set is used to train the parameters of the n-gram model, while the test set measures the model's performance. The better model is the one that assigns a higher probability to the test set, meaning it more accurately predicts the test set.\n",
    "\n",
    "It's crucial not to include test sentences in the training set, as this would introduce bias and lead to inaccurate probability-based metrics. Sometimes, we might use a test set so frequently that we tune to its characteristics. In such cases, a fresh test set, called the _development test set_ or _devset_, should be used.\n",
    "\n",
    "To divide data into training, development, and test sets, a common practice is to allocate 80% for training, 10% for development, and 10% for testing. Test data can either be taken from a continuous sequence within the corpus or be selected as smaller \"stripes\" from random parts of the corpus combined into a test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbcb85ef",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "Perplexity is a measure used to evaluate the performance of a language model. It quantifies how well a model predicts the probability distribution of a test set. A lower perplexity value indicates that the model is more accurate in predicting the test set.\n",
    "\n",
    "Perplexity can be understood as the weighted average branching factor of a language model. In other words, it represents the average number of possible next words the model considers at each step while generating a sentence. Lower perplexity means the model has fewer choices to consider, resulting in better prediction accuracy.\n",
    "\n",
    "The perplexity (PP) of a language model on a test set can be calculated using the following equation:\n",
    "\n",
    "$$\n",
    "PP(W) = P(w_1, w_2, ..., w_N)^(-1/N)\n",
    "$$\n",
    "\n",
    "Here, W is the sequence of words in the test set, and N is the total number of words in the test set. $P(w_1, w_2, ..., w_N)$ is the joint probability of the entire word sequence in the test set.\n",
    "\n",
    "Since we usually work with log probabilities to avoid numerical underflow, we can rewrite the perplexity equation using log probabilities:\n",
    "\n",
    "$$\n",
    "PP(W) = P(w_1, w_2, ..., w_N)^(-1/N)\n",
    "$$\n",
    "\n",
    "In the context of n-gram models, we can compute the perplexity by replacing the joint probability with the product of conditional probabilities of each word given its (n-1) previous words:\n",
    "\n",
    "$$\n",
    "PP(W) = exp(-1/N * (log(P(w_1|w_0)) + log(P(w_2|w_1)) + ... + log(P(w_N|w_(N-1)))))\n",
    "$$\n",
    "\n",
    "**Example**\n",
    "\n",
    "Suppose we have a bigram model and the following test sentence: \"I like pizza\". The perplexity of the model on this test sentence can be calculated using the bigram probabilities as follows:\n",
    "\n",
    "1.  Compute the log probabilities: $\\log(P(\\text{I}|\\text{<s>}))$, $\\log(P(\\text{like}|\\text{I}))$, $\\log(P(\\text{pizza}|\\text{like}))$, and $\\log(P(\\text{<s>}|\\text{pizza}))$.\n",
    "2.  Add up the log probabilities.\n",
    "3.  Divide the sum by the number of words in the test sentence (including the start and end symbols): $-1/5 * \\text{sum of log probabilities}$.\n",
    "4.  Take the exponent of the result: $\\exp(-1/5 * \\text{sum of log probabilities})$.\n",
    "\n",
    "The perplexity value obtained will be a measure of how well the bigram model predicts the test sentence \"I like pizza\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60a3d47e",
   "metadata": {},
   "source": [
    "Here's how to implement perplexity manually using Python and NLTK. We'll use the Brown Corpus and train a trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a2f555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/yjlee/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "# Set n-gram order to 3 (trigram)\n",
    "n = 3\n",
    "\n",
    "# Divide the data into 80% training and 20% test\n",
    "train_size = int(len(brown.sents()) * 0.8)\n",
    "train_data = brown.sents()[:train_size]\n",
    "test_data = brown.sents()[train_size:]\n",
    "\n",
    "# Create n-grams and vocabulary from training data\n",
    "ngrams_counts = defaultdict(Counter)\n",
    "vocabulary = set()\n",
    "\n",
    "for sentence in train_data:\n",
    "    for gram in range(1, n + 1):\n",
    "        for ng in ngrams(sentence, gram, pad_left=True, pad_right=True):\n",
    "            ngrams_counts[gram][ng] += 1\n",
    "            vocabulary.update(ng)\n",
    "\n",
    "V = len(vocabulary)  # Vocabulary size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49a05f9c",
   "metadata": {},
   "source": [
    "Now, let's calculate the perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98dd1ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 39764.020490463474\n"
     ]
    }
   ],
   "source": [
    "# Calculate the probability of a trigram using Laplace smoothing\n",
    "def trigram_prob(trigram):\n",
    "    bigram = trigram[:-1]\n",
    "    return (ngrams_counts[3][trigram] + 1) / (ngrams_counts[2][bigram] + V)\n",
    "\n",
    "\n",
    "# Calculate perplexity on the test data\n",
    "log_prob_sum = 0\n",
    "test_word_count = 0\n",
    "\n",
    "for sentence in test_data:\n",
    "    for trigram in ngrams(sentence, n, pad_left=True, pad_right=True):\n",
    "        log_prob_sum += math.log2(trigram_prob(trigram))\n",
    "        if trigram[-1] is not None:\n",
    "            test_word_count += 1\n",
    "\n",
    "perplexity = 2 ** (-log_prob_sum / test_word_count)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5021ca89",
   "metadata": {},
   "source": [
    "## Sampling Sentences from a Language Model\n",
    "\n",
    "Sampling sentences from a language model is a method to generate sentences based on the probabilities defined by the model. By doing this, we can visualize what the language model has learned and understand its knowledge representation. In this process, sentences with higher probabilities are more likely to be generated than those with lower probabilities.\n",
    "\n",
    "For example, consider a unigram language model. We can visualize the probability distribution of all the words in the model's vocabulary, each word covering an interval proportional to its frequency. To generate a sentence, we randomly pick a value between 0 and 1, and select the word whose interval includes the chosen value. We repeat this process until we generate the sentence-final token `</s>`.\n",
    "\n",
    "For bigram models, the process is similar. We first generate a random bigram that starts with `<s>` based on its bigram probability. Let's say the second word of the bigram is `w`. We then choose a random bigram starting with `w` according to its bigram probability, and continue this process until we generate a sentence.\n",
    "\n",
    "In essence, sampling sentences from a language model helps us understand the kind of sentences the model considers to be more likely and thus provides insights into its knowledge representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0a7ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You love apples\n",
      "You love bananas\n",
      "You like apples\n",
      "You like bananas\n",
      "I love apples\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample bigram model\n",
    "bigram_model = {\n",
    "    '<s>': {'I': 0.5, 'You': 0.5},\n",
    "    'I': {'like': 0.5, 'love': 0.5},\n",
    "    'You': {'like': 0.5, 'love': 0.5},\n",
    "    'like': {'apples': 0.5, 'bananas': 0.5},\n",
    "    'love': {'apples': 0.5, 'bananas': 0.5},\n",
    "    'apples': {'</s>': 1.0},\n",
    "    'bananas': {'</s>': 1.0}\n",
    "}\n",
    "\n",
    "def generate_sentence(model):\n",
    "    sentence = []\n",
    "    current_word = '<s>'\n",
    "    \n",
    "    while current_word != '</s>':\n",
    "        next_word_candidates = list(model[current_word].keys())\n",
    "        next_word_probabilities = list(model[current_word].values())\n",
    "        current_word = random.choices(next_word_candidates, next_word_probabilities)[0]\n",
    "        \n",
    "        if current_word != '</s>':\n",
    "            sentence.append(current_word)\n",
    "    \n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Generate 5 sentences\n",
    "for _ in range(5):\n",
    "    print(generate_sentence(bigram_model))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4aa79c7c",
   "metadata": {},
   "source": [
    "This code defines a simple bigram model and a function `generate_sentence` to generate sentences using this model. The function starts with the `<s>` token and continues generating words based on the bigram probabilities until it encounters the `</s>` token. Running this code will generate and print 5 sentences using the given bigram model. Note that this example uses a very simple and limited bigram model, but the same principle can be applied to more complex models.\n",
    "\n",
    "### **An example using the Brown Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/yjlee/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram model sentence: The first year hailed the queen , Brooks Robinson slammed a local director at Rice Stadium Oct. 14 , and must have never have never is to solicit funds for violating the government and won it .\n",
      "Trigram model sentence: When Mickey went to see them .\n",
      "4-gram model sentence: He managed to maneuver the missile to a landing speed of 200 m.p.h. -- fast even for a high school where there were lots of cars `` might not be realistic and would not work '' .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "# Load Brown news corpus\n",
    "news_sents = brown.sents(categories=\"news\")\n",
    "\n",
    "# Function to create n-gram models\n",
    "def create_ngram_model(sentences, n):\n",
    "    model = defaultdict(Counter)\n",
    "    for sent in sentences:\n",
    "        sent = [\"<s>\"] * (n - 1) + sent + [\"</s>\"]\n",
    "        for i in range(len(sent) - n + 1):\n",
    "            ngram = tuple(sent[i : i + n])\n",
    "            prefix, word = ngram[:-1], ngram[-1]\n",
    "            model[prefix][word] += 1\n",
    "\n",
    "    # Convert counts to probabilities\n",
    "    for prefix, word_counts in model.items():\n",
    "        total = sum(word_counts.values())\n",
    "        for word, count in word_counts.items():\n",
    "            model[prefix][word] = count / total\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to generate sentences from n-gram models\n",
    "def generate_sentence(model):\n",
    "    sentence = []\n",
    "    current_ngram = (\"<s>\",) * (len(list(model.keys())[0]))\n",
    "\n",
    "    while \"</s>\" not in current_ngram:\n",
    "        next_word_candidates = list(model[current_ngram].keys())\n",
    "        next_word_probabilities = list(model[current_ngram].values())\n",
    "        next_word = random.choices(next_word_candidates, next_word_probabilities)[0]\n",
    "\n",
    "        if next_word != \"</s>\":\n",
    "            sentence.append(next_word)\n",
    "\n",
    "        current_ngram = current_ngram[1:] + (next_word,)\n",
    "\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "\n",
    "# Create bigram, trigram, and 4-gram models\n",
    "bigram_model = create_ngram_model(news_sents, 2)\n",
    "trigram_model = create_ngram_model(news_sents, 3)\n",
    "fourgram_model = create_ngram_model(news_sents, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecea41b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram model sentence: A cheer here is clearly that Georgia to the Texas , activities .\n",
      "Trigram model sentence: Indications as late as the views of another one .\n",
      "4-gram model sentence: The game players saw the Air Force Academy .\n"
     ]
    }
   ],
   "source": [
    "# Generate sentences using the models\n",
    "print(\"Bigram model sentence:\", generate_sentence(bigram_model))\n",
    "print(\"Trigram model sentence:\", generate_sentence(trigram_model))\n",
    "print(\"4-gram model sentence:\", generate_sentence(fourgram_model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91a637c1",
   "metadata": {},
   "source": [
    "This code loads the Brown news corpus, defines functions to create n-gram models and generate sentences using these models, and then creates bigram, trigram, and 4-gram models. It generates and prints one sentence for each model. Note that the generated sentences may vary each time the code is run due to the random sampling process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b20469ca",
   "metadata": {},
   "source": [
    "## Unknown Words\n",
    "\n",
    "In language modeling, we may encounter words that we've never seen before in the training data, known as unknown words or out-of-vocabulary (OOV) words. Handling these unknown words is an important aspect of language modeling.\n",
    "\n",
    "There are two common strategies for dealing with unknown words:\n",
    "\n",
    "1. **Closed Vocabulary System**: In this approach, we assume a fixed vocabulary in advance. The test set can only contain words from this known vocabulary, and there will be no unknown words. However, this is not a practical approach in most real-world situations.\n",
    "\n",
    "2. **Open Vocabulary System**: We create an open vocabulary system by adding a pseudo-word `<UNK>` to represent all potential unknown words in the test set. There are two ways to train the probabilities of the unknown word model `<UNK>`:\n",
    "\n",
    "  - Choose a fixed vocabulary (word list) in advance, convert any word not in this set to the unknown word token `<UNK>` in the training set, and estimate the probabilities for `<UNK>` based on its counts just like any other regular word in the training set.\n",
    "\n",
    "  - Alternatively, create a vocabulary implicitly by replacing words in the training data with `<UNK>` based on their frequency. For example, we can replace all words that occur fewer than n times in the training set by `<UNK>` or select a vocabulary size V in advance and choose the top V words by frequency and replace the rest by `<UNK>`. Then, proceed to train the language model, treating `<UNK>` as a regular word.\n",
    "\n",
    "The choice of `<UNK>` affects metrics like perplexity, as a language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability. Perplexities can only be compared across language models with the same vocabularies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87a4a21e",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "Smoothing is an essential technique in language modeling to address the problem of unseen n-grams in the training data. The main idea behind smoothing is to assign non-zero probabilities to these unseen n-grams, which helps avoid issues like zero probabilities when evaluating a language model on a test set.\n",
    "\n",
    "For example, consider a bigram language model trained on a corpus. If a specific word pair (bigram) never appeared in the training data but appears in the test set, the model would assign it a probability of zero. This zero probability would then affect the entire sentence's probability, making it zero as well. Smoothing techniques help prevent this problem by redistributing some probability mass from seen n-grams to unseen n-grams.\n",
    "\n",
    "There are various smoothing techniques, such as Additive (Laplace) smoothing, Good-Turing smoothing, and Kneser-Ney smoothing. One common method is Additive (Laplace) smoothing:\n",
    "\n",
    "Let's assume we have a bigram language model with vocabulary V and bigram counts $C(w_{i-1}, w_i)$. Without smoothing, the probability of a bigram is calculated as:\n",
    "\n",
    "$$ P(w_i | w_{i-1}) = C(w){i-1}, w_i) / \\sum C(w_{i-1}, w) $$\n",
    "\n",
    "With Additive (Laplace) smoothing, we add a constant k (usually k=1) to the count of each bigram, which in turn adds a non-zero probability to unseen bigrams:\n",
    "\n",
    "$$ P(w_i | w_{i-1}) = (C(w_{i-1}, w_i) + k) / ( \\sum C(w_{i-1}, w) + k * |V| ) $$\n",
    "\n",
    "Here, $|V|$ represents the size of the vocabulary. By adding k to each bigram count, we ensure that unseen bigrams get a non-zero probability, thus preventing zero probabilities in the language model evaluation.\n",
    "\n",
    "In summary, smoothing techniques in language modeling help to handle unseen n-grams by redistributing probability mass and ensuring that no n-gram has a zero probability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "652df51b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
    "- [Evaluation Metrics for Language Modeling](https://thegradient.pub/understanding-evaluation-metrics-for-language-models)\n",
    "- [Perplexity in Language Models](https://towardsdatascience.com/perplexity-in-language-models-87a196019a94)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
