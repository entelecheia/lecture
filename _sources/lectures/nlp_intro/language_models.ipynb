{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## General Concept\n",
    "\n",
    "Language models are computational models that assign probabilities to sequences of words or predict the next word in a sequence. They play a crucial role in various natural language processing tasks, such as:\n",
    "\n",
    "- Speech recognition\n",
    "- Machine translation\n",
    "- Spelling and grammar correction\n",
    "- Text generation\n",
    "\n",
    "Language models capture the structure and patterns within a language, allowing them to estimate how likely a given word or phrase is to appear in a specific context. There are different types of language models, such as:\n",
    "\n",
    "- N-gram models, which rely on sequences of words\n",
    "- Neural language models, which utilize deep learning techniques to understand and generate text\n",
    "\n",
    "Understanding and developing effective language models is essential for improving the performance of natural language processing systems.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59377615",
   "metadata": {},
   "source": [
    "## Why do we need language models?\n",
    "\n",
    "Language models are essential for various reasons in natural language processing tasks:\n",
    "\n",
    "1. **Disambiguation**: Language models help in resolving ambiguities in speech recognition and text processing, as they can assign probabilities to different interpretations based on the context, selecting the most likely one.\n",
    "\n",
    "2. **Machine Translation**: In translating text from one language to another, language models can help choose the most fluent and accurate translations by estimating the likelihood of word sequences in the target language.\n",
    "\n",
    "3. **Text Generation**: Language models can generate coherent and contextually relevant text, which is useful for tasks like summarization, question-answering, and dialogue systems.\n",
    "\n",
    "4. **Spelling and Grammar Correction**: Language models can identify and correct errors in written text by comparing the probabilities of different word sequences and suggesting more likely alternatives.\n",
    "\n",
    "5. **Assistive Technologies**: Language models are crucial for augmentative and alternative communication (AAC) systems, as they can predict and suggest likely words or phrases for users with speech or language impairments, making communication more efficient.\n",
    "\n",
    "Overall, language models play a critical role in improving the performance and accuracy of natural language processing systems by capturing the structure, patterns, and nuances of a language.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8469ff40",
   "metadata": {},
   "source": [
    "## N-gram Language Models\n",
    "\n",
    "N-gram language models are a simple yet powerful approach to language modeling. They predict the next word in a sequence based on the previous n-1 words, where n is the order of the model. The main types of n-grams are:\n",
    "\n",
    "- **Unigram**: Considers only a single word, ignoring context (n=1).\n",
    "- **Bigram**: Considers a sequence of two words (n=2).\n",
    "- **Trigram**: Considers a sequence of three words (n=3).\n",
    "- **Higher-order n-grams**: Considers longer sequences of words (n>3).\n",
    "\n",
    "In n-gram models, the probability of a word depends only on the (n-1) previous words. This simplification is known as the Markov assumption. To estimate the probabilities, n-gram models rely on counting the occurrences of n-grams in a large corpus and normalizing the counts.\n",
    "\n",
    "Advantages of n-gram models:\n",
    "- Relatively simple and easy to implement.\n",
    "- Efficient in terms of computation and memory usage.\n",
    "\n",
    "Limitations of n-gram models:\n",
    "- Unable to capture long-range dependencies between words.\n",
    "- Sensitive to data sparsity issues, as some n-grams may not appear in the training corpus.\n",
    "\n",
    "Despite their limitations, n-gram models serve as a foundational tool for understanding language modeling concepts and are still used in various NLP applications.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6cc96c0",
   "metadata": {},
   "source": [
    "## N-Grams and Probability Estimation\n",
    "\n",
    "N-grams can be used to estimate the probability of a word given a history (P(w|h)). For instance, if the history h is \"I like to eat\", we might want to estimate the probability of the next word being \"pizza\" (P(pizza|I like to eat)).\n",
    "\n",
    "### Estimating Probabilities from Counts\n",
    "We can estimate this probability by counting the occurrences of the history followed by the target word in a large corpus and dividing by the total count of the history:\n",
    "\n",
    "$$\n",
    "P(\\text{pizza}|\\text{I like to eat}) = \\frac{C(\\text{I like to eat pizza})}{C(\\text{I like to eat})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{먹습니다}|\\text{저는 김치를}) = \\frac{C(\\text{저는 김치를 먹습니다})}{C(\\text{저는 김치를})}\n",
    "$$\n",
    "\n",
    "However, even with a large corpus, it's often not sufficient to provide good estimates due to the creative nature of language and the possibility of unseen word sequences.\n",
    "\n",
    "### Bigram Model\n",
    "To tackle this issue, we can use the bigram model, which approximates the probability of a word given its entire history by considering only the preceding word:\n",
    "\n",
    "$$\n",
    "P(\\text{pizza}|\\text{I like to eat}) \\approx P(\\text{pizza}|\\text{eat})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{먹습니다}|\\text{저는 김치를}) \\approx P(\\text{먹습니다}|\\text{김치를})\n",
    "$$\n",
    "\n",
    "This simplification allows us to estimate probabilities more reliably, but it may not capture longer context dependencies. Nevertheless, n-grams serve as a foundational tool for understanding language modeling concepts and can be useful in various NLP applications.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0a11e1f",
   "metadata": {},
   "source": [
    "## Estimating Joint Probabilities of Word Sequences\n",
    "\n",
    "To estimate the joint probability of an entire sequence of words, such as \"The cat sat on the mat\", we can decompose this probability using the chain rule of probability:\n",
    "\n",
    "$$\n",
    "P(w_1:n) = P(w_1)P(w_2|w_1)P(w_3|w_1:2)...P(w_n|w_1:n−1) = \\prod_{k=1}^n P(w_k|w_1:k−1)\n",
    "$$\n",
    "\n",
    "This decomposition shows the link between computing the joint probability of a sequence and computing the conditional probability of a word given previous words. However, it doesn't really seem to help us! Computing the exact probability of a word given a long sequence of preceding words (e.g., P(wn|w1:n−1)) is challenging because language is creative, and any particular context might have never occurred before.\n",
    "\n",
    "### Example\n",
    "\n",
    "For the sequence \"The cat sat on the mat\", we can decompose the joint probability as follows:\n",
    "\n",
    "$$\n",
    "P(\\text{The, cat, sat, on, the, mat}) = P(\\text{The})P(\\text{cat}|\\text{The})P(\\text{sat}|\\text{The, cat})...P(\\text{mat}|\\text{The, cat, sat, on, the})\n",
    "$$\n",
    "\n",
    "Estimating each conditional probability using counts from a large corpus is not feasible since many long sequences might never have occurred before.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59fb5f1c",
   "metadata": {},
   "source": [
    "## N-gram Models and Markov Assumption\n",
    "\n",
    "N-gram models are used to predict the probability of a word given a fixed number of preceding words. The assumption that the probability of a word depends only on a limited number of previous words is called the Markov assumption.\n",
    "\n",
    "For a bigram model (N=2), the approximation is:\n",
    "\n",
    "$$\n",
    "P(w_n|w_1:n−1) ≈ P(w_n|w_{n−1})\n",
    "$$\n",
    "\n",
    "For an n-gram model with size N, the approximation is:\n",
    "\n",
    "$$\n",
    "P(w_n|w_1:n−1) ≈ P(w_n|w_{n−N+1:n−1})\n",
    "$$\n",
    "\n",
    "Given the n-gram assumption for the probability of an individual word, we can compute the probability of a complete word sequence as:\n",
    "\n",
    "$$\n",
    "P(w_1:n) ≈ \\prod_{k=1}^n P(w_k|w_{k−N+1:k−1})\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "For the trigram model (N=3), the approximation for the sequence \"The cat sat on the mat\" is:\n",
    "\n",
    "$$\n",
    "P(\\text{The, cat, sat, on, the, mat}) ≈ P(\\text{The})P(\\text{cat}|\\text{The})P(\\text{sat}|\\text{The, cat})P(\\text{on}|\\text{cat, sat})P(\\text{the}|\\text{sat, on})P(\\text{mat}|\\text{on, the})\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a30e87ee",
   "metadata": {},
   "source": [
    "### Stock Prices and Markov Assumption\n",
    "\n",
    "The Markov assumption can be applied to model stock prices as well. In finance, the Markov assumption is often used to represent the idea that future stock prices depend only on the current price and a limited number of past prices, rather than the entire price history.\n",
    "\n",
    "The relationship between stock prices and the Markov assumption can be understood as follows:\n",
    "\n",
    "1. **Memoryless Property**: The Markov assumption implies that the stock price at a certain time is only influenced by a fixed number of previous time steps. This means that the future price movement doesn't depend on the entire history but only on the recent past. This property is also known as the memoryless property of Markov models.\n",
    "\n",
    "2. **Simplifying Complex Systems**: Stock prices are affected by a vast number of factors, including market trends, company performance, global events, and investor sentiment. By applying the Markov assumption, we can simplify the modeling of stock prices by focusing only on the most recent price changes, which are assumed to capture relevant information.\n",
    "\n",
    "3. **Prediction and Analysis**: Using the Markov assumption in financial models allows us to predict and analyze stock price movements. For example, we can create Markov models to estimate the probabilities of stock price changes, which can be useful for trading strategies, risk management, and portfolio optimization.\n",
    "\n",
    "It's important to note that the Markov assumption is a simplification and may not always accurately represent the complexities of the stock market. However, it serves as a useful tool in modeling and analyzing stock prices.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2c898bb",
   "metadata": {},
   "source": [
    "## Estimating Bigram or N-gram Probabilities using Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "To estimate the probabilities for bigrams or n-grams, we use maximum likelihood estimation (MLE) by counting the occurrences in a corpus and normalizing the counts.\n",
    "\n",
    "MLE, or Maximum Likelihood Estimation, is a statistical method used for estimating the parameters of a probability distribution or a model. It works by finding the parameter values that maximize the likelihood of the observed data under the given model. In other words, MLE selects the parameters that make the observed data most probable.\n",
    "\n",
    "For example, if we have a dataset of coin tosses (heads and tails), and we want to estimate the probability of getting heads, we can use MLE to find the parameter value that makes the observed sequence of coin tosses most likely. This is usually done by calculating the ratio of the number of heads to the total number of tosses.\n",
    "\n",
    "### Bigram Probability Calculation\n",
    "\n",
    "Compute the bigram probability of a word $w_n$ given the previous word $w_{n-1}$:\n",
    "\n",
    "$$ P(w_n|w_{n−1}) = C(w_{n−1}w_n) / C(w_{n−1}) $$\n",
    "\n",
    "where $C(w_{n−1} w_n)$ is the count of the bigram $w_{n−1}w_n$ and $C(w_{n−1})$ is the count of the unigram $w_{n−1}$.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a mini-corpus with three sentences:\n",
    "\n",
    "1. `<s> I am Sam </s>`\n",
    "2. `<s> Sam I am </s>`\n",
    "3. `<s> I do not like green eggs and ham </s>`\n",
    "\n",
    "Here are the bigram probabilities for some pairs in this corpus:\n",
    "\n",
    "| Bigram         | Probability |\n",
    "|----------------|-------------|\n",
    "| P(I\\|`<s>`)       | 2/3         |\n",
    "| P(Sam\\|`<s>`)     | 1/3         |\n",
    "| P(am\\|I)        | 2/3         |\n",
    "| P(`</s>`\\|Sam)    | 1/2         |\n",
    "| P(Sam\\|am)      | 1/2         |\n",
    "| P(do\\|I)        | 1/3         |\n",
    "\n",
    "For general MLE n-gram parameter estimation:\n",
    "\n",
    "$$ P(w_n|w_{n−N+1:n−1}) = C(w_{n−N+1:n−1} w_n) / C(w_{n−N+1:n−1}) $$\n",
    "\n",
    "Bigram probabilities capture various linguistic phenomena, such as syntax, task-specific patterns, and cultural preferences.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af0ed12d",
   "metadata": {},
   "source": [
    "## Practical Issues in N-gram Models\n",
    "\n",
    "1. **Higher-order n-grams**: In practice, trigram (conditioning on the previous two words), 4-gram, or even 5-gram models are more common when there is sufficient training data. For larger n-grams, extra contexts are needed to the left and right of the sentence end (e.g., P(I|`<s><s>`) for trigrams).\n",
    "\n",
    "2. **Log probabilities**: Since multiplying probabilities can lead to numerical underflow, it's better to represent and compute language model probabilities in log format. Adding log probabilities is equivalent to multiplying probabilities in linear space. Convert back to probabilities when needed by taking the exponential of the log probability:\n",
    "\n",
    "$$   p_1 × p_2 × p_3 × p_4 = \\exp(\\log{p_1} + \\log{p_2} + \\log{p_3} + \\log{p_4}) $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5da0611",
   "metadata": {},
   "source": [
    "## Examples of MLE\n",
    "\n",
    "Here's a practical example of MLE using Python. We'll estimate the parameter p of a Bernoulli distribution (coin toss) based on some observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a90560c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE for p (probability of heads): 0.65\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Observed data (1 for heads, 0 for tails)\n",
    "data = np.array([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1])\n",
    "\n",
    "# Calculate MLE for p (probability of heads)\n",
    "mle_p = np.mean(data)\n",
    "\n",
    "print(\"MLE for p (probability of heads):\", mle_p)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "502fba90",
   "metadata": {},
   "source": [
    "In this example, we have a sequence of 20 coin tosses, where 1 represents heads, and 0 represents tails. To estimate the probability of getting heads (p) using MLE, we simply calculate the mean of the observed data, which is the ratio of the number of heads to the total number of tosses.\n",
    "\n",
    "Here's another example, this time estimating the mean (mu) and standard deviation (sigma) of a Gaussian distribution given some observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c3965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE for mu (mean): 3.4899999999999998\n",
      "MLE for sigma (standard deviation): 0.6441273166075167\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Observed data\n",
    "data = np.array([2.5, 3.2, 4.1, 3.7, 2.8, 3.9, 4.5, 3.1, 2.9, 4.2])\n",
    "\n",
    "# Calculate MLE for mu (mean) and sigma (standard deviation)\n",
    "mle_mu = np.mean(data)\n",
    "mle_sigma = np.std(data)\n",
    "\n",
    "print(\"MLE for mu (mean):\", mle_mu)\n",
    "print(\"MLE for sigma (standard deviation):\", mle_sigma)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7f999e0",
   "metadata": {},
   "source": [
    "## How to generate n-grams using NLTK\n",
    "\n",
    "Here's an example of how to generate n-grams using the Natural Language Toolkit (NLTK) library in Python.\n",
    "\n",
    "First, make sure to install NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7861292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk) (2022.10.31)\n",
      "Installing collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.2.0 nltk-3.8.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3d038f5",
   "metadata": {},
   "source": [
    "Now, let's generate bigrams, trigrams, and 4-grams using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27982ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yjlee/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams:\n",
      "[('I', 'love'), ('love', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'and'), ('and', 'machine'), ('machine', 'learning'), ('learning', '.')]\n",
      "\n",
      "Trigrams:\n",
      "[('I', 'love', 'natural'), ('love', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'and'), ('processing', 'and', 'machine'), ('and', 'machine', 'learning'), ('machine', 'learning', '.')]\n",
      "\n",
      "4-grams:\n",
      "[('I', 'love', 'natural', 'language'), ('love', 'natural', 'language', 'processing'), ('natural', 'language', 'processing', 'and'), ('language', 'processing', 'and', 'machine'), ('processing', 'and', 'machine', 'learning'), ('and', 'machine', 'learning', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Download the punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"I love natural language processing and machine learning.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(\"Bigrams:\")\n",
    "print(bigrams)\n",
    "\n",
    "# Generate trigrams\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "print(\"\\nTrigrams:\")\n",
    "print(trigrams)\n",
    "\n",
    "# Generate 4-grams (quadgrams)\n",
    "quadgrams = list(ngrams(tokens, 4))\n",
    "print(\"\\n4-grams:\")\n",
    "print(quadgrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb85ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "652df51b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
