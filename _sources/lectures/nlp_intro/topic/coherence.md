# Topic Coherence Measures

![](figs/entelecheia_topic_coherence.png)

## Introduction

Topic coherence represents the overall interpretability of topics and is used to assess their quality. It is essential in topic modeling, a technique that aims to explain a collection of documents as a mixture of topics. Evaluating topics based on their interpretability is necessary, as mathematically optimal topics are not always human-readable. Topic coherence is a measure of interpretability and is used to evaluate the quality of topics. It tries to represent the degree of semantic similarity between high scoring words in a topic.

## Topic Modeling

Topic modeling aims to explain a collection of documents as a mixture of topics, where each topic is a distribution over words, and each document is a distribution over topics. The goal of topic modeling is to find the topics and their distributions over words and documents. The assumption behind topic modeling is that:

- A text (document) is composed of several topics.
- A topic is composed of several words.

## Evaluating Topics

Topic modeling algorithms rely on statistical inference to find the topics, but the quality of the topics is not directly observable. Hence, we need to evaluate the topics to assess their quality. Topic coherence is a measure of interpretability and is used to evaluate the quality of topics.

## Topic Coherence

Topic coherence assesses how well a topic is supported by a text corpus. It uses statistics and probabilities drawn from the text corpus to measure the coherence of a topic, focusing on the word's context. Topic coherence depends on both the words in a topic and the reference corpus.

![Topic Coherence](figs/topic_coherence.png)

A general structure for topic coherence measures has been proposed by Röder, M. et al {cite}`roder2015exploring`. It consists of three components:

1. Segmentation
2. Probability Calculation
3. Confirmation Measure
4. Aggregation

![](figs/topic_coherence_structure.png)

### 1. Segmentation

The segmentation module creates pairs of word subsets that will be used to compute the coherence of a topic. Considering $W={w_1, w_2, …, w_n}$ as the top-n most important words of a topic $t$, the application of a segmentation $S$ results in a set of subset pairs from $W$.

$$
S = \{(W^{\prime}, W^*) | W^{\prime}, W^* \subseteq W\}
$$

### 2. Probability Calculation

Coherence metrics use probabilities drawn from the textual corpus. The probability calculation module calculates the probabilities of the word subsets generated by the segmentation module. Different techniques can estimate these probabilities, such as document-level, sentence-level, or sliding window probabilities.

### 3. Confirmation Measure

The confirmation measure module calculates the confirmation of a word subset. It computes how well the subset $W^*$ supports the words in the subset $W^{\prime}$. If the words in $W^{\prime}$ are more likely to appear together with the words in $W^*$, then the confirmation measure will be high. Otherwise, it will be low. There are two types of confirmation measures: direct and indirect.

#### Direct Confirmation Measure

The direct confirmation measure compares the probabilities of the word subsets $W^{\prime}$ and $W^*$.

$$ m*r(S_i) = \frac{P(W^{\prime}, W^*)}{P(W^{\prime})P(W^\_)} $$

or, using logarithms:

$$ m\_{lr}(S*i) = log \frac{P(W^{\prime}, W^*) + \epsilon}{P(W^{\prime})P(W^\_) + \epsilon} $$

where $\epsilon$ is a small constant to avoid undefined values for logarithms.

#### Indirect Confirmation Measure

The indirect confirmation measure is a more complex one. It computes a direct confirmation measure for each word in the subsets $W^{\prime}$ with all other words in the subset $W^*$, yielding a vector of confirmation scores for each word in $W^{\prime}$.

$$
\vec{v}_m(W^{\prime}) = \left\{\sum_{w \in W^{\prime}} m(w_i, w_j)\right\}_{j=1,2, …, |W|}
$$

where $|W|$ is the number of words in $W$.

The same is done for the subset $W^*$.

Then, the indirect confirmation measure is the similarity between the two vectors.

$$
\tilde{m}_{cos}(W^{\prime}, W^*) = sim(\vec{v}_m(W^{\prime}), \vec{v}_m(W^*))
$$

where $sim$ is a similarity function, such as cosine similarity.

![](figs/indirect_confirmation_measure.png)

The idea behind this indirect confirmation measure is that it tries to capture some relationships that are not captured by the direct confirmation measure.

For example, the words ‘cats’ and ‘dogs’ may never appear together in our dataset, but they might appear frequently with the words ‘toys’, ‘pets’, and ‘cute’. In this case, the direct confirmation measure will not be able to capture the relationship between ‘cats’ and ‘dogs’ because they never appear together. However, the indirect confirmation measure will be able to capture this relationship because it will be able to see that ‘cats’ and ‘dogs’ appear frequently with the words ‘toys’, ‘pets’, and ‘cute’.

### Aggregation

The aggregation module is responsible for aggregating the confirmation scores of the pairs generated in the previous step. It computes the final coherence score for the topic.

There are different types of aggregation techniques: mean, median, geometric mean, and so on.

![](figs/aggregation.png)

### Putting everything together

Measuring the coherence metrics follows the following steps:

- We have a topic $T$ that we want to measure the coherence of.
- We choose a reference corpus $C$.
- The top-n most important words in the topic $T$ are extracted, yielding a word subset $W$.
- $W$ is segmented into pairs of words, yielding a set of word subsets $S$.
- Using the reference corpus $C$, we calculate the probabilities of the word subsets $S$.
- With the segmented word subsets $S$ and the probabilities calculated from the reference corpus $C$, we calculate the confirmation measure for each pair of words in $S$.
- All the confirmation scores are aggregated into a single coherence score for the topic $T$.

If we have more than one topic, we can repeat the same process for each topic and use the average coherence score as a measure of the quality of the topic model.

![](figs/multiple_topics.png)

The Gensim library provides a class that implements the four most famous coherence models:
$u_{mass}$, $c_v$, $c_{uci}$, $c_{npmi}$.

![](figs/gensim_coherence_models.png)

$C_{NPMI}$ uses the following steps:

- Segmentation: S-one-one (one word in each subset)
- Probability estimation: the probabilities are calculated over a sliding window of size 10.
- Confirmation measure: the confirmation measure is the Normalized Pointwise Mutual Information (NPMI).

$$
NPMI(W^{\prime}, W^*) = \frac{\log \frac{P(W^{\prime}, W^*) + \epsilon}{P(W^{\prime})P(W^*) + \epsilon}}{-\log (P(W^{\prime}, W^*) + \epsilon)}
$$
