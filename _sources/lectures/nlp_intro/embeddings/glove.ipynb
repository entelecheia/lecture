{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e28f22bd",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. It is based on the co-occurrence matrix of words from a corpus.\n",
    "\n",
    "GloVe stands for Global Vectors for Word Representation. It was introduced by {cite}`pennington2014glove` in 2014.\n",
    "\n",
    "### Improvement over word2vec\n",
    "\n",
    "- Word2vec uses a window-based approach, in which it only considers the local context of a word. This means that it does not consider the global context of a word.\n",
    "- GloVe uses a global context of a word in addition to the local context. This means that it can capture the meaning of a word better than word2vec.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce79d657",
   "metadata": {},
   "source": [
    "### Co-occurrence matrix\n",
    "\n",
    "What are the global contexts of a word? The global contexts of a word are the words that co-occur with it in a corpus.\n",
    "\n",
    "For example, we have the following sentences:\n",
    "\n",
    "**Document 1:** \"All that glitters is not gold.\"\n",
    "\n",
    "**Document 2:** \" All is well that ends well.\"\n",
    "\n",
    "Then, with a window size of 1, the co-occurrence matrix of the words in the corpus is:\n",
    "\n",
    "```{image} figs/co-occurrence_matrix.png\n",
    ":alt: co-occurrence_matrix\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "- The rows and columns represent the words in the corpus.\n",
    "- `<START>` and `<END>` are special tokens that represent the start and end of a sentence.\n",
    "- Since `that` and `is` occur only once in the window of `glitters`, the value of (`that`, `glitters`) and (`is`, `glitters`) is 1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8928d204",
   "metadata": {},
   "source": [
    "### Training GloVe\n",
    "\n",
    "Glove model is a weighted least squares regression model, where the weights are the word vectors. The objective function is the sum of squared errors between the co-occurrence matrix and the dot product of the word vectors.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L} = \\frac{1}{2}\\sum_{i,j=1}^V f(X_{ij}) (\\log X_{ij} - \\mathbf{u}_i^\\top \\mathbf{v}_j)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $X_{ij}$ is the co-occurrence matrix, $\\mathbf{u}_i$ is the word vector of the $i$ th word, and $\\mathbf{v}_j$ is the word vector of the $j$ th word. The function $f$ is a weighting function that is used to downweight the common words.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "188485cf",
   "metadata": {},
   "source": [
    "### GloVe vs word2vec\n",
    "\n",
    "- GloVe is a global model, while word2vec is a local model.\n",
    "- GloVe ouputperforms word2vec on word analogy, word similarity, and Named Entity Recognition (NER) tasks.\n",
    "- If the nature of the problem is similar to the above tasks, then GloVe is a better choice than word2vec.\n",
    "- Since it uses a global context, GloVe is better at capturing the meaning of rare words even on small datasets.\n",
    "- GloVe is slower than word2vec.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0d3cc4c",
   "metadata": {},
   "source": [
    "### Using GloVe\n",
    "\n",
    "GloVe is available to download from the [Stanford NLP website](https://nlp.stanford.edu/projects/glove/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
