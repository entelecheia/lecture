{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16570c80",
   "metadata": {},
   "source": [
    "## Neural Language Models\n",
    "\n",
    "Word embeddings were proposed by {cite}`bengio2003neural` as a way to represent words as vectors.\n",
    "\n",
    "Bengioâ€™s method could train a neural network such that each training sentence could inform the model about a number of semantically available neighboring words, which was known as `distributed representation of words`. The nueural network preserved relationships between words in terms of their contexts (semantic and syntactic).\n",
    "\n",
    "![](figs/bengio.png)\n",
    "\n",
    "This introduced a neural network architecture approach that laid the foundation for many current approaches.\n",
    "\n",
    "This neural network has three components:\n",
    "\n",
    "- **Embedding layer**: maps words to vectors, the parameters are shared across the network.\n",
    "- **Hidden layer**: a fully connected layer with a non-linear activation function.\n",
    "- **Output layer**: produces a probability distribution over the vocabulary using a softmax function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f12b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"i like apples\", \"i like bananas\", \"i hate oranges\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "540d5820",
   "metadata": {},
   "source": [
    "### Step 1: Indexing the words.\n",
    "\n",
    "For each word in the sentence, we assign an index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e3b77c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 6\n"
     ]
    }
   ],
   "source": [
    "words = \" \".join(sentences).split()\n",
    "word_list = list(set(words))\n",
    "word_to_id = {w: i for i, w in enumerate(word_list)}\n",
    "id_to_word = {i: w for w, i in word_to_id.items()}\n",
    "vocab_size = len(word_to_id)\n",
    "print(\"vocab_size:\", vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a676257f",
   "metadata": {},
   "source": [
    "### Step 2: Building the model.\n",
    "\n",
    "- An embedding layer is a lookup table that maps each word to a vector.\n",
    "- Once the input index of the word is embedded, it is passed through the first hidden layer with bias added to it.\n",
    "- The output of the first hidden layer is passed through a tanh activation function.\n",
    "- The output from the embedding layer is also passed through the final layer where the output of the tanh layer is added to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7b093db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_hiddens, num_steps):\n",
    "        self.num_steps = num_steps\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        super(NNLM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            vocab_size, embedding_size\n",
    "        )  # embedding layer or look up table\n",
    "\n",
    "        self.hidden1 = nn.Linear(num_steps * embedding_size, num_hiddens, bias=False)\n",
    "        self.ones = nn.Parameter(torch.ones(num_hiddens))\n",
    "\n",
    "        self.hidden2 = nn.Linear(num_hiddens, vocab_size, bias=False)\n",
    "        self.hidden3 = nn.Linear(\n",
    "            num_steps * embedding_size, vocab_size, bias=False\n",
    "        )  # final layer\n",
    "\n",
    "        self.bias = nn.Parameter(torch.ones(vocab_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        word_embeds = self.embeddings(X)  # embeddings\n",
    "        X = word_embeds.view(-1, self.num_steps * self.embedding_size)  # first layer\n",
    "        tanh = torch.tanh(self.ones + self.hidden1(X))  # tanh layer\n",
    "        output = (\n",
    "            self.bias + self.hidden3(X) + self.hidden2(tanh)\n",
    "        )  # summing up all the layers with bias\n",
    "        return word_embeds, output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2cbc857",
   "metadata": {},
   "source": [
    "### Step 3: Loss and optimization function.\n",
    "\n",
    "Now that we have the model, we need to define the loss function and the optimization function.\n",
    "\n",
    "We are using the cross-entropy loss function and the Adam optimizer.\n",
    "\n",
    "The cross-entropy loss function is made up of two parts:\n",
    "\n",
    "- The softmax function: this is used to normalize the output of the model so that the sum of the probabilities of all the words in the vocabulary is equal to one.\n",
    "- The negative log-likelihood: this is used to calculate the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cfb60fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 2\n",
    "num_hiddens = 2\n",
    "embedding_size = 2\n",
    "\n",
    "model = NNLM(vocab_size, embedding_size, num_hiddens, num_steps)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba837b38",
   "metadata": {},
   "source": [
    "### Step 4: Training the model.\n",
    "\n",
    "Finally, we train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "99b920be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(sentences, word_to_id):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        word = sent.split()\n",
    "        input = [word_to_id[n] for n in word[:-1]]\n",
    "        target = word_to_id[word[-1]]\n",
    "\n",
    "        input_batch.append(input)\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3218cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.488254\n",
      "Epoch: 2000 cost = 0.466801\n",
      "Epoch: 3000 cost = 0.463683\n",
      "Epoch: 4000 cost = 0.462811\n",
      "Epoch: 5000 cost = 0.462459\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch = make_batch(sentences, word_to_id)\n",
    "input_batch = torch.LongTensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    embeddings, output = model(input_batch)\n",
    "\n",
    "    # output : [batch_size, n_class], target_batch : [batch_size]\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(\"Epoch:\", \"%04d\" % (epoch + 1), \"cost =\", \"{:.6f}\".format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d1938dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'like'], ['i', 'like'], ['i', 'hate']] -> ['bananas', 'bananas', 'oranges']\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "predict = model(input_batch)[1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "print(\n",
    "    [sen.split()[:2] for sen in sentences],\n",
    "    \"->\",\n",
    "    [id_to_word[n.item()] for n in predict.squeeze()],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c60aa00",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Word embeddings are a way to represent words as low-dimensional dense vectors.\n",
    "- These embeddings have associated learnable vectors, which optimize themselves through back propagation.\n",
    "- Essentially, the embedding layer is the first layer of a neural network.\n",
    "- They try to preserve the semantic and syntactic relationships between words.\n",
    "\n",
    "![](figs/w2v.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
