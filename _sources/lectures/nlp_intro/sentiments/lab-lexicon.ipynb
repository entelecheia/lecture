{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f175c4e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Lab: Lexicon-Based Sentiment Analysis\n",
    "\n",
    "In this lab session, we will explore various lexicon-based methods for sentiment analysis. Lexicon-based methods rely on a predefined list of words with associated sentiment scores to determine the overall sentiment of a text. We will use the following lexicon-based methods:\n",
    "\n",
    "1. TextBlob\n",
    "2. AFINN\n",
    "3. VADER\n",
    "\n",
    "We will perform sentiment analysis on the `movie_reviews` dataset from the NLTK package.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the required packages and load the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63fe3370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/yjlee/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/yjlee/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "nltk.download(\"movie_reviews\")\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89ee3f64",
   "metadata": {},
   "source": [
    "Next, we'll extract the reviews and their categories (positive or negative).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0677c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileids = movie_reviews.fileids()\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca9fed8c",
   "metadata": {},
   "source": [
    "Now, we'll define a function to evaluate the classification performance of each lexicon-based method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c142ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_performance(true_labels, predicted_labels):\n",
    "    print(\"Accuracy: \", accuracy_score(true_labels, predicted_labels))\n",
    "    print(\n",
    "        \"Precision: \",\n",
    "        precision_score(true_labels, predicted_labels, average=\"weighted\"),\n",
    "    )\n",
    "    print(\"Recall: \", recall_score(true_labels, predicted_labels, average=\"weighted\"))\n",
    "    print(\"F1 Score: \", f1_score(true_labels, predicted_labels, average=\"weighted\"))\n",
    "    print(\"Model Report: \\n___________________________________________________\")\n",
    "    print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0ece566",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "\n",
    "TextBlob is a simple Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "\n",
    "- https://textblob.readthedocs.io/en/dev/quickstart.html\n",
    "\n",
    "### Performing sentiment analysis using TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f15b4bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: joblib in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2023.3.23)\n",
      "Requirement already satisfied: click in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd2f84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6\n",
      "Precision:  0.7225010902553423\n",
      "Recall:  0.6\n",
      "F1 Score:  0.5361560556566348\n",
      "Model Report: \n",
      "___________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.23      0.36      1000\n",
      "         pos       0.56      0.97      0.71      1000\n",
      "\n",
      "    accuracy                           0.60      2000\n",
      "   macro avg       0.72      0.60      0.54      2000\n",
      "weighted avg       0.72      0.60      0.54      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def sentiment_TextBlob(docs):\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        testimonial = TextBlob(doc)\n",
    "        if testimonial.sentiment.polarity > 0:\n",
    "            results.append(\"pos\")\n",
    "        else:\n",
    "            results.append(\"neg\")\n",
    "    return results\n",
    "\n",
    "\n",
    "predictions = sentiment_TextBlob(reviews)\n",
    "evaluate_classification_performance(categories, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dcf99a4",
   "metadata": {},
   "source": [
    "## AFINN\n",
    "\n",
    "AFINN is a list of English words rated for valence with an integer between minus five (negative) and plus five (positive). The words have been manually labeled by Finn Årup Nielsen in 2009-2011. The file is tab-separated. There are two versions:\n",
    "\n",
    "- AFINN-111: Newest version with 2477 words and phrases.\n",
    "- AFINN-96: 1468 unique words and phrases on 1480 lines. Note that there are 1480 lines, as some words are listed twice. The word list in AFINN-96 is the same as AFINN-111, but with 1009 fewer words and phrases.\n",
    "\n",
    ">\n",
    "\n",
    "- https://github.com/fnielsen/afinn\n",
    "- http://corpustext.com/reference/sentiment_afinn.html\n",
    "\n",
    "### Performing sentiment analysis using AFINN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ac5498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting afinn\n",
      "  Downloading afinn-0.1.tar.gz (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: afinn\n",
      "  Building wheel for afinn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53430 sha256=3a6e16220649c10b4cd5b3ca2aa3ac50b9e5b24f322be56bbed0737c3feefaaa\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b1osgjx9/wheels/f6/6f/c3/b305c5107a17618f2938a067d5ffcbb556909d82398762089e\n",
      "Successfully built afinn\n",
      "Installing collected packages: afinn\n",
      "Successfully installed afinn-0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509c0d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.664\n",
      "Precision:  0.6783880680137142\n",
      "Recall:  0.664\n",
      "F1 Score:  0.6570854714462421\n",
      "Model Report: \n",
      "___________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.73      0.52      0.61      1000\n",
      "         pos       0.63      0.81      0.71      1000\n",
      "\n",
      "    accuracy                           0.66      2000\n",
      "   macro avg       0.68      0.66      0.66      2000\n",
      "weighted avg       0.68      0.66      0.66      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "\n",
    "def sentiment_Afinn(docs):\n",
    "    afn = Afinn(emoticons=True)\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        if afn.score(doc) > 0:\n",
    "            results.append(\"pos\")\n",
    "        else:\n",
    "            results.append(\"neg\")\n",
    "    return results\n",
    "\n",
    "\n",
    "predictions = sentiment_Afinn(reviews)\n",
    "evaluate_classification_performance(categories, predictions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cf9f17f",
   "metadata": {},
   "source": [
    "## VADER\n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon## Lexicon-based Methods in Practice and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. VADER uses a combination of a sentiment lexicon, a set of linguistic rules, and grammatical heuristics to predict the sentiment of a given text.\n",
    "\n",
    "- https://github.com/cjhutto/vaderSentiment\n",
    "\n",
    "### Performing sentiment analysis using VADER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d70835d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.635\n",
      "Precision:  0.6580655585685583\n",
      "Recall:  0.635\n",
      "F1 Score:  0.6211802777111816\n",
      "Model Report: \n",
      "___________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.72      0.44      0.55      1000\n",
      "         pos       0.60      0.83      0.69      1000\n",
      "\n",
      "    accuracy                           0.64      2000\n",
      "   macro avg       0.66      0.64      0.62      2000\n",
      "weighted avg       0.66      0.64      0.62      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "def sentiment_vader(docs):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        score = analyser.polarity_scores(doc)\n",
    "        if score[\"compound\"] > 0:\n",
    "            results.append(\"pos\")\n",
    "        else:\n",
    "            results.append(\"neg\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "predictions = sentiment_vader(reviews)\n",
    "evaluate_classification_performance(categories, predictions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1592650",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab session, we explored various lexicon-based sentiment analysis methods, including TextBlob, AFINN, and VADER. We applied these methods to the `movie_reviews` dataset and evaluated their classification performance using accuracy, precision, recall, and F1 score metrics.\n",
    "\n",
    "It is important to note that lexicon-based methods have limitations, such as sensitivity to the context in which words are used and the inability to capture complex semantic relationships between words. However, they can still provide valuable insights for sentiment analysis tasks, especially when combined with other approaches like machine learning-based techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4173dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
