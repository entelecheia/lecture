{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Topic Modeling\n",
    "\n",
    "![](../figs/intro_nlp/topic/entelecheia_grouping.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47f6ede8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "As the digital age continues to expand, the volume of unstructured text data grows exponentially. From social media posts and news articles to research papers and customer reviews, there is a wealth of information waiting to be extracted and analyzed. Topic modeling emerges as a powerful tool to summarize and make sense of this large-scale unstructured text data, enabling users to better understand, navigate, and utilize the available information.\n",
    "\n",
    "At its core, topic modeling is an unsupervised machine learning technique that employs the words within a document to infer its underlying subject. By identifying patterns of word co-occurrence and frequencies, topic models can reveal the latent structure or \"topics\" that characterize a collection of documents. This process not only allows for a high-level summary of the document's content but also serves as a valuable method for dimension reduction.\n",
    "\n",
    "Dimension reduction is crucial for simplifying complex data and enhancing the efficiency of subsequent analysis. In the context of text data, topic modeling excels at reducing the dimensionality of the document-word matrix by representing documents as mixtures of topics and topics as distributions over words. This condensed representation not only captures the essence of the document's content but is also more interpretable compared to other dimension reduction techniques.\n",
    "\n",
    "While other methods, such as Principal Component Analysis (PCA), are also used for dimension reduction, topic models offer a more interpretable and contextually meaningful output. PCA transforms the original data into orthogonal components that explain the maximum variance, which can be difficult to relate back to the original features. In contrast, topic models generate a set of human-understandable topics that provide a clear view of the document's thematic structure.\n",
    "\n",
    "In summary,\n",
    "\n",
    "- Summarize unstructured text: Topic modeling helps in making sense of large-scale unstructured text data, providing high-level summaries of documents' content.\n",
    "- Infer subject using words: Analyzes patterns of word co-occurrence and frequencies to reveal latent topics characterizing a collection of documents.\n",
    "- Useful for dimension reduction: Represents documents as mixtures of topics and topics as distributions over words, simplifying complex data and enhancing analysis efficiency.\n",
    "- More interpretable than PCA: Generates human-understandable topics that provide a clear view of the document's thematic structure, compared to PCA's orthogonal components which can be difficult to relate back to original features.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34b244b8",
   "metadata": {},
   "source": [
    "## Topic Modeling Methodologies\n",
    "\n",
    "1.  Probabilistic Latent Semantic Analysis (pLSA): pLSA is a precursor to LDA and is based on a generative probabilistic model. It discovers latent topics in a document collection by modeling the co-occurrence of words and documents as a mixture of multinomial distributions. Despite its success in revealing hidden topics, pLSA suffers from overfitting due to the lack of regularization.\n",
    "2.  Latent Dirichlet Allocation (LDA): LDA is a widely-used generative probabilistic model for topic modeling, which extends pLSA by incorporating Dirichlet priors for document-topic and topic-term distributions. LDA's assumptions about the generative process of documents and the incorporation of Dirichlet priors help in overcoming overfitting and provide better generalization.\n",
    "3.  Non-negative Matrix Factorization (NMF): NMF is a linear algebraic method for dimensionality reduction, which has been applied to topic modeling. NMF factorizes the document-term matrix into two non-negative lower-dimensional matrices representing document-topic and topic-term relationships. Although NMF lacks the probabilistic foundation of LDA, it often results in more interpretable topics.\n",
    "4.  Correlated Topic Model (CTM): CTM is an extension of LDA that allows topics to be correlated, capturing more complex relationships between topics. In CTM, the distribution of topics within documents is modeled using a logistic normal distribution instead of a Dirichlet distribution. This approach results in more realistic topic models, especially when topics are not independent in the underlying data.\n",
    "5.  Dynamic Topic Models (DTM): DTM is a class of topic models designed to analyze the evolution of topics over time. By modeling topic distributions as a function of time, DTMs can capture the temporal dynamics of topics in ordered document collections. This makes DTMs suitable for analyzing text data with an inherent temporal structure, such as news articles or scientific publications.\n",
    "\n",
    "Each of these topic modeling methodologies offers unique advantages and drawbacks, catering to different needs and data types. By understanding the underlying principles and assumptions of these methods, researchers and practitioners can select the most appropriate approach for their specific text data analysis tasks and effectively extract meaningful insights from large-scale unstructured document collections.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56c70a92",
   "metadata": {},
   "source": [
    "## Probabilistic Latent Semantic Analysis (pLSA)\n",
    "\n",
    "Probabilistic Latent Semantic Analysis (pLSA) is a topic modeling technique that aims to uncover latent topics within a collection of documents by utilizing a generative probabilistic model. Introduced by Thomas Hofmann in 1999, pLSA is a precursor to the more popular Latent Dirichlet Allocation (LDA). It models the co-occurrence of words and documents as a mixture of multinomial distributions, attempting to capture the hidden structure that governs these co-occurrences.\n",
    "\n",
    "In pLSA, each document is considered a mixture of topics, and each topic is represented as a distribution over words. The underlying assumption is that the observed words in a document are generated by first selecting a topic and then sampling a word from the corresponding word distribution of the selected topic. The goal of pLSA is to learn these latent topic distributions and the document-topic relationships that best explain the observed data.\n",
    "\n",
    "The pLSA model is trained using the Expectation-Maximization (EM) algorithm, which iteratively refines the estimates of topic distributions and document-topic relationships. During the Expectation step, the algorithm computes the posterior probabilities of topics given the words and the current model parameters. In the Maximization step, the model parameters are updated to maximize the likelihood of the observed data given these posterior probabilities.\n",
    "\n",
    "While pLSA has been successful in revealing latent topics in text data, it has some limitations:\n",
    "\n",
    "1.  Overfitting: pLSA is prone to overfitting due to the lack of regularization or priors in the model. This can result in poor generalization to new, unseen documents.\n",
    "2.  Parameter growth: The number of parameters in pLSA grows linearly with the number of documents, making it computationally expensive for large-scale document collections.\n",
    "3.  No generative model for new documents: pLSA does not provide an explicit generative model for creating topic distributions for new documents, which makes it difficult to apply the learned model to new data.\n",
    "\n",
    "Despite these limitations, pLSA laid the foundation for more advanced topic modeling techniques, such as Latent Dirichlet Allocation (LDA), which incorporates Dirichlet priors to address the overfitting issue and provides a more comprehensive generative model for document collections.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b49c6157",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "```{figure} ../figs/intro_nlp/topic/14.png\n",
    "---\n",
    "width: 70%\n",
    "name: fig-lda\n",
    "---\n",
    "Latent Dirichlet Allocation\n",
    "```\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a popular generative probabilistic model used for topic modeling. As the name suggests, it involves discovering hidden (latent) topics within a collection of documents, where Dirichlet refers to the type of probability distribution used for modeling. LDA assumes that each document is a mixture of topics, and each topic is a distribution over words. The main goal is to find these hidden topic distributions and their relationships to the documents.\n",
    "\n",
    "In LDA, given an $N \\times M$ document-term count matrix $X$, where $N$ represents the number of documents and $M$ denotes the number of unique terms or words in the corpus, we assume there are $K$ topics to be discovered. The number of topics, $K$, is a tunable hyperparameter, and its optimal value can be found using methods like coherence score analysis.\n",
    "\n",
    "LDA, similar to PCA and NMF, aims to factorize the document-term matrix $X$ into two lower-dimensional matrices:\n",
    "\n",
    "1.  An $N \\times K$ document-topic matrix: This matrix represents the relationship between documents and topics, with each element indicating the contribution of a particular topic to a specific document.\n",
    "2.  A $K \\times M$ topic-term matrix: This matrix represents the relationship between topics and terms, with each element denoting the probability of a word belonging to a specific topic.\n",
    "\n",
    "LDA operates under the assumption that documents are generated through the following process:\n",
    "\n",
    "1.  For each document, choose a distribution over topics.\n",
    "2.  For each word in the document: a. Choose a topic according to the document's distribution over topics. b. Choose a word from the topic's distribution over words.\n",
    "\n",
    "By applying LDA, we can infer the hidden topic structure of the documents, making it easier to understand, navigate, and analyze large-scale unstructured text data. LDA has been widely used for various applications, including document clustering, text summarization, and information retrieval, among others.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0a208a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "```{figure} ../figs/intro_nlp/topic/16.png\n",
    "---\n",
    "width: 70%\n",
    "name: fig-lda-example\n",
    "---\n",
    "LDA Example\n",
    "```\n",
    "\n",
    "When applying Latent Dirichlet Allocation (LDA) to a set of questions, the goal is to discover the hidden topic structure that characterizes these questions. For instance, consider the following four questions:\n",
    "\n",
    "1.  How do football players stay safe?\n",
    "2.  What is the most hated NFL football team of all time?\n",
    "3.  Who is the greatest political leader in the world and why?\n",
    "4.  Why do people treat politics like it's a football team or some kind of sport?\n",
    "\n",
    "Fitting LDA to these questions would result in topic assignments that might look like the following:\n",
    "\n",
    "- Question 1: 100% Topic A\n",
    "- Question 2: 90% Topic A, 10% Topic C\n",
    "- Question 3: 100% Topic B\n",
    "- Question 4: 40% Topic A, 60% Topic B\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7498987",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this example, Topic A appears to be related to sports, while Topic B is associated with politics, and Topic C might represent a more specific aspect of football, such as team rivalries or sentiments.\n",
    "\n",
    "Using these topic assignments, we can then infer the topics for each original question:\n",
    "\n",
    "1.  How do football players stay safe? (Sport)\n",
    "2.  What is the most hated NFL football team of all time? (Sport, with a small contribution from team rivalries or sentiments)\n",
    "3.  Who is the greatest political leader in the world and why? (Politics)\n",
    "4.  Why do people treat politics like it's a football team or some kind of sport? (Sport, Politics)\n",
    "\n",
    "```{figure} ../figs/intro_nlp/topic/17.png\n",
    "---\n",
    "width: 70%\n",
    "name: fig-lda-example-topics\n",
    "---\n",
    "LDA Example Topics\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c58bad9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "LDA represents each question as a probability distribution of topics and each topic as a probability distribution of words. In other words, every question can be seen as a mixture of topics, and every topic can be viewed as a mixture of words.\n",
    "\n",
    "```{figure} ../figs/intro_nlp/topic/18.png\n",
    "---\n",
    "width: 70%\n",
    "name: fig-lda-topic-dist\n",
    "---\n",
    "LDA Example Topic Distribution\n",
    "```\n",
    "\n",
    "When fitting LDA to a collection of questions or documents, the algorithm attempts to find the best topic mix and word mix that explain the observed data. This process uncovers the latent topic structure in the data, providing valuable insights into the thematic relationships between questions or documents and facilitating more efficient text data exploration and analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8df7c4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "```{figure} ../figs/intro_nlp/topic/19.png\n",
    "---\n",
    "width: 70%\n",
    "name: fig-nmf\n",
    "---\n",
    "Non-negative Matrix Factorization\n",
    "```\n",
    "\n",
    "Non-negative Matrix Factorization (NMF) is a linear algebraic method used for dimensionality reduction and data representation. It has been widely applied in various fields, including image processing, signal processing, and text mining, where it has been utilized for topic modeling. NMF was introduced by Lee and Seung in 1999 as a method to decompose a non-negative data matrix into two non-negative lower-dimensional matrices that, when multiplied, approximate the original data matrix.\n",
    "\n",
    "Given a non-negative document-term matrix $X$ of size $N \\times M$, where $N$ is the number of documents and $M$ is the number of unique terms in the corpus, NMF aims to find two non-negative matrices $W$ and $H$ such that $X \\approx WH$, where $W$ is an $N \\times K$ document-topic matrix and $H$ is a $K \\times M$ topic-term matrix. Here, $K$ is the number of topics or latent factors.\n",
    "\n",
    "The primary objective of NMF is to minimize the reconstruction error between the original matrix $X$ and its approximation $WH$. This is achieved by iteratively updating the elements of $W$ and $H$ using multiplicative update rules or other optimization algorithms, such as gradient descent or alternating least squares."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15862809",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Consider the following three questions:\n",
    "\n",
    "1.  How do football players stay safe?\n",
    "2.  What is the most hated NFL football team of all time?\n",
    "3.  Who is the greatest political leader in the world and why?\n",
    "\n",
    "To apply Non-negative Matrix Factorization (NMF) for topic modeling on these questions, we first create a term-document matrix that represents the frequency of terms in each question. This matrix will have rows representing unique terms and columns representing the questions.\n",
    "\n",
    "Next, we choose the number of topics, k=2 in this case, and decompose the term-document matrix into two non-negative matrices: a term-topic matrix (n words by k topics) and a document-topic matrix (k topics by m original documents).\n",
    "\n",
    "The term-topic matrix represents the relationship between terms and topics, where each topic is characterized by a distribution of words. The document-topic matrix represents the relationship between documents (questions) and topics, where each document is a mixture of topics with varying proportions.\n",
    "\n",
    "```{figure} ../figs/intro_nlp/topic/20.png\n",
    "---\n",
    "width: 70%\n",
    "name: fig-nmf-example\n",
    "---\n",
    "Non-negative Matrix Factorization Example\n",
    "```\n",
    "\n",
    "After applying NMF, we can examine the resulting term-topic and document-topic matrices to identify the underlying topics and their association with the original questions. For instance, we might find that Topic 1 is related to football and sports, while Topic 2 is related to politics and leadership. The document-topic matrix would then show the extent to which each question is associated with these topics, allowing us to interpret the hidden topic structure in the data.\n",
    "\n",
    "```{figure} ../figs/intro_nlp/topic/22.png\n",
    "---\n",
    "width: 70%\n",
    "name: fig-nmf-example2\n",
    "---\n",
    "Non-negative Matrix Factorization Example Result\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a85c361c",
   "metadata": {},
   "source": [
    "NMF has several attractive properties that make it suitable for topic modeling:\n",
    "\n",
    "1.  Non-negativity constraint: The non-negativity constraint on the factor matrices ensures that the resulting topics and their relationships to documents are interpretable, as they reflect additive combinations of the original features.\n",
    "2.  Sparsity: NMF often results in sparse representations, where each document is associated with only a few topics, and each topic is characterized by a limited set of words. This sparsity promotes interpretability and simplifies the identification of meaningful topics.\n",
    "3.  Flexibility: NMF can be adapted to different problem settings by incorporating additional constraints or using different objective functions, such as the Kullback-Leibler divergence or the Itakura-Saito divergence, to better capture the underlying structure of the data.\n",
    "\n",
    "However, NMF also has some limitations:\n",
    "\n",
    "1.  Lack of probabilistic foundation: Unlike LDA, NMF is not based on a probabilistic generative model, which may limit its applicability in certain scenarios where probabilistic interpretations are desired.\n",
    "2.  Non-unique solutions: NMF does not guarantee unique factorization, as there can be multiple combinations of $W$ and $H$ that approximate the original matrix $X$. This can lead to inconsistencies in the extracted topics.\n",
    "\n",
    "Despite these limitations, NMF remains a popular and useful technique for topic modeling, as it often provides interpretable and meaningful topic representations in a computationally efficient manner.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5982a541",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Topic Modeling Example using NMF and SVD\n",
    "\n",
    "### The problem\n",
    "\n",
    "In this example, we will perform topic modeling on a dataset of newsgroup documents using two matrix factorization techniques: Non-negative Matrix Factorization (NMF) and Singular Value Decomposition (SVD).\n",
    "\n",
    "**Term-document matrix**:\n",
    "\n",
    "We start by creating a term-document matrix, which represents the frequency of terms in each document. The goal is to decompose this matrix into the product of one tall, thin matrix and one wide, short matrix (possibly with a diagonal matrix in between).\n",
    "\n",
    "It is important to note that this representation does not consider word order or sentence structure and is an example of a **bag of words** approach.\n",
    "\n",
    "```{figure} ../figs/intro_nlp/topic/document_term.png\n",
    "---\n",
    "width: 70%\n",
    "name: fig-document-term\n",
    "---\n",
    "Term-Document Matrix ([source](http://player.slideplayer.com/15/4528582/#))\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27911b39",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "The idea behind matrix factorization is to find an optimal way of representing the original term-document matrix using lower-dimensional matrices that capture the latent topic structure in the data.\n",
    "\n",
    "We will use a dataset of newsgroup documents belonging to several different categories and find topics (groups of words) for them. Knowing the actual categories helps us evaluate whether the discovered topics make sense.\n",
    "\n",
    "We will try this with two different matrix factorizations: **Singular Value Decomposition (SVD)** and **Non-negative Matrix Factorization (NMF)**.\n",
    "\n",
    "- **Data source**: The dataset used in this example is the 20 Newsgroups dataset, which contains 18,000 newsgroup posts across 20 topics. Newsgroups were popular discussion groups on Usenet in the 80s and 90s before the web took off. We will focus on four categories for this example: \"alt.atheism\", \"talk.religion.misc\", \"comp.graphics\", and \"sci.space\". To remove noise from the data, we will exclude headers, footers, and quotes from the posts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba565f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from scikit-learn) (1.9.3)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from scikit-learn) (1.23.5)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.2.2 threadpoolctl-3.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install the necessary packages\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7beb55f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yjlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yjlee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/yjlee/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import decomposition\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57d793c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2034,), (2034,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [\"alt.atheism\", \"talk.religion.misc\", \"comp.graphics\", \"sci.space\"]\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, remove=remove\n",
    ")\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset=\"test\", categories=categories, remove=remove\n",
    ")\n",
    "newsgroups_train.filenames.shape, newsgroups_train.target.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "210d48bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's take a look at some of the data. Can you guess which category these messages belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a687a32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(newsgroups_train.data[:2]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8b87968",
   "metadata": {},
   "source": [
    "The `target` attribute is the integer index of the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5f01b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['comp.graphics', 'talk.religion.misc', 'sci.space'], dtype='<U18')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efcbf0e6",
   "metadata": {},
   "source": [
    "In this example, we will use NMF and SVD to decompose the term-document matrix and extract latent topics from the newsgroup documents. By comparing the discovered topics with the actual categories, we can evaluate the effectiveness of these matrix factorization techniques in uncovering meaningful topic structures in text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "960aa35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 0, 2, 0, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65347572",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics, num_top_words = 6, 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a471e113",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "```{note}\n",
    "**Stopwords**\n",
    "\n",
    "Stopwords are extremely common words that appear to have little value in helping to identify or match documents based on user needs. In text processing and information retrieval, these words are often excluded from the analysis to reduce noise and improve efficiency. Examples of stopwords include \"a\", \"an\", \"the\", \"and\", and \"in\".\n",
    "\n",
    "Over time, the trend in information retrieval systems has shifted from using large stop lists (200-300 terms) to very small stop lists (7-12 terms), and eventually to not using stop lists at all. Web search engines typically do not use stop lists.\n",
    "\n",
    "**Stemming and Lemmatization**\n",
    "\n",
    "Stemming and lemmatization are text preprocessing techniques that aim to generate the root form of words. This is particularly useful in information retrieval, as it helps group similar words together and improves search accuracy. Consider the following words:\n",
    "\n",
    "- organize, organizes, and organizing\n",
    "- democracy, democratic, and democratization\n",
    "\n",
    "Both stemming and lemmatization aim to reduce these words to their base forms, making it easier to identify their similarities.\n",
    "\n",
    "Lemmatization uses linguistic rules and knowledge about a language to generate the root form of words. The resulting tokens are actual words that carry meaning in the language. In contrast, stemming is a simpler and more heuristic-based approach that truncates words by chopping off their ends. The resulting tokens may not always be real words. As a result, stemming is faster than lemmatization but may not be as accurate.\n",
    "\n",
    "In summary, stopwords, stemming, and lemmatization are essential techniques in text processing and information retrieval. They help reduce noise, improve efficiency, and enhance the overall effectiveness of search and topic modeling tasks by simplifying the input text and generating meaningful word representations.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd7c9810",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "print(len(STOPWORDS))\n",
    "print(STOPWORDS[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea58e460",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import stem\n",
    "\n",
    "wnl = stem.WordNetLemmatizer()\n",
    "porter = stem.porter.PorterStemmer()\n",
    "\n",
    "word_list = [\"feet\", \"foot\", \"foots\", \"footing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "348d8a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foot', 'foot', 'foot', 'footing']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wnl.lemmatize(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8986738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feet', 'foot', 'foot', 'foot']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(word) for word in word_list]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4385bf26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Data Processing\n",
    "\n",
    "In this step, we will process the newsgroup data by extracting word counts using scikit-learn's `CountVectorizer`. This method will create a term-document matrix representing the frequency of terms in each document. In a later lesson, we will learn how to create our own version of `CountVectorizer` to gain a better understanding of what happens under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b7963a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 (2034, 26576)\n",
      "(26576,)\n",
      "['zurich' 'zurvanism' 'zus' 'zvi' 'zwaartepunten' 'zwak' 'zwakke' 'zware'\n",
      " 'zwarte' 'zyxel']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer with English stopwords\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Fit the vectorizer to the newsgroups_train.data and transform the text data\n",
    "vectors = vectorizer.fit_transform(\n",
    "    newsgroups_train.data\n",
    ").todense()  # (documents, vocab)\n",
    "\n",
    "# Print the shape of the term-document matrix (documents, vocabulary)\n",
    "print(len(newsgroups_train.data), vectors.shape)\n",
    "\n",
    "# Retrieve the vocabulary from the vectorizer\n",
    "vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the shape of the vocabulary\n",
    "print(vocab.shape)\n",
    "\n",
    "# Display the last 10 terms in the vocabulary\n",
    "print(vocab[-10:])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "466229fe",
   "metadata": {},
   "source": [
    "In this code snippet, we first import `CountVectorizer` from scikit-learn's `feature_extraction.text` module. We then initialize the `CountVectorizer` object, specifying that we want to remove English stopwords from the text data. Next, we fit the vectorizer to the training data and transform the text data into a term-document matrix. Finally, we retrieve the vocabulary from the vectorizer and display its shape and the last 10 terms.\n",
    "\n",
    "By applying `CountVectorizer` to the newsgroup data, we can transform the text data into a structured format suitable for further analysis using NMF and SVD."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c7d46be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "SVD is a matrix factorization technique that can be used to identify latent topics in a term-document matrix. We expect the topics to be **orthogonal** since words that appear frequently in one topic should appear less frequently in others, making them suitable for separating topics.\n",
    "\n",
    "The SVD algorithm factorizes a matrix into one matrix with **orthogonal columns** and one with **orthogonal rows**, along with a diagonal matrix that contains the **relative importance** of each factor. SVD is an **exact decomposition** as the resulting matrices fully cover the original matrix. It is widely used in data science for tasks such as semantic analysis, collaborative filtering, data compression, and principal component analysis. Latent Semantic Analysis (LSA) also employs SVD.\n",
    "\n",
    "```{figure} ../figs/intro_nlp/topic/svd_fb.png\n",
    "---\n",
    "width: 70%\n",
    "name: fig-svd-fb\n",
    "---\n",
    "Fast Randomized SVD ([source](https://research.fb.com/fast-randomized-svd/))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a52cc0c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Latent Semantic Analysis (LSA) uses SVD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b15cedff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 2034) (2034,) (2034, 26576)\n",
      "CPU times: user 9min 13s, sys: 7min 22s, total: 16min 35s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from scipy import linalg\n",
    "\n",
    "# Perform SVD on the term-document matrix\n",
    "U, s, Vh = linalg.svd(vectors, full_matrices=False)\n",
    "\n",
    "# Print the shapes of the resulting matrices\n",
    "print(U.shape, s.shape, Vh.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07e62031",
   "metadata": {},
   "source": [
    "In this code snippet, we import the linalg module from SciPy and use its svd function to perform SVD on the term-document matrix. We then print the shapes of the resulting matrices U, s, and Vh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3296249",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_top_words = 8\n",
    "\n",
    "\n",
    "def show_topics(a):\n",
    "    # Helper function to retrieve top words for each topic\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[: -num_top_words - 1 : -1]]\n",
    "    topic_words = [top_words(t) for t in a]\n",
    "    return [\" \".join(t) for t in topic_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29a71994",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['critus ditto propagandist surname galacticentric kindergarten surreal imaginative',\n",
       " 'jpeg gif file color quality image jfif format',\n",
       " 'graphics edu pub mail 128 3d ray ftp',\n",
       " 'jesus god matthew people atheists atheism does graphics',\n",
       " 'image data processing analysis software available tools display',\n",
       " 'god atheists atheism religious believe religion argument true',\n",
       " 'space nasa lunar mars probe moon missions probes',\n",
       " 'image probe surface lunar mars probes moon orbit',\n",
       " 'argument fallacy conclusion example true ad argumentum premises',\n",
       " 'space larson image theory universe physical nasa material']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the top topics extracted using SVD\n",
    "show_topics(Vh[:10])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7124d609",
   "metadata": {},
   "source": [
    "In this part, we define a function `show_topics` to display the top words associated with each topic. We then use this function to display the top topics extracted using SVD. The results match the expected clusters, even though this is an **unsupervised algorithm** - meaning we never provided information about the grouping of our documents during the process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3356eb3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "In contrast to constraining our factors to be _orthogonal_ as in SVD, we can constrain them to be _non-negative_. NMF is a factorization of a non-negative data set $V$ into non-negative matrices $W$ and $H$: $V=WH$. Often, positive factors are **more easily interpretable**, contributing to NMF's popularity.\n",
    "\n",
    "NMF is a non-exact factorization that factors a matrix into one skinny positive matrix and one short positive matrix. It is NP-hard and non-unique, with various versions created by adding different constraints.\n",
    "\n",
    "```{figure} ../figs/intro_nlp/topic/nmf_doc.png\n",
    "---\n",
    "width: 70%\n",
    "name: fig-nmf-doc\n",
    "---\n",
    "Non-negative Matrix Factorization ([source](http://perso.telecom-paristech.fr/~essid/teach/NMF_tutorial_ICME-2014.pdf))\n",
    "```\n",
    "\n",
    "#### NMF Applications\n",
    "\n",
    "NMF has been applied in various fields, such as:\n",
    "\n",
    "- [Face Decompositions](http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py)\n",
    "- [Collaborative Filtering, eg movie recommendations](http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/)\n",
    "- [Audio source separation](https://pdfs.semanticscholar.org/cc88/0b24791349df39c5d9b8c352911a0417df34.pdf)\n",
    "- [Chemistry](http://ieeexplore.ieee.org/document/1532909/)\n",
    "- [Bioinformatics](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0485-4)\n",
    "- [Gene Expression](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2623306/)\n",
    "\n",
    "#### NMF using scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cab6059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 1min 33s, total: 3min 9s\n",
      "Wall time: 2.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import decomposition\n",
    "\n",
    "# Perform NMF on the term-document matrix\n",
    "m, n = vectors.shape\n",
    "d = 5  # num topics\n",
    "\n",
    "clf = decomposition.NMF(n_components=d, random_state=1)\n",
    "\n",
    "W1 = clf.fit_transform(np.asarray(vectors))\n",
    "H1 = clf.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "667b4dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jpeg image gif file color images format quality',\n",
       " 'edu graphics pub mail 128 ray ftp send',\n",
       " 'space launch satellite nasa commercial satellites year market',\n",
       " 'jesus god people matthew atheists does atheism said',\n",
       " 'image data available software processing ftp edu analysis']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the top topics extracted using NMF\n",
    "show_topics(H1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3351516e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In this code snippet, we import the `decomposition` module from scikit-learn and use its `NMF` class to perform NMF on the term-document matrix. We then display the top topics extracted using NMF.\n",
    "\n",
    "```{note}\n",
    "-   For NMF, the matrix needs to be at least as tall as it is wide, or we get an error with `fit_transform`.\n",
    "-   You can use `min_df` in `CountVectorizer` to only consider words that appear in at least k of the split texts.\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4327e31c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### runcated SVD\n",
    "\n",
    "When we calculated NMF, we saved a lot of time by only calculating the subset of columns we were interested in. We can achieve a similar benefit with SVD by using truncated SVD. Truncated SVD focuses on the vectors corresponding to the **largest** singular values, which can save computation time.\n",
    "\n",
    "#### Shortcomings of classical algorithms for decomposition:\n",
    "\n",
    "-   Matrices can be \"stupendously big.\"\n",
    "-   Data are often **missing or inaccurate**. Spending extra computational resources may not be worthwhile when the imprecision of input limits the precision of the output.\n",
    "-   **Data transfer** plays a significant role in the time complexity of algorithms. Techniques that require fewer passes over the data might be faster, even if they require more floating point operations (flops).\n",
    "-   It is essential to take advantage of **GPUs**.\n",
    "\n",
    "#### Advantages of randomized algorithms:\n",
    "\n",
    "-   Inherently stable.\n",
    "-   Performance guarantees do not depend on subtle spectral properties.\n",
    "-   The required matrix-vector products can be performed in parallel.\n",
    "\n",
    "(source: [Halko](https://arxiv.org/abs/0909.4061))\n",
    "\n",
    "#### Timing comparison\n",
    "\n",
    "In the following code snippets, we compare the computation time for full SVD and randomized SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f18515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 29s, sys: 10min 50s, total: 21min 20s\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Full SVD\n",
    "u, s, v = np.linalg.svd(vectors, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "725317ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 2min 30s, total: 3min 32s\n",
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Randomized SVD\n",
    "u, s, v = decomposition.randomized_svd(vectors, n_components=10, random_state=123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d25c96b0",
   "metadata": {},
   "source": [
    "These code snippets demonstrate the time difference between full SVD and randomized SVD, showing that the latter can be more efficient in certain cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8746b609",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Document Clustering\n",
    "\n",
    "Document clustering is a technique used in text mining and natural language processing to group documents based on their content or other features. This can help in organizing, summarizing, and extracting meaningful information from large collections of text. Some common applications include topic modeling, document classification, and information retrieval.\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "Cosine similarity is a popular similarity measure used in text mining and information retrieval. It measures the cosine of the angle between two non-zero vectors, with a range between -1 and 1. In the context of document clustering, each document is represented as a vector of term frequencies or other weights, and the cosine similarity quantifies the similarity between two documents.\n",
    "\n",
    "The cosine similarity between two vectors A and B can be calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum_{i=1}^n A_iB_i}{\\sqrt{\\sum_{i=1}^n A_i^2}\\sqrt{\\sum_{i=1}^n B_i^2}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76985296",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "```{figure} ../figs/intro_nlp/topic/1.png\n",
    "---\n",
    "width: 70%\n",
    "name: fig-cosine-sim\n",
    "---\n",
    "Cosine Similarity Example\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "```{figure} ../figs/intro_nlp/topic/3-800x852.png\n",
    "---\n",
    "width: 60%\n",
    "name: fig-projected-docs\n",
    "---\n",
    "Projecting Documents onto a 3D Space\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2260b115",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "Before applying clustering algorithms to document collections, it is often beneficial to perform dimensionality reduction. High-dimensional data can be noisy and may result in poor clustering performance due to the curse of dimensionality. Dimensionality reduction techniques, such as PCA, SVD, or t-SNE, can help in reducing the number of dimensions while preserving the essential structure and relationships in the data. This can lead to more efficient and accurate clustering results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ca121ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Clustering Algorithms\n",
    "\n",
    "There are several clustering algorithms that can be applied to document clustering. Some popular ones include:\n",
    "\n",
    "1.  **K-means**: K-means is a widely-used partitioning clustering algorithm that aims to minimize the sum of squared distances between data points and their respective cluster centroids. K-means requires the number of clusters (K) to be specified beforehand and is sensitive to the initial placement of cluster centroids.\n",
    "2.  **DBSCAN**: Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a density-based clustering algorithm that identifies clusters based on the density of data points in a region. DBSCAN does not require the number of clusters to be specified beforehand and can handle noise and outliers effectively.\n",
    "3.  **Agglomerative Hierarchical Clustering**: This is a bottom-up approach to hierarchical clustering that starts with each data point as its own cluster and iteratively merges the closest pair of clusters until only one cluster remains. The result is a tree-like structure, called a dendrogram, that can be cut at different levels to obtain the desired number of clusters.\n",
    "4.  **Latent Dirichlet Allocation (LDA)**: LDA is a generative probabilistic model for collections of discrete data, such as text corpora. It is particularly useful for topic modeling, where the goal is to discover the underlying topics in a document collection. LDA represents documents as mixtures of topics and topics as mixtures of words, and it learns these representations using a Dirichlet prior.\n",
    "\n",
    "To perform document clustering, first preprocess the text data (e.g., tokenization, stemming, and removing stop words), convert the documents into a suitable vector representation (e.g., TF-IDF or word embeddings), and then apply the desired clustering algorithm to group similar documents together.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
