

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Transformers &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../../_static/tabs.js"></script>
    <script src="../../../_static/js/hoverxref.js"></script>
    <script src="../../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/nlp_deep/transformers/index';</script>
    <link rel="canonical" href="https://lecture.entelecheia.ai/lectures/nlp_deep/transformers/index.html" />
    <link rel="shortcut icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="BERT: Bidirectional Encoder Representations from Transformers" href="bert.html" />
    <link rel="prev" title="Pretrained Language Models" href="../llms/plms.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content"><p>
  <strong>Announcement:</strong>
  You can install the accompanying AI tutor <a href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd" target="_blank">here</a>.
  <a class="reference external" href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd" target="_blank"><img alt="chrome-web-store-image" src="https://img.shields.io/chrome-web-store/v/lfgfgbomindbccgidgalhhndggddpagd" /></a>
</p>
</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_intro/index.html">Introduction to NLP</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/intro/index.html">Introduction</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/apps/index.html">NLP Applications</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/apps/research1.html">Research Part I</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/apps/research2.html">Research Part II</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/lm/index.html">Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/lm/ngram.html">N-gram Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/lm/usage.html">Usage of Language Models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/datasets/index.html">Datasets</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/datasets/corpus.html">Text Data Collection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/datasets/lab-dart.html">Lab: Crawling DART Data</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/topic/index.html">Topic Modeling</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/methods.html">Topic Modeling Methodologies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/lab-methods.html">Lab: Topic Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/lab-coherence.html">Lab: Topic Coherence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/tomotopy.html">Lab: Tomotopy</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/sentiments/index.html">Sentiment Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lexicon.html">Lexicon-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/ml.html">Machine Learning-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lab-lexicon.html">Lab: Lexicon-based Sentiment Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lab-ml.html">Lab: ML-based Sentiment Classification</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/tokenization/index.html">Tokenization</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/tokenization.html">Understanding the Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/pos.html">Part-of-Speech Tagging and Parsing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/ngrams.html">N-grams for Tokenization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/korean.html">Tokenization in Korean</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/lab-tokenization.html">Lab: Tokenization and Pre-processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/lab-korean.html">Lab: Korean Text Processing</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/vectorization/index.html">Vector Representation</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/semantics.html">Vector Semantics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/bow.html">Bags of Words Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/tf-idf.html">TF-IDF Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/similarity.html">Word Similarity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/lab-similarity.html">Lab: Word Similarity</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/embeddings/index.html">Word Embeddings</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/nlm.html">Neural Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/w2v.html">Word2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/glove.html">GloVe</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/fasttext.html">FastText</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Deep Learning for NLP</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../llms/index.html">Large Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../llms/zeroshot.html">Zero Shot and Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llms/decoding.html">Decoding and Search Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llms/plms.html">Pretrained Language Models</a></li>
</ul>
</li>
<li class="toctree-l2 current active has-children"><a class="current reference internal" href="#">Transformers</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="bert.html">BERT: Bidirectional Encoder Representations from Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="bertviz.html">BERT: Visualizing Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../datasets/index.html">Datasets</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../datasets/mc4.html">mC4 Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../datasets/lab-eda.html">Lab: Exploratory Data Analysis (EDA)</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tokenization/index.html">Tokenization</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../tokenization/subword.html">Subword Tokenization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tokenization/pipeline.html">Tokenization Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tokenization/bpe.html">BPE Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tokenization/wordpiece.html">WordPiece Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tokenization/unigram.html">Unigram Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tokenization/sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tokenization/lab-train-tokenizers.html">Lab: Training Tokenizers</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../training/index.html">Training Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../training/lab-pretrain-mlm.html">Lab: Pretraining LMs - MLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../training/lab-pretrain-clm.html">Lab: Pretraining LMs - CLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../training/lab-finetune-mlm.html">Lab: Finetuining a MLM</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../chatbots/index.html">Conversational AI and Chatbots</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../chatbots/rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chatbots/detectGPT.html">How to Spot Machine-Written Texts</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_advances/index.html">Advances in AI and NLP</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/thesis.html">Writing a Thesis</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_advances/gpt/index.html">Generative Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/detectGPT.html">DetectGPT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/gpt4.html">GPT-4</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/camelids.html">Meet the Camelids: A Family of LLMs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/sam/index.html">Segment Anything</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../aiart/index.html">AI Art (Generative AI)</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/intro/index.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/brave/index.html">A Brave New World</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/aiart/index.html">Art and Music in Light of AI</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../aiart/text-to-image/index.html">Text-to-Image Models</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/imagen.html">Imagen</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/motion-capture-and-synthesis/index.html">Motion Capture and Motion Synthesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/robot/index.html">Robot Drawing System</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/devops/index.html">DevOps</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/gitops.html">GitOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/devsecops.html">DevSecOps</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/dotfiles/index.html">Dotfiles</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai">Dotfiles Project</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dotdrop/">Dotdrop Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/github/">GitHub Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/ssh-gpg-age/">SSH, GPG, and Age Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/sops/">SOPS Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/pass/">Pass and Passage Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/doppler/">Doppler Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dockerfiles/">Dockerfiles Usage</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/security/index.html">Security Management</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/auth-enc-sign.html">Authentication, Encryption, and Signing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/age-gpg-ssh.html">SSH, GPG, and AGE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/pass.html">Unix Password Managers</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/github/index.html">GitHub Workflow</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/fork-pull.html">Github’s Fork &amp; Pull Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/template.html">Project Templating Tools</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-python.entelecheia.ai/">Hyperfast Python Template</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-template.entelecheia.ai/">Hyperfast Template</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/containerization/index.html">Containerization</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/docker.html">Docker</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/containerd.html"><code class="docutils literal notranslate"><span class="pre">containerd</span></code></a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/simple-pipeline/index.html">Simple MLOps Pipeline</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/simple-pipeline/server.html">Server Setup &amp; Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/simple-pipeline/vpn.html">VPN Connectivity</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/llmops/index.html">LLMOps</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/llmops/bentoml.html">Introduction to BentoML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/llmops/bentochain.html">Deploy a Voice-Based Chatbot with BentoML, LangChain, and Gradio</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ds/index.html">Data Science for Economics and Finance</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ds/fomc/index.html">Textual Analysis of FOMC contents</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/01_numerical_data.html">Preparing Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/02_textual_data.html">Preparing Textual Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/03_EDA_numericals1.html">EDA on Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/03_EDA_numericals2.html">EDA on Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/04_training_datasets.html">Create Training Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/05_features.html">Visualizing Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/06_AutoML.html">Checking Baseline with AutoML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/07_predict_sentiments.html">Predicting Sentiments of FOMC Corpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/08_EDA_sentiments1.html">EDA on Sentiments: Correlation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/08_EDA_sentiments2.html">EDA on Sentiment Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/09_visualize_features.html">Visualize Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/10_monetary_shocks.html">Monetary Policy Shocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/fomc/11_AutoML_with_tones.html">Predicting the next decisions with tones</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ds/esg-ratings/index.html">ESG Ratings</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ds/esg-ratings/prepare_datasets.html">Preparing training datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/esg-ratings/improve_datasets.html">Improving classification datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/esg-ratings/train_classifiers.html">Training Classifiers for ESG Ratings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/esg-ratings/build_news_corpus.html">Building <code class="docutils literal notranslate"><span class="pre">econ_news_kr</span></code> corpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/esg-ratings/predict_esg_classes.html">Predicting ESG Categories and Polarities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/esg-ratings/cross_validate_datasets.html">Cross validating datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/esg-ratings/prepare_datasets_for_labeling.html">Preparing active learning data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ds/esg-ratings/all_in_one_pipeline.html">Putting them together in a pipeline</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../about/lecture-bot.html">LectureBot for ἐντελέχεια.άι</a></li>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference external" href="https://course.entelecheia.ai">course.entelecheia.ai</a></li>
<li class="toctree-l1"><a class="reference external" href="https://research.entelecheia.ai">research.entelecheia.ai</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about/index.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/entelecheia/lecture/blob/main/book/lectures/nlp_deep/transformers/index.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/lectures/nlp_deep/transformers/index.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Transformers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-transformers">Why do we need transformers?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-is-all-you-need">Attention is all you need?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-implement-this-attention-mechanism">How do we implement this attention mechanism?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-head attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tranformer-architecture">Tranformer Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-the-transformer-architecture">Problems with the Transformer architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-is-not-all-you-need">Attention Is Not All You Need</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#so-why-are-transformers-so-powerful">So why are transformers so powerful?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vision-transformers">Vision Transformers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-input-images-into-a-transformer">How to input images into a transformer?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vision-transformer-architecture">Vision Transformer Architecture</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-in-transformer">Transformer in Transformer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#timesformers">TimeSformers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-machine-learning">Multimodal Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vatt-transformers-for-multimodal-self-supervised-learning">VATT: Transformers for Multimodal Self-Supervised Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gato-a-generalist-agent">GATO: A Generalist Agent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next">Next</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="transformers">
<h1>Transformers<a class="headerlink" href="#transformers" title="Permalink to this heading">#</a></h1>
<p>On Transformers, TimeSformers, And Attention</p>
<a class="bg-primary mb-1 reference internal image-reference" href="../../../_images/entelecheia_transformers.png"><img alt="transformers" class="bg-primary mb-1 align-center" src="../../../_images/entelecheia_transformers.png" style="width: 70%;" /></a>
<p>Transformers are a highly influential and powerful class of Deep Learning models that have become a standard in numerous Natural Language Processing (NLP) tasks, and are on the verge of revolutionizing the field of Computer Vision as well.</p>
<p>In 2017, researchers from Google Brain published a groundbreaking paper titled “Attention Is All You Need” <span id="id1">[<a class="reference internal" href="../../../about/index.html#id5" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017.">Vaswani <em>et al.</em>, 2017</a>]</span>. This paper introduced the Transformer model, which has since become a major force in the field of deep learning, particularly for NLP tasks.</p>
<p>The Transformer model is built upon the concept of attention, a mechanism that allows the model to focus on specific parts of the input data. Attention enables the model to weigh the importance of different input elements, thereby allowing it to concentrate on the most relevant aspects when processing data. This ability to focus on the most pertinent information has proven to be particularly effective for tasks that involve understanding and manipulating sequences of data, such as text or time-series information.</p>
<p>The Transformer model has demonstrated impressive performance across a wide range of NLP tasks. Some notable examples include machine translation, where the model converts text from one language to another; text summarization, which involves generating a concise summary of a longer text; and question answering, where the model provides answers to questions based on a given context.</p>
<p>Aside from its exceptional performance in NLP, the Transformer model has also shown great promise in other domains, such as image classification and speech recognition. In these tasks, the attention mechanism helps the model focus on the most significant features of the input data, which leads to better overall performance. As the model continues to be refined and adapted for different applications, it is poised to have a substantial impact on a wide range of fields, from NLP and computer vision to speech processing and beyond.</p>
<figure class="align-default" id="fig-transformers-history">
<a class="reference internal image-reference" href="../../../_images/transformers-history.jpeg"><img alt="../../../_images/transformers-history.jpeg" src="../../../_images/transformers-history.jpeg" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 98 </span><span class="caption-text">History of Transformers</span><a class="headerlink" href="#fig-transformers-history" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In 2020, Google Brain posed an intriguing question: “Will Transformers be as effective on images as they are on text?” To explore this, a team of researchers published a paper titled “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” <span id="id2">[<a class="reference internal" href="../../../about/index.html#id7" title="Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and others. An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.">Dosovitskiy <em>et al.</em>, 2020</a>]</span>. This paper demonstrated the potential of Transformer models to excel in various Computer Vision tasks, including image classification and object detection.</p>
<p>The authors of the paper proposed a new Transformer-based architecture, called Vision Transformer (ViT), which treats image patches as if they were words in a text. By dividing an image into fixed-size, non-overlapping patches, and then flattening and linearly embedding them into a sequence of tokens, the model can apply the powerful attention mechanisms of Transformers to image data. The ViT model has shown impressive results on a variety of Computer Vision tasks, proving that the Transformer architecture can be highly effective for image processing as well as text-based tasks.</p>
<p>At the beginning of 2021, Facebook researchers took the concept of Transformers even further by introducing TimeSformer, a new variation of the Transformer model specifically designed for video understanding. They published a paper titled “Is space-time attention all you need for video understanding?” <span id="id3">[<a class="reference internal" href="../../../about/index.html#id8" title="Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, volume 2, 4. 2021.">Bertasius <em>et al.</em>, 2021</a>]</span>. TimeSformer leverages the attention mechanism to process not only spatial information in video frames but also temporal information across multiple frames. This approach allows the model to recognize and analyze patterns in both space and time, making it well-suited for tasks such as action recognition and video classification.</p>
<p>These advancements in Transformer-based models for both image and video processing showcase the versatility of the Transformer architecture. By adapting the attention mechanism to different types of data, researchers are discovering new ways to leverage the power of Transformers across a wide range of domains.</p>
<section id="why-do-we-need-transformers">
<h2>Why do we need transformers?<a class="headerlink" href="#why-do-we-need-transformers" title="Permalink to this heading">#</a></h2>
<p>Transformers were introduced to address several limitations and challenges associated with previous models, particularly those based on Recurrent Neural Networks (RNNs). RNNs were designed to process sequences of data, such as text or audio, by maintaining an internal state that can capture information about previous elements in the sequence. However, there were a number of issues with RNNs that motivated the development of the Transformer architecture.</p>
<p>One of the main drawbacks of RNNs is their inherently sequential nature. In order to process an input sequence, RNNs must process each element one at a time, starting with the first and continuing through to the last. This sequential operation makes it challenging to parallelize RNN models, which in turn limits their computational efficiency and scalability. In contrast, Transformers can process all elements of a sequence simultaneously, enabling them to take full advantage of parallel computing resources and significantly improve training and inference times.</p>
<p>Another issue with RNNs is the difficulty they face in capturing long-range dependencies between distant elements within a sequence. Due to their sequential processing, RNNs can suffer from problems like gradient explosion or vanishing gradients, which make it challenging for them to learn and maintain information about relationships between far-apart elements. Transformers, on the other hand, leverage the attention mechanism to directly model relationships between all elements in the input, regardless of their positions in the sequence. This allows Transformers to better capture long-range dependencies and improves their overall performance in tasks that require understanding complex relationships within the data.</p>
<p>To illustrate the difference between RNNs and Transformers, consider the task of translating a sentence from English to Italian. With an RNN-based approach, the first word of the source sentence would be passed into an encoder along with an initial state. The resulting state would then be passed to a second encoder, together with the second word of the sentence, and so on, until the last word. The final state from the last encoder would then be fed into a decoder, which would generate the first translated word and a subsequent state. This state would be passed to another decoder, and the process would continue until the entire sentence was translated. This sequential approach is inherently slower and less efficient than the parallel processing enabled by Transformer models, which can consider the entire input sequence at once and generate translations more effectively.</p>
 <!-- ![](figs/problem-rnn.gif) -->
<figure class="align-default" id="fig-problem-rnn">
<a class="reference internal image-reference" href="../../../_images/problem-rnn.gif"><img alt="../../../_images/problem-rnn.gif" src="../../../_images/problem-rnn.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 99 </span><span class="caption-text">Problem with RNNs</span><a class="headerlink" href="#fig-problem-rnn" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="attention-is-all-you-need">
<h2>Attention is all you need?<a class="headerlink" href="#attention-is-all-you-need" title="Permalink to this heading">#</a></h2>
<figure class="align-default" id="fig-attention">
<a class="reference internal image-reference" href="../../../_images/attention.gif"><img alt="../../../_images/attention.gif" src="../../../_images/attention.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 100 </span><span class="caption-text">Attention</span><a class="headerlink" href="#fig-attention" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The attention mechanism is a key component of the Transformer architecture that allows it to extract relevant information from a sentence or sequence in a parallelized manner, as opposed to the sequential processing of traditional Recurrent Neural Networks (RNNs). The attention mechanism enables the model to selectively focus on different parts of the input data that are most relevant to the task at hand, and it can do so simultaneously for all elements in the sequence.</p>
<p>Consider the example sentence: <code class="docutils literal notranslate"><span class="pre">I</span> <span class="pre">gave</span> <span class="pre">my</span> <span class="pre">dog</span> <span class="pre">Charlie</span> <span class="pre">some</span> <span class="pre">food.</span></code></p>
<p>In this context, let’s explore how the attention mechanism can identify important relationships between words in the sentence to better understand its meaning.</p>
<ol class="arabic simple">
<li><p>Focusing on the word “gave”, we may want to determine which words in the sentence provide context to the action of giving. In this case, we could ask, “Who gave the food to the dog?” The attention mechanism would recognize the importance of the word “I” in answering this question, as it indicates the subject performing the action.</p></li>
<li><p>Similarly, we might want to determine the recipient of the action. By asking, “To whom did I give the food?”, the attention mechanism would identify the words “dog” and “Charlie” as crucial for understanding the recipient of the action.</p></li>
<li><p>Finally, to determine the object of the action, we could ask, “What did I give to the dog?” Here, the attention mechanism would focus on the word “food”, as it represents the object being given.</p></li>
</ol>
<p>In each of these cases, the attention mechanism can identify and focus on the most relevant words in the sentence to provide context and meaning to the central action of “gave”. By doing so, the model can extract essential information and relationships within the sentence in a highly parallelized and efficient manner. This powerful mechanism is at the core of the Transformer architecture, enabling it to excel in various natural language processing tasks, such as machine translation, text summarization, and question answering.</p>
<section id="how-do-we-implement-this-attention-mechanism">
<h3>How do we implement this attention mechanism?<a class="headerlink" href="#how-do-we-implement-this-attention-mechanism" title="Permalink to this heading">#</a></h3>
<figure class="align-default" id="fig-attention-calculate">
<a class="reference internal image-reference" href="../../../_images/attention-calculate.gif"><img alt="../../../_images/attention-calculate.gif" src="../../../_images/attention-calculate.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 101 </span><span class="caption-text">Attention mechanism</span><a class="headerlink" href="#fig-attention-calculate" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>To understand and implement the attention mechanism, let’s break it down into a series of steps. We’ll use an analogy with databases to help explain the process more clearly.</p>
<ol class="arabic simple">
<li><p><strong>Encoding words as vectors</strong>: First, we represent the sentence as a set of vectors. Each word in the sentence is encoded into a vector using a word embedding mechanism. We’ll call these vectors “keys” (K).</p></li>
<li><p><strong>Defining the query</strong>: We then define a query (Q) as a vector representing the word we want to focus on. The query can be a word from the same sentence (self-attention) or a word from another sentence (cross-attention).</p></li>
<li><p><strong>Computing similarity</strong>: We calculate the similarity between the query (Q) and each of the keys (K) in the sentence. This is typically done by computing the dot product between the query vector and the transpose of the key vectors. The result is a vector of scores, with each score representing the similarity between the query and a key.</p></li>
<li><p><strong>Normalization</strong>: We normalize the scores by applying the softmax function, which converts the scores into a probability distribution. The resulting vector contains the “attention weights” that signify the importance of each word in the sentence with respect to the query.</p></li>
<li><p><strong>Computing the context</strong>: We multiply the attention weights by the sentence vectors, which are the same dimension as the keys (K). This results in a context vector ©, which is a weighted sum of the words in the sentence, capturing the relevant context for the word we want to focus on.</p></li>
<li><p><strong>Linear transformation</strong>: Finally, the context vector © is passed through a linear layer, which is a fully connected layer, to obtain the final output of the attention mechanism.</p></li>
</ol>
<figure class="align-default" id="fig-attention-focus">
<a class="reference internal image-reference" href="../../../_images/attention-focus.jpeg"><img alt="../../../_images/attention-focus.jpeg" src="../../../_images/attention-focus.jpeg" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 102 </span><span class="caption-text">Attention implementation</span><a class="headerlink" href="#fig-attention-focus" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>To summarize, the attention mechanism involves encoding words in a sentence as vectors, defining a query vector for the word of interest, calculating the similarity between the query and each word in the sentence, and using the resulting attention weights to compute a context vector that captures the relevant information about the word of focus. This context vector is then transformed by a linear layer to obtain the final output.</p>
<p>By following these steps, the attention mechanism can selectively focus on different parts of the input sentence to provide the necessary context for understanding and processing the information. This powerful mechanism is at the core of the Transformer architecture and plays a crucial role in its success in various natural language processing tasks.</p>
</section>
<section id="multi-head-attention">
<h3>Multi-head attention<a class="headerlink" href="#multi-head-attention" title="Permalink to this heading">#</a></h3>
<p>The basic attention mechanism described earlier focuses on a single word and its context within a sentence. However, to effectively capture various relationships between words, we need to examine the input from multiple perspectives. This is where the multi-head attention mechanism comes into play.</p>
<p>Multi-head attention is an extension of the basic attention mechanism that utilizes multiple sets of queries, keys, and values, known as “heads”. Each head has its own set of learnable parameters and is designed to capture different aspects of the relationships between words in the input. By using multiple heads, the model can attend to different parts of the input simultaneously and understand the input more comprehensively.</p>
<p>Here’s how multi-head attention works:</p>
<ol class="arabic simple">
<li><p><strong>Multiple sets of queries, keys, and values</strong>: Instead of using a single set of query, key, and value vectors, we use multiple sets (or “heads”) with their own learnable parameters. Each head focuses on different aspects of the relationships between words in the input.</p></li>
<li><p><strong>Compute attention for each head</strong>: For each head, we perform the same attention mechanism steps as before, namely, computing similarity scores, normalizing the scores using the softmax function, computing the context vector, and transforming the context vector using a linear layer.</p></li>
<li><p><strong>Concatenate the results</strong>: Once we have obtained the context vectors for each head, we concatenate these vectors to form a single, combined context vector. This concatenated vector captures information from all the attention heads and provides a more comprehensive understanding of the relationships between words in the input.</p></li>
<li><p><strong>Final linear transformation</strong>: The concatenated vector is passed through an additional linear layer to produce the final output of the multi-head attention mechanism.</p></li>
</ol>
<figure class="align-default" id="fig-attention-multihead">
<a class="reference internal image-reference" href="../../../_images/attention-multihead.jpeg"><img alt="../../../_images/attention-multihead.jpeg" src="../../../_images/attention-multihead.jpeg" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 103 </span><span class="caption-text">Multi-head attention</span><a class="headerlink" href="#fig-attention-multihead" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In summary, multi-head attention enhances the basic attention mechanism by using multiple heads to focus on different parts of the input simultaneously. This allows the model to capture a more comprehensive understanding of the relationships between words in the input, ultimately improving its performance in various natural language processing tasks.</p>
</section>
</section>
<section id="tranformer-architecture">
<h2>Tranformer Architecture<a class="headerlink" href="#tranformer-architecture" title="Permalink to this heading">#</a></h2>
<p>The Transformer architecture, designed for various natural language processing tasks such as translation, consists of two primary components: the encoder and the decoder. Let’s consider a Transformer model trained to translate a sentence from English to Italian and examine the roles of the encoder and decoder in this process.</p>
<figure class="align-default" id="fig-transformer-architecture">
<a class="reference internal image-reference" href="../../../_images/transformer-architecture.gif"><img alt="../../../_images/transformer-architecture.gif" src="../../../_images/transformer-architecture.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 104 </span><span class="caption-text">Transformer architecture</span><a class="headerlink" href="#fig-transformer-architecture" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="encoder">
<h3>Encoder<a class="headerlink" href="#encoder" title="Permalink to this heading">#</a></h3>
<p>The encoder’s role is to convert the input sentence into a meaningful vector representation. The steps involved are as follows:</p>
<ol class="arabic simple">
<li><p><strong>Tokenization</strong>: The input sentence in English is first tokenized into individual words.</p></li>
<li><p><strong>Word Embedding</strong>: Each word is converted into a vector using a word embedding mechanism.</p></li>
<li><p><strong>Positional Encoding</strong>: Since the Transformer does not process the sentence sequentially, positional encoding vectors are added to the word embeddings to retain information about the order of the words in the sentence. These vectors are computed using sine and cosine functions and are of the same size as the word embedding vectors.</p></li>
<li><p><strong>Multi-head Attention</strong>: The combined vectors are passed through the multi-head attention mechanism, which captures the relationships between words in the input sentence.</p></li>
<li><p><strong>Normalization and Feed-Forward Neural Network</strong>: The output from the attention mechanism is normalized and passed through a feed-forward neural network.</p></li>
<li><p><strong>Stacking Encoder Layers</strong>: The encoding process can be repeated multiple times, with each layer refining the sentence representation further.</p></li>
</ol>
</section>
<section id="decoder">
<h3>Decoder<a class="headerlink" href="#decoder" title="Permalink to this heading">#</a></h3>
<p>The decoder’s responsibility is to transform the encoded vector representation into a translated sentence in the target language, in this case, Italian. The steps involved are:</p>
<ol class="arabic simple">
<li><p><strong>Input Preparation</strong>: The decoder takes as input the previously translated words in Italian and the output from the encoder. Initially, the input consists of the first two translated words.</p></li>
<li><p><strong>Positional Encoding and Multi-head Attention</strong>: The decoder applies positional encoding and multi-head attention mechanisms to the translated words in Italian.</p></li>
<li><p><strong>Concatenation and Recalculation</strong>: The output from the attention mechanism is concatenated with the output from the encoder, and attention is recalculated on the concatenated vector.</p></li>
<li><p><strong>Normalization and Feed-Forward Neural Network</strong>: The concatenated vector is normalized and passed through a feed-forward neural network.</p></li>
<li><p><strong>Predicting Next Word</strong>: The output from the neural network is a vector of potential candidates for the next word in the translated Italian sentence.</p></li>
<li><p><strong>Iterative Decoding</strong>: In the next iteration, the decoder takes as input the first three translated words in Italian along with the encoder’s output. This process is repeated until the entire translated sentence is generated.</p></li>
</ol>
<p>In summary, the Transformer architecture consists of an encoder that converts the input sentence into a vector representation and a decoder that translates the encoded vector into the target language. The model leverages multi-head attention, positional encoding, and feed-forward neural networks to capture and process the relationships between words effectively, resulting in improved performance in various natural language processing tasks.</p>
<figure class="align-default" id="fig-transformer-best">
<a class="reference internal image-reference" href="../../../_images/transformer-best.jpeg"><img alt="../../../_images/transformer-best.jpeg" src="../../../_images/transformer-best.jpeg" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 105 </span><span class="caption-text">Problems with the Transformer architecture</span><a class="headerlink" href="#fig-transformer-best" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="problems-with-the-transformer-architecture">
<h2>Problems with the Transformer architecture<a class="headerlink" href="#problems-with-the-transformer-architecture" title="Permalink to this heading">#</a></h2>
<p>The Transformer architecture has revolutionized the field of natural language processing, offering significant improvements in various tasks. However, it also comes with some inherent limitations:</p>
<figure class="align-default" id="fig-transformer-problem">
<a class="reference internal image-reference" href="../../../_images/transformer-problem.gif"><img alt="../../../_images/transformer-problem.gif" src="../../../_images/transformer-problem.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 106 </span><span class="caption-text">Problems with the Transformer architecture</span><a class="headerlink" href="#fig-transformer-problem" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ol class="arabic simple">
<li><p><strong>Computational Complexity</strong>: One of the main strengths of the Transformer architecture, its attention mechanism, also contributes to its primary weakness: high computational complexity. The attention mechanism requires a significant amount of computation, particularly when dealing with long input sequences.</p></li>
<li><p><strong>Quadratic Scaling</strong>: In order to compute the attention for each word with respect to all other words, the model needs to perform <span class="math notranslate nohighlight">\(N^2\)</span> calculations, where N is the number of words in the input. This can be visualized as filling a matrix with attention values for each word pair, leading to quadratic scaling in terms of computational complexity.</p></li>
<li><p><strong>Masked Attention</strong>: To mitigate the computational complexity to some extent, particularly in the decoder, masked attention is sometimes used. Masked attention is a mechanism that calculates the attention of a word only with respect to previous words, excluding the following words in the sequence. While this approach reduces complexity, it still doesn’t fully resolve the issue, especially for very long sequences.</p></li>
</ol>
<figure class="align-default" id="fig-transformer-n2">
<a class="reference internal image-reference" href="../../../_images/transformer-N2.gif"><img alt="../../../_images/transformer-N2.gif" src="../../../_images/transformer-N2.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 107 </span><span class="caption-text">Attention matrix</span><a class="headerlink" href="#fig-transformer-n2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In summary, while the Transformer architecture has significantly advanced the field of natural language processing, it comes with some inherent challenges, particularly in terms of computational complexity and quadratic scaling. Efforts to address these issues are ongoing, with researchers continually exploring new techniques and optimizations to further improve the efficiency and performance of Transformer-based models.</p>
</section>
<section id="attention-is-not-all-you-need">
<h2>Attention Is Not All You Need<a class="headerlink" href="#attention-is-not-all-you-need" title="Permalink to this heading">#</a></h2>
<p>In March 2021, Google researchers published a paper titled “Attention Is Not All You Need” <span id="id4">[<a class="reference internal" href="../../../about/index.html#id9" title="Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning, 2793–2803. PMLR, 2021.">Dong <em>et al.</em>, 2021</a>]</span>, challenging the prevailing notion that the self-attention mechanism was the sole key to the Transformer’s success.</p>
<p>In the paper, the researchers conducted experiments specifically analyzing the behavior of the self-attention mechanism in isolation, without incorporating any of the other components that make up the Transformer architecture. They sought to understand the self-attention mechanism’s limitations and how it interacts with the other components of the Transformer model.</p>
<p>Their findings revealed that, when used in isolation, the self-attention mechanism converges to a rank 1 matrix with a doubly exponential rate. In other words, the self-attention mechanism alone is not sufficient to achieve meaningful results, and it is practically ineffective without the other components of the Transformer architecture.</p>
<p>The research underscores the importance of considering the interplay of various components within the Transformer model, rather than solely focusing on the attention mechanism. This insight can help guide further development and improvement of Transformer-based models, as researchers continue to explore the optimal balance and combination of different components for various natural language processing tasks.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the context of the paper “Attention Is Not All You Need,” “convergence to a rank 1 matrix” refers to the behavior of the self-attention mechanism when used in isolation. A rank 1 matrix is a matrix whose columns (or rows) can be expressed as a linear combination of a single vector. In other words, all the columns (or rows) of the matrix are essentially scaled versions of the same vector.</p>
<p>Converging to a rank 1 matrix implies that the self-attention mechanism, when used alone, produces an output that has very limited expressiveness and diversity. Since the information in a rank 1 matrix can be represented by just one vector, it indicates that the self-attention mechanism isn’t capable of capturing complex relationships and patterns in the data by itself.</p>
<p>The researchers found that combining the self-attention mechanism with other components of the Transformer architecture, like skip connections and MLPs, is essential to achieving the powerful performance that the Transformer model is known for. The combination of these components allows the Transformer to overcome the limitations of the self-attention mechanism and effectively capture complex patterns and dependencies in the input data.</p>
</div>
<section id="so-why-are-transformers-so-powerful">
<h3>So why are transformers so powerful?<a class="headerlink" href="#so-why-are-transformers-so-powerful" title="Permalink to this heading">#</a></h3>
<p>The success of the Transformer model is not solely due to the self-attention mechanism, as previously thought. The Google researchers discovered that the power of the Transformer model comes from a combination of components working together:</p>
<ol class="arabic simple">
<li><p><strong>Self-attention mechanism</strong>: This component allows the model to focus on different parts of the input, depending on their relevance to the task at hand.</p></li>
<li><p><strong>Skip connections</strong>: Also known as residual connections, skip connections allow the model to maintain information from earlier layers without losing it in the process. This helps the model diversify the distribution of paths and avoid convergence to a rank 1 matrix, which would make it ineffective.</p></li>
<li><p><strong>MLP (Multilayer Perceptron)</strong>: The MLP component adds non-linearity to the model due to the activation functions used in its layers. This non-linearity helps increase the rank of the matrix, contributing to the model’s expressive power.</p></li>
</ol>
<figure class="align-default" id="fig-transformer-tug-of-war">
<a class="reference internal image-reference" href="../../../_images/transformer-tug-of-war.jpeg"><img alt="../../../_images/transformer-tug-of-war.jpeg" src="../../../_images/transformer-tug-of-war.jpeg" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 108 </span><span class="caption-text">Tug of war between the self-attention mechanism and skip connections and MLP</span><a class="headerlink" href="#fig-transformer-tug-of-war" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In summary, the true strength of the Transformer model comes from the combined effect of the self-attention mechanism, skip connections, and MLP. These components work together in a delicate balance to make the Transformer architecture highly effective at processing and understanding complex input data, ultimately leading to its impressive performance in various natural language processing tasks.</p>
</section>
</section>
<section id="vision-transformers">
<h2>Vision Transformers<a class="headerlink" href="#vision-transformers" title="Permalink to this heading">#</a></h2>
<p>The success of Transformers in the field of Natural Language Processing raises the question of whether they can be equally effective in the realm of image processing. In order to explore this possibility, let’s consider an example: a picture of a dog standing in front of a wall.</p>
<p>When we look at the picture, our attention is naturally drawn to the dog, which is the main subject of the image, while the wall serves as the background. This ability to focus on the dominant subject in the scene is a crucial aspect of understanding and interpreting the image’s content.</p>
<p>This concept of focusing on the most important parts of the input is similar to what the self-attention mechanism does when applied to sentences in Natural Language Processing tasks. The self-attention mechanism helps the model pay attention to the most relevant words or phrases, depending on the context and the task.</p>
<p>Applying the self-attention mechanism to images involves the same principle. By focusing on the most important features or regions in the image, the model can effectively extract valuable information, recognize patterns, and understand the content of the image. This approach has led to the development of Vision Transformers, which leverage the power of the Transformer architecture to achieve impressive performance in various computer vision tasks.</p>
<figure class="align-default" id="fig-vit">
<a class="reference internal image-reference" href="../../../_images/vit.jpeg"><img alt="../../../_images/vit.jpeg" src="../../../_images/vit.jpeg" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 109 </span><span class="caption-text">Vision Transformer</span><a class="headerlink" href="#fig-vit" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="how-to-input-images-into-a-transformer">
<h3>How to input images into a transformer?<a class="headerlink" href="#how-to-input-images-into-a-transformer" title="Permalink to this heading">#</a></h3>
<figure class="align-default" id="fig-vit-pixels">
<a class="reference internal image-reference" href="../../../_images/vit-pixels.gif"><img alt="../../../_images/vit-pixels.gif" src="../../../_images/vit-pixels.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 110 </span><span class="caption-text">Vision Transformer with pixels</span><a class="headerlink" href="#fig-vit-pixels" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Feeding images into a Transformer requires a method that efficiently handles the high computational complexity associated with image data. One might initially consider using all the image pixels as input to the Transformer, but this approach would not be efficient due to the significant computational resources required.</p>
<p>The attention calculation in Transformers has a complexity of <span class="math notranslate nohighlight">\(O(N^2)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> represents the number of pixels. Consequently, the attention calculation would require <span class="math notranslate nohighlight">\(O(N^4)\)</span> operations, which is highly resource-intensive and not a practical solution for processing images, especially those with high resolutions.</p>
<p>A more feasible approach to input images into a Transformer is to divide the image into smaller patches and convert each patch into a vector. This method greatly reduces the computational complexity while still preserving the necessary information for the model to process.</p>
<p>By breaking the image down into smaller patches, the Transformer can focus on individual regions, extracting valuable features from each patch. Each patch is then converted into a vector using a linear projection, allowing the Transformer to process the image data more efficiently without compromising its ability to perform various computer vision tasks effectively.</p>
<figure class="align-default" id="fig-vit-projection">
<a class="reference internal image-reference" href="../../../_images/vit-projection.gif"><img alt="../../../_images/vit-projection.gif" src="../../../_images/vit-projection.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 111 </span><span class="caption-text">Linear projection of the patches</span><a class="headerlink" href="#fig-vit-projection" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="vision-transformer-architecture">
<h3>Vision Transformer Architecture<a class="headerlink" href="#vision-transformer-architecture" title="Permalink to this heading">#</a></h3>
<p>The Vision Transformer (ViT) architecture is designed to process images effectively by adapting the standard Transformer architecture for computer vision tasks. Here’s a breakdown of the key steps involved in the Vision Transformer architecture:</p>
<ol class="arabic simple">
<li><p>The image is divided into patches, and each patch is converted into a vector using a linear projection. These vectors represent the individual regions of the image, capturing essential information from each patch.</p></li>
<li><p>Positional encoding vectors are added to the patch vectors. This step is crucial for providing the Transformer with information about the spatial positions of the patches within the image, ensuring that the model can understand the relationships between different regions.</p></li>
<li><p>The combined patch and positional encoding vectors are then passed through a standard Transformer architecture, which is composed of layers of self-attention and feed-forward neural networks. This allows the model to process the image data effectively, learning to recognize patterns and features within the image.</p></li>
<li><p>The output of the Transformer is a vector that represents the entire image, encapsulating the critical information and relationships between the different regions. This vector is then passed to a classifier, which can be a linear layer, a convolutional neural network, or another appropriate model, to produce the final result. This could be a classification label, an object detection output, or any other desired output for a specific computer vision task.</p></li>
</ol>
<figure class="align-default" id="fig-vit-architecture">
<a class="reference internal image-reference" href="../../../_images/vit-architecture.gif"><img alt="../../../_images/vit-architecture.gif" src="../../../_images/vit-architecture.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 112 </span><span class="caption-text">Vision Transformer Architecture</span><a class="headerlink" href="#fig-vit-architecture" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The Vision Transformer architecture leverages the power of Transformers to tackle various computer vision tasks efficiently and effectively, achieving impressive results in image classification, object detection, and more.</p>
</section>
</section>
<section id="transformer-in-transformer">
<h2>Transformer in Transformer<a class="headerlink" href="#transformer-in-transformer" title="Permalink to this heading">#</a></h2>
<figure class="align-default" id="fig-tnt">
<a class="reference internal image-reference" href="../../../_images/tnt.gif"><img alt="../../../_images/tnt.gif" src="../../../_images/tnt.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 113 </span><span class="caption-text">Transformer in Transformer</span><a class="headerlink" href="#fig-tnt" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The Transformer in Transformer (TnT) architecture <span id="id5">[<a class="reference internal" href="../../../about/index.html#id10" title="Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. Advances in Neural Information Processing Systems, 34:15908–15919, 2021.">Han <em>et al.</em>, 2021</a>]</span> aims to address the issue of losing spatial information within image patches when transforming them into vectors for the standard Vision Transformer. The authors propose an approach that preserves the arrangement of pixels within patches, leading to improved performance on various computer vision tasks. Here’s a step-by-step explanation of the TnT architecture:</p>
<ol class="arabic simple">
<li><p>For each individual patch (pxp) of the input image, which consists of 3 RGB channels, the TnT architecture transforms it into a c-channel tensor. This tensor retains the spatial information and arrangement of pixels within the patch.</p></li>
<li><p>The c-channel tensor is then divided into smaller <span class="math notranslate nohighlight">\(p^\prime\)</span> parts, where <span class="math notranslate nohighlight">\(p^\prime\)</span> is less than <span class="math notranslate nohighlight">\(p\)</span>. For example, if <span class="math notranslate nohighlight">\(p^\prime\)</span> is 4, the tensor is divided into four smaller sections.</p></li>
<li><p>By dividing the tensor into smaller parts, <span class="math notranslate nohighlight">\(p^\prime\)</span> vectors in <span class="math notranslate nohighlight">\(c\)</span> dimensions are obtained. These vectors now carry information about the arrangement of pixels within the patch, ensuring that important spatial details are preserved.</p></li>
<li><p>The <span class="math notranslate nohighlight">\(p^\prime\)</span> vectors are concatenated and linearly projected to match the size of the vector obtained from the linear projection of the original patch. This step allows the model to effectively combine the preserved spatial information with the original patch representation.</p></li>
<li><p>The concatenated and linearly projected vectors are then combined with the original patch vector, forming a more informative representation of the image patch for the Transformer to process.</p></li>
</ol>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="../../../_images/tnt-architecture.gif"><img alt="../../../_images/tnt-architecture.gif" src="../../../_images/tnt-architecture.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 114 </span><span class="caption-text">Transformer in Transformer Architecture</span><a class="headerlink" href="#id6" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The Transformer in Transformer architecture enhances the standard Vision Transformer by incorporating crucial spatial information from image patches. As a result, TnT delivers improved performance on a variety of computer vision tasks, such as image classification and object detection.</p>
</section>
<section id="timesformers">
<h2>TimeSformers<a class="headerlink" href="#timesformers" title="Permalink to this heading">#</a></h2>
<figure class="align-default" id="fig-timesformer-architecture">
<a class="reference internal image-reference" href="../../../_images/timesformer-architecture.gif"><img alt="../../../_images/timesformer-architecture.gif" src="../../../_images/timesformer-architecture.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 115 </span><span class="caption-text">TimeSformer Architecture</span><a class="headerlink" href="#fig-timesformer-architecture" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>n 2021, Facebook researchers explored the possibility of applying the Vision Transformer architecture to videos, leading to the development of TimeSformers. The core idea is to divide a video into individual frames and process them using a similar procedure as for images. However, there is an essential distinction between TimeSformers and Vision Transformers: TimeSformers must take into account the temporal dimension of the video in addition to the spatial dimension.</p>
<p>To address this challenge, the authors proposed several new attention mechanisms. These mechanisms range from focusing exclusively on space (used primarily as a reference point) to computing attention axially, scattered, or jointly between space and time.</p>
<p>The most effective method, achieving the best results, is called Divided Space-Time Attention. This approach involves selecting a frame at a specific time (t) and one of its patches as a query. The model then computes the spatial attention across the entire frame, followed by calculating the temporal attention within the same patch of the query but in the previous and next frames.</p>
<figure class="align-default" id="fig-timesformer-attentions">
<a class="reference internal image-reference" href="../../../_images/timesformer-attentions.jpeg"><img alt="../../../_images/timesformer-attentions.jpeg" src="../../../_images/timesformer-attentions.jpeg" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 116 </span><span class="caption-text">TimeSformer Attention Mechanisms</span><a class="headerlink" href="#fig-timesformer-attentions" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The Divided Space-Time Attention approach works exceptionally well because it learns more distinct features compared to other methods. This capability enables the model to better understand and classify videos from various categories. By incorporating both spatial and temporal dimensions, TimeSformers can effectively process and analyze video data, leading to improved performance on video-related tasks.</p>
<p>In the following visualization, each video is represented by a point in space, and its color indicates the category it belongs to.</p>
<figure class="align-default" id="fig-timesformer-divide">
<a class="reference internal image-reference" href="../../../_images/timesformer-divide.jpeg"><img alt="../../../_images/timesformer-divide.jpeg" src="../../../_images/timesformer-divide.jpeg" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 117 </span><span class="caption-text">TimeSformer Divide Space-Time Attention</span><a class="headerlink" href="#fig-timesformer-divide" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The authors discovered that increasing the resolution of the video leads to better model accuracy, up to a certain point. Similarly, they found that as the number of frames in a video increases, the model’s accuracy also improves. However, they were unable to conduct tests with a higher number of frames than what is shown in the graph, so it remains unclear whether the accuracy could continue to improve with even more frames. The upper limit of this potential improvement is still unknown.</p>
<p>In summary, TimeSformers benefit from higher resolution and a greater number of frames in videos, which contribute to improved accuracy in understanding and classifying video content. However, further research is needed to determine the optimal balance between resolution and frame count for the best possible performance.</p>
<figure class="align-default" id="fig-timesformer-resolution">
<a class="reference internal image-reference" href="../../../_images/timesformer-resolution.jpeg"><img alt="../../../_images/timesformer-resolution.jpeg" src="../../../_images/timesformer-resolution.jpeg" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 118 </span><span class="caption-text">TimeSformer Resolution</span><a class="headerlink" href="#fig-timesformer-resolution" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Similar to Vision Transformers, the authors of TimeSformers explored the impact of using larger training datasets on the model’s accuracy. They found that as the number of training videos considered increases, the accuracy of the model also improves. This suggests that TimeSformers, like Vision Transformers, can benefit from larger datasets for better performance.</p>
<figure class="align-default" id="fig-timesformer-dataset">
<a class="reference internal image-reference" href="../../../_images/timesformer-dataset.jpeg"><img alt="../../../_images/timesformer-dataset.jpeg" src="../../../_images/timesformer-dataset.jpeg" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 119 </span><span class="caption-text">TimeSformer Dataset</span><a class="headerlink" href="#fig-timesformer-dataset" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In essence, having more training data allows the model to learn a broader range of features and patterns, which in turn helps it to generalize better when encountering new video content. This finding highlights the importance of using large and diverse training datasets to maximize the potential and accuracy of TimeSformers.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<p>Transformers have recently made their way into the realm of computer vision, showing great potential to either replace traditional convolutional networks or, at the very least, claim a significant role within this domain.</p>
<p>Originally, transformers gained prominence in the field of Natural Language Processing, where they demonstrated remarkable performance across a variety of tasks. This powerful architecture has since been adapted and applied to computer vision, yielding promising results that suggest their effectiveness will only continue to grow in the future.</p>
<p>The success of transformers in both NLP and computer vision showcases their versatility and adaptability, indicating that they may play an increasingly important role in the advancement of machine learning techniques across multiple domains.</p>
</section>
<section id="multimodal-machine-learning">
<h2>Multimodal Machine Learning<a class="headerlink" href="#multimodal-machine-learning" title="Permalink to this heading">#</a></h2>
<p>Multimodal machine learning is an emerging field that aims to develop models capable of processing and combining different types of data, much like how humans are able to draw inferences from various sources of information.</p>
<p>People can understand the world around them by integrating information gathered through their senses, such as sight, smell, hearing, and touch. This ability to process and synthesize multiple sources of data allows us to make sense of our surroundings.</p>
<p>The challenge in multimodal machine learning lies in designing models that can treat various inputs consistently and effectively without losing vital information. Transformers, with their proven capability to work with different data types, present a promising solution for this task.</p>
<p>As a versatile architecture, transformers can process and combine diverse data sources within a single model, paving the way for advancements in multimodal machine learning and the development of more sophisticated AI systems that can better understand and interact with the world around them.</p>
</section>
<section id="vatt-transformers-for-multimodal-self-supervised-learning">
<h2>VATT: Transformers for Multimodal Self-Supervised Learning<a class="headerlink" href="#vatt-transformers-for-multimodal-self-supervised-learning" title="Permalink to this heading">#</a></h2>
<p>VATT (Visual-Audio Text Transformer) is a significant development in the field of Multimodal Machine Learning <span id="id7">[<a class="reference internal" href="../../../about/index.html#id11" title="Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: transformers for multimodal self-supervised learning from raw video, audio and text. Advances in Neural Information Processing Systems, 34:24206–24221, 2021.">Akbari <em>et al.</em>, 2021</a>]</span>. This innovative architecture utilizes a single Transformer Encoder to process different types of input data simultaneously, transforming each into a sequence of tokens.</p>
<figure class="align-default" id="fig-vatt">
<a class="reference internal image-reference" href="../../../_images/vatt.gif"><img alt="../../../_images/vatt.gif" src="../../../_images/vatt.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 120 </span><span class="caption-text">VATT Architecture</span><a class="headerlink" href="#fig-vatt" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>During the forward pass, the transformer takes in these sequences and generates three distinct sets of features, one for each type of input data. These features are then fed into a contrastive estimation block, which calculates a single loss and performs the backward pass. As a result, the model learns to minimize the loss by better managing and integrating information from all three data sources.</p>
<p>VATT embodies the primary goal of Multimodal Machine Learning, which is to develop a unified model capable of handling and processing various data types effectively. This breakthrough has the potential to significantly advance the field and inspire further innovations in the development of multimodal AI systems.</p>
</section>
<section id="gato-a-generalist-agent">
<h2>GATO: A Generalist Agent<a class="headerlink" href="#gato-a-generalist-agent" title="Permalink to this heading">#</a></h2>
<p>he concept of creating a neural network that can handle various input types and perform multiple tasks is the driving force behind GATO (Generalist Agent) <span id="id8">[<a class="reference internal" href="../../../about/index.html#id12" title="Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, and others. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.">Reed <em>et al.</em>, 2022</a>]</span>. As one of the most remarkable accomplishments in the field, GATO demonstrates the potential for multi-modal, multi-task, and multi-embodiment learning in a single model.</p>
<p>GATO operates using a single Transformer Encoder, which processes a sequence of tokens representing different data types. The key to GATO’s success lies in its ability to unify these inputs and leverage the Transformer architecture to learn how to effectively combine diverse data types. As a result, GATO can perform various tasks, achieving an unparalleled degree of generalization.</p>
<p>In summary, GATO represents a significant advancement in the development of neural networks capable of handling multiple input types and tasks, showcasing the potential for even more sophisticated AI systems in the future.</p>
</section>
<section id="next">
<h2>Next<a class="headerlink" href="#next" title="Permalink to this heading">#</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT: Bidirectional Encoder Representations from Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="bertviz.html">BERT: Visualizing Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
</ul>
</div>
<p><img alt="" src="lectures/nlp_deep/transformers/figs/gato.gif" /></p>
<p><img alt="" src="lectures/nlp_deep/transformers/figs/gato-examples.gif" /></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/nlp_deep/transformers"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../llms/plms.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Pretrained Language Models</p>
      </div>
    </a>
    <a class="right-next"
       href="bert.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">BERT: Bidirectional Encoder Representations from Transformers</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-transformers">Why do we need transformers?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-is-all-you-need">Attention is all you need?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-implement-this-attention-mechanism">How do we implement this attention mechanism?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-head attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tranformer-architecture">Tranformer Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-the-transformer-architecture">Problems with the Transformer architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-is-not-all-you-need">Attention Is Not All You Need</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#so-why-are-transformers-so-powerful">So why are transformers so powerful?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vision-transformers">Vision Transformers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-input-images-into-a-transformer">How to input images into a transformer?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vision-transformer-architecture">Vision Transformer Architecture</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-in-transformer">Transformer in Transformer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#timesformers">TimeSformers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-machine-learning">Multimodal Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vatt-transformers-for-multimodal-self-supervised-learning">VATT: Transformers for Multimodal Self-Supervised Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gato-a-generalist-agent">GATO: A Generalist Agent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next">Next</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://entelecheia.me" target="_blank">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>