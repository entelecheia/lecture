

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Datasets for NLP &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../../_static/tabs.js"></script>
    <script src="../../../_static/js/hoverxref.js"></script>
    <script src="../../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/chatgpt.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/nlp_deep/datasets/index';</script>
    <link rel="canonical" href="https://lecture.entelecheia.ai/lectures/nlp_deep/datasets/index.html" />
    <link rel="shortcut icon" href="../../../_static/favicon-v2-circle.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Tokenization" href="../tokenization/index.html" />
    <link rel="prev" title="ByT5: Towards a token-free future with pre-trained byte-to-byte models" href="../transformers/byt5.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content"><p>
  <strong>Announcement:</strong>
  You can install the accompanying AI tutor <a href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd" target="_blank">here</a>.
  <a class="reference external" href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd" target="_blank"><img alt="chrome-web-store-image" src="https://img.shields.io/chrome-web-store/v/lfgfgbomindbccgidgalhhndggddpagd" /></a>
</p>
</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_intro/index.html">Introduction to NLP</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/research.html">Research Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/language_models.html">Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/datasets/index.html">Datasets for NLP</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/topic/index.html">Topic Modeling</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/coherence-practice.html">Topic Coherence in Practice</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/tomotopy.html">Topic Modeling Tools - Tomotopy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/sentiments.html">Sentiment Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/word_segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/vectorization.html">Vector Semantics and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/word_embeddings.html">Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/lab2-corpus-eda.html">Lab 2: EDA on Corpora</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Deep Learning for NLP</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../intro.html">Introduction</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../llms/index.html">Large Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../llms/zeroshot.html">Zero Shot and Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llms/decoding.html">Decoding and Search Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llms/plms.html">Pretrained Language Models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../transformers/index.html">Transformers</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../transformers/bert.html">BERT: Bidirectional Encoder Representations from Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../transformers/bertviz.html">BERT: Visualizing Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../transformers/t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../transformers/byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
</ul>
</li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Datasets for NLP</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tokenization/index.html">Tokenization</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../tokenization/sentencepiece.html">SentencePiece Tokenizer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../detectGPT.html">How to Spot Machine-Written Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lab3-train-tokenizers.html">Lab 3: Training Tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lab4-pretraining-lms.html">Lab 4: Pretraining Language Models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_advances/index.html">Advances in AI and NLP</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/thesis.html">Writing a Thesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/detectGPT.html">DetectGPT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/gpt4.html">GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/camelids.html">Meet the Camelids: A Family of LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/sam/index.html">Segment Everything</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../aiart/index.html">AI Art (Generative AI)</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/aiart.html">Art and Music in Light of AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/imagen.html">Imagen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/textual-inversion.html">Textual Inversion (Dreambooth)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/whisper.html">Automatic Speech Recognition (Whisper)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/text2music.html">Text to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/image2music.html">Image to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/robot_drawings.html">Robot Drawing Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/project-themes.html">Project Themes - A Brave New World</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/devops/index.html">DevOps</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/gitops.html">GitOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/devsecops.html">DevSecOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/llmops.html">LLMOps</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/dotfiles/index.html">Dotfiles</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai">Dotfiles in Practice</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/dotfiles/dotdrop.html">Dotdrop: A Powerful Tool for Managing Dotfiles</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/security/index.html">Security Management</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/auth-enc-sign.html">Authentication, Encryption, and Signing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/age-gpg-ssh.html">SSH, GPG, and AGE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/pass.html">Unix Password Managers</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/github/index.html">Github’s Fork &amp; Pull Workflow</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/template.html">Project Templating Tools</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-python.entelecheia.ai/">Hyperfast Python Template</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-template.entelecheia.ai/">Hyperfast Template</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/containerization/index.html">Containerization</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/docker.html">Docker</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/containerd.html"><code class="docutils literal notranslate"><span class="pre">containerd</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/simple-pipeline/index.html">Simple MLOps Pipeline</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../ds/index.html">Data Science for Economics and Finance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../about/lecture-bot.html">LectureBot for ἐντελέχεια.άι</a></li>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference external" href="https://course.entelecheia.ai">course.entelecheia.ai</a></li>
<li class="toctree-l1"><a class="reference external" href="https://research.entelecheia.ai">research.entelecheia.ai</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about/index.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/lectures/nlp_deep/datasets/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Datasets for NLP</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-state-of-llms-today">The State of LLMs Today</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#high-quality-vs-low-quality-data">High-Quality vs. Low-Quality Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-collecting-data">Challenges in Collecting Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#three-data-must-haves-for-nlp">Three Data Must-Haves for NLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acquiring-high-quality-datasets">Acquiring High-Quality Datasets</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#public-datasets">Public datasets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specialized-domains">Specialized domains</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#proprietary-datasets">Proprietary datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multilingual-datasets">Multilingual datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-your-own-dataset-through-web-crawling">Building your own dataset through web crawling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="datasets-for-nlp">
<h1>Datasets for NLP<a class="headerlink" href="#datasets-for-nlp" title="Permalink to this heading">#</a></h1>
<a class="bg-primary mb-1 reference internal image-reference" href="../../../_images/ekorpkit-corpus.png"><img alt="ekorpkit-corpus" class="bg-primary mb-1 align-center" src="../../../_images/ekorpkit-corpus.png" style="width: 70%;" /></a>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language. NLP involves a wide range of tasks, such as sentiment analysis, text summarization, machine translation, and question-answering systems. Central to the success of NLP applications are large language models (LLMs), which are machine learning models trained on vast amounts of text data.</p>
<p>LLMs are based on deep learning neural networks, such as the Transformer architecture, and are designed to learn the intricate patterns and structures of natural language. These models have significantly advanced the field of NLP, enabling applications like conversational AI, automated content generation, and information extraction. However, the quality of these models heavily depends on the datasets used for training.</p>
<p>High-quality datasets are essential for training LLMs, as they help the models to learn and generalize better, thus improving their performance on various NLP tasks.</p>
</section>
<section id="the-state-of-llms-today">
<h2>The State of LLMs Today<a class="headerlink" href="#the-state-of-llms-today" title="Permalink to this heading">#</a></h2>
<p>LLMs have seen rapid advancements in recent years, driven by the increasing interest in natural language processing and AI applications. These models have grown in scale and complexity, with the number of parameters (i.e., the model’s “knowledge”) increasing exponentially to improve their understanding of language and context. The state-of-the-art LLMs developed by leading technology companies, like OpenAI, Google, Nvidia, and Microsoft, have contributed significantly to the progress of NLP.</p>
<p>OpenAI’s GPT-3, released in June 2020, was a milestone in LLM development, featuring 175 billion parameters. It showcased an impressive understanding of context and the ability to generate human-like text, demonstrating the potential of LLMs in various applications. In response, other tech giants have invested in developing their own LLMs. For instance, Google’s Bard, which is still in the beta testing phase, aims to compete with GPT-3 in terms of versatility and accuracy.</p>
<p>More recently, Nvidia and Microsoft announced the Megatron-Turing Natural Language Generation model (MT-NLG), a massive monolithic transformer language model with 530 billion parameters. Google followed suit by announcing the development of PaLM, a 540 billion-parameter model. These advancements highlight the continuous growth and competition in the LLM landscape.</p>
<p>The core component driving the success of these LLMs is the training data used to fine-tune their understanding of language, context, and semantics. Training data is a collection of text from various sources, such as websites, books, scientific papers, and social media, that helps the LLM learn the intricacies of language. However, the quality of the training data has a significant impact on the performance of the LLM, making the choice of datasets a critical aspect of LLM development.</p>
<p>As the race to develop more sophisticated LLMs intensifies, it becomes increasingly important to understand the challenges associated with collecting high-quality datasets and the various sources from which these datasets can be acquired. By addressing these challenges, researchers and companies can enhance the capabilities of LLMs and unlock new potential applications in natural language processing.</p>
</section>
<section id="high-quality-vs-low-quality-data">
<h2>High-Quality vs. Low-Quality Data<a class="headerlink" href="#high-quality-vs-low-quality-data" title="Permalink to this heading">#</a></h2>
<p>The quality of training data used to develop LLMs is crucial for their performance, generalization capabilities, and applicability across a wide range of NLP tasks. In this context, it is essential to differentiate between high-quality and low-quality data, as they have different implications for the resulting LLM.</p>
<p><strong>High-quality data</strong> typically originates from reliable, vetted sources, such as peer-reviewed articles, reputable news websites, and curated knowledge repositories like encyclopedias. This type of data is characterized by its accuracy, consistency, and relevance to the target domain. High-quality data helps LLMs learn meaningful patterns, comprehend complex language structures, and improve their understanding of context. As a result, LLMs trained on high-quality data tend to be more versatile and accurate in their performance.</p>
<p><strong>Low-quality data</strong> , on the other hand, is often sourced from unfiltered, user-generated content, such as social media posts, web forums, and informal blogs. This type of data may contain inaccuracies, inconsistencies, and noisy information that can negatively impact the LLM’s learning process. Training LLMs on low-quality datasets can lead to several issues, including:</p>
<ol class="arabic simple">
<li><p><strong>Data bias</strong> : If the training data contains unbalanced or biased information, the LLM may learn to favor certain inputs or produce biased outputs. This can result in the model performing poorly on particular inputs or generating outputs that perpetuate stereotypes and misinformation.</p></li>
<li><p><strong>Spurious correlations</strong> : Low-quality datasets may contain language patterns that are specific to a certain context or source, leading the LLM to learn incorrect shortcuts and associations. This can cause the model to make mistakes when faced with real-world scenarios, as it may rely on these spurious correlations instead of understanding the true underlying structure of the language.</p></li>
<li><p><strong>Mislabeled examples</strong> : Noisy and mislabeled data can introduce confusion into the LLM’s learning process, ultimately lowering the quality of its outputs. The model may struggle to discern between relevant and irrelevant information, making it difficult to generate accurate and coherent responses.</p></li>
</ol>
<p>Given these potential issues, it is evident that there is a growing demand for high-quality, high-volume datasets in LLM development. Ensuring that LLMs are trained on high-quality data not only improves their performance but also mitigates the risks associated with data bias and spurious correlations, resulting in more reliable and trustworthy AI applications.</p>
</section>
<section id="challenges-in-collecting-data">
<h2>Challenges in Collecting Data<a class="headerlink" href="#challenges-in-collecting-data" title="Permalink to this heading">#</a></h2>
<p>Collecting high-quality data for training LLMs can be a complex and resource-intensive process. There are several challenges associated with acquiring, preprocessing, and filtering data to ensure that it meets the requirements for training state-of-the-art LLMs. These challenges include:</p>
<ol class="arabic simple">
<li><p><strong>Data scarcity</strong>: Finding high-quality data sources is a daunting task, as much of the publicly available data consists of unfiltered or lightly filtered content. This can make it difficult for researchers to obtain large volumes of reliable, high-quality data for training LLMs. Furthermore, the rapid growth of LLMs exacerbates this issue, as the demand for high-quality data outpaces the availability of such data sources.</p></li>
<li><p><strong>Data diversity</strong>: Ensuring that the training data is diverse and representative of various domains, genres, and languages is essential for developing versatile LLMs. However, acquiring diverse datasets can be challenging, as it may require access to proprietary or specialized data sources that may not be readily available.</p></li>
<li><p><strong>Data preprocessing and cleaning</strong>: Raw data often contains noise, inconsistencies, and irrelevant information that must be removed or corrected before the data can be used for training LLMs. This preprocessing and cleaning process can be time-consuming and expensive, as it may require manual intervention or the development of specialized algorithms to filter and structure the data.</p></li>
<li><p><strong>Data labeling and annotation</strong>: Supervised learning approaches, which require labeled data, can be particularly challenging when working with large datasets. Manual labeling can be costly and time-consuming, while automated labeling methods may introduce errors or inaccuracies. This challenge is further compounded when dealing with multilingual or specialized domain data, which may require expertise in specific languages or subject areas.</p></li>
<li><p><strong>Legal and ethical considerations</strong>: Collecting data for LLM training may involve accessing copyrighted or sensitive information, which raises legal and ethical concerns. Ensuring compliance with data protection regulations, such as GDPR, and respecting user privacy can be challenging, as it may require additional measures to anonymize or obtain consent for the use of personal data.</p></li>
</ol>
<p>To overcome these challenges, researchers and companies developing LLMs must invest in innovative methods for data collection, preprocessing, and filtering. This may involve leveraging public datasets, partnering with organizations that possess proprietary data, or developing in-house tools and algorithms for data acquisition and processing. By addressing these challenges, LLM developers can ensure the quality and reliability of their models, paving the way for more accurate and useful NLP applications.</p>
</section>
<section id="three-data-must-haves-for-nlp">
<h2>Three Data Must-Haves for NLP<a class="headerlink" href="#three-data-must-haves-for-nlp" title="Permalink to this heading">#</a></h2>
<p>To train high-performing LLMs and improve the overall effectiveness of NLP applications, companies and researchers should prioritize the following three key attributes when selecting datasets for training:</p>
<ol class="arabic simple">
<li><p><strong>Rich metadata</strong>: When choosing a dataset, it is essential to consider the metadata that accompanies the text. Rich metadata, which includes information such as authorship, publication date, source, content length, and topic, can significantly enhance the LLM’s understanding of context and improve its performance on various NLP tasks. High-quality dataset providers should offer numerous metadata parameters, including descriptive, structural, and administrative information. Additionally, the datasets should be cleaned and structured, with all irrelevant HTML elements removed.</p></li>
<li><p><strong>Filtering options</strong>: Datasets can be more useful and efficient when they offer extensive filtering options. By filtering data based on specific criteria such as language, location, keywords, sentiment, and more, you can improve the relevance and quality of the dataset. This customization allows you to focus on the data most pertinent to your LLM’s intended applications, reducing the need for manual processing and enhancing the overall efficiency of the training process. Choose a dataset provider that enables you to filter data by various fields and criteria, ensuring the highest quality and relevance for your LLM.</p></li>
<li><p><strong>Frequent data updates</strong>: For LLMs to stay relevant and up-to-date, it is essential to have access to frequently updated datasets. Many publicly available datasets may only be updated periodically, resulting in outdated or stale information. By choosing a dataset provider that offers near real-time updates, LLM developers can access the latest information and trends as they become available, ensuring that the model remains current and effective in handling emerging topics and language patterns.</p></li>
</ol>
</section>
<section id="acquiring-high-quality-datasets">
<h2>Acquiring High-Quality Datasets<a class="headerlink" href="#acquiring-high-quality-datasets" title="Permalink to this heading">#</a></h2>
<p>When it comes to training LLMs for NLP applications, the quality of the dataset is crucial. To acquire high-quality datasets, you can consider several options, including public datasets, proprietary datasets, and creating your own dataset through web crawling. Here, we explore various options for acquiring high-quality datasets.</p>
<section id="public-datasets">
<h3>Public datasets<a class="headerlink" href="#public-datasets" title="Permalink to this heading">#</a></h3>
<p>Public datasets are often a good starting point when searching for high-quality data. While many public datasets may consist of unfiltered or lightly filtered content, some are curated and maintained by reputable organizations, ensuring a higher level of quality. Some popular public datasets include:</p>
<ol class="arabic simple">
<li><p><strong>Common Crawl</strong>: A massive, publicly available web crawl dataset that contains petabytes of raw web page data, extracted metadata, and plain text extractions.</p></li>
<li><p><strong>WebText2</strong>: A large-scale web text dataset that has been cleaned and preprocessed for NLP applications.</p></li>
<li><p><strong>Wikipedia database</strong>: The full text of Wikipedia articles provides a high-quality source of diverse and well-structured information.</p></li>
</ol>
</section>
<section id="specialized-domains">
<h3>Specialized domains<a class="headerlink" href="#specialized-domains" title="Permalink to this heading">#</a></h3>
<p>Datasets from specialized domains can provide valuable training data for LLMs, particularly for applications that require domain-specific knowledge. Some examples include:</p>
<ol class="arabic simple">
<li><p><strong>PubMed</strong>: A database of biomedical and life science literature.</p></li>
<li><p><strong>USPTO</strong>: United States Patent and Trademark Office data, containing patent applications and granted patents.</p></li>
<li><p><strong>Courtlistener</strong>: A repository of legal opinions and court documents.</p></li>
</ol>
</section>
</section>
<section id="proprietary-datasets">
<h2>Proprietary datasets<a class="headerlink" href="#proprietary-datasets" title="Permalink to this heading">#</a></h2>
<p>Companies and organizations often have access to proprietary datasets that can be leveraged for training LLMs. These datasets may include internal documents, customer interactions, or other domain-specific content that is not publicly available.</p>
</section>
<section id="multilingual-datasets">
<h2>Multilingual datasets<a class="headerlink" href="#multilingual-datasets" title="Permalink to this heading">#</a></h2>
<p>Developing LLMs that support multiple languages requires incorporating multilingual datasets into the training process. These datasets should ideally cover a wide range of languages, dialects, and writing systems to ensure the LLM’s versatility and applicability across different linguistic contexts. However, there are some challenges associated with multilingual models:</p>
<ol class="arabic simple">
<li><p><strong>Imbalanced data</strong>: High-quality datasets are often more readily available for widely spoken languages, such as English, Chinese, and Spanish. This can lead to an imbalance in the training data, where the LLM becomes more proficient in some languages than others. Striving for balanced representation across languages is essential for fair and effective multilingual models.</p></li>
<li><p><strong>Low-resource languages</strong>: Many languages, particularly those with fewer speakers or limited digital presence, suffer from a lack of high-quality training data. These low-resource languages are often underrepresented in multilingual datasets, leading to less accurate and less reliable NLP applications for these languages. To address this issue, researchers can explore techniques such as data augmentation, transfer learning, and unsupervised learning to leverage the limited data available and improve the LLM’s performance for low-resource languages.</p></li>
<li><p><strong>Language-specific nuances</strong>: Each language has its unique characteristics, such as grammar, syntax, and cultural context, which can pose challenges when developing a multilingual LLM. Ensuring that the training data incorporates language-specific nuances and contexts is crucial for achieving accurate and reliable NLP applications across different languages.</p></li>
</ol>
<p>To create or obtain multilingual datasets, consider the following resources:</p>
<ol class="arabic simple">
<li><p><strong>Language-specific datasets</strong>: High-quality datasets containing text from various languages, such as web news articles or social media content. Examples include the Korean datasets for Korean text or the Leipzig Corpora Collection for numerous languages.</p></li>
<li><p><strong>Multilingual parallel corpora</strong>: Datasets containing aligned texts in multiple languages, often used for tasks like machine translation. Examples include the Europarl Parallel Corpus or the United Nations Parallel Corpus.</p></li>
<li><p><strong>Cross-lingual datasets</strong>: Datasets designed specifically for cross-lingual tasks, such as the Cross-lingual Natural Language Inference (XNLI) dataset or the Tatoeba dataset.</p></li>
</ol>
<p>By incorporating multilingual datasets and addressing the challenges associated with imbalanced data, low-resource languages, and language-specific nuances, you can develop LLMs that effectively support a wide range of languages and provide accurate NLP applications across diverse linguistic contexts.</p>
</section>
<section id="building-your-own-dataset-through-web-crawling">
<h2>Building your own dataset through web crawling<a class="headerlink" href="#building-your-own-dataset-through-web-crawling" title="Permalink to this heading">#</a></h2>
<p>When existing datasets don’t meet your requirements, you can consider building your own dataset by developing a web crawler. Web crawlers can be programmed to scrape specific websites or domains, allowing you to collect targeted and relevant information for your LLM.</p>
<ol class="arabic simple">
<li><p><strong>Web scraping tools</strong>: Utilize web scraping tools and libraries such as Scrapy, BeautifulSoup, or Selenium to extract data from websites.</p></li>
<li><p><strong>Customization</strong>: Customize your crawler to focus on specific topics, languages, or websites, ensuring that the collected data is relevant to your LLM’s intended applications.</p></li>
</ol>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<p>Acquiring high-quality datasets for training LLMs is a critical aspect of developing effective NLP applications. By exploring public datasets, specialized domain data, multilingual resources, proprietary datasets, and even creating your own dataset through web crawling, you can ensure that your LLM is trained on the most relevant and high-quality data available. Prioritizing rich metadata, filtering options, and frequent updates will lead to improved performance and more accurate NLP applications.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/nlp_deep/datasets"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../transformers/byt5.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">ByT5: Towards a token-free future with pre-trained byte-to-byte models</p>
      </div>
    </a>
    <a class="right-next"
       href="../tokenization/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Tokenization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-state-of-llms-today">The State of LLMs Today</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#high-quality-vs-low-quality-data">High-Quality vs. Low-Quality Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-collecting-data">Challenges in Collecting Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#three-data-must-haves-for-nlp">Three Data Must-Haves for NLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acquiring-high-quality-datasets">Acquiring High-Quality Datasets</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#public-datasets">Public datasets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specialized-domains">Specialized domains</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#proprietary-datasets">Proprietary datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multilingual-datasets">Multilingual datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-your-own-dataset-through-web-crawling">Building your own dataset through web crawling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://entelecheia.me" target="_blank">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>