

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Zero Shot and Prompt Engineering &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/js/hoverxref.js"></script>
    <script src="../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/chatgpt.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/nlp_deep/zeroshot';</script>
    <link rel="canonical" href="https://lecture.entelecheia.ai/lectures/nlp_deep/zeroshot.html" />
    <link rel="shortcut icon" href="../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Decoding and Search Strategies" href="decoding.html" />
    <link rel="prev" title="Large Language Models" href="llms.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content"><p>
  <strong>Announcement:</strong>
  You can install the accompanying AI tutor <a href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd">here</a>.
</p>
</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_intro/index.html">Introduction to NLP</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/research.html">Research Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/language_models.html">Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic_modeling.html">Topic Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic_coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/sentiments.html">Sentiment Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/word_segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/vectorization.html">Vector Semantics and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/word_embeddings.html">Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab2-corpus-eda.html">Lab 2: EDA on Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab4-tomotopy.html">Lab 4: Topic Modeling Tools- Tomotopy</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Deep Learning for NLP</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="llms.html">Large Language Models</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Zero Shot and Prompt Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="decoding.html">Decoding and Search Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers.html">Transformers</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="detectGPT.html">How to Spot Machine-Written Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
<li class="toctree-l2"><a class="reference internal" href="plms.html">Pretrained Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab3-train-tokenizers.html">Lab 3: Training Tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab4-pretraining-lms.html">Lab 4: Pretraining Language Models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_advances/index.html">Advances in AI and NLP</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/thesis.html">Writing a Thesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/detectGPT.html">DetectGPT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/gpt4.html">GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/camelids.html">Meet the Camelids: A Family of LLMs</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../aiart/index.html">AI Art (Generative AI)</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../aiart/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/aiart.html">Art and Music in Light of AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/imagen.html">Imagen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/textual-inversion.html">Textual Inversion (Dreambooth)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/whisper.html">Automatic Speech Recognition (Whisper)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/text2music.html">Text to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/image2music.html">Image to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/robot_drawings.html">Robot Drawing Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/project-themes.html">Project Themes - A Brave New World</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/devops.html">DevOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/gitops.html">GitOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/devsecops.html">DevSecOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/dotfiles.html">Dotfiles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/auth-enc-sign.html">Authentication, Encryption, and Signing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/age-gpg-ssh.html">SSH, GPG, and AGE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/github.html">Github’s Fork &amp; Pull Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/pass.html">Unix Password Managers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/docker.html">Docker - Containerization and Management</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ds/index.html">Data Science for Economics and Finance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../about/lecture-bot.html">LectureBot for ἐντελέχεια.άι</a></li>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/index.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/entelecheia/lecture/blob/main/book/lectures/nlp_deep/zeroshot.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/nlp_deep/zeroshot.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Zero Shot and Prompt Engineering</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-and-few-shot-learners">Zero Shot and Few Shot Learners</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-engineering">Prompt Engineering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting-on-llms">Prompting on LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-learning">Zero-shot learning:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-shot-learning">One-shot learning:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#few-shot-learning">Few-shot learning:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-reasoners-and-chain-of-thought-prompting">Zero-Shot Reasoners and Chain-of-Thought Prompting</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="zero-shot-and-prompt-engineering">
<h1>Zero Shot and Prompt Engineering<a class="headerlink" href="#zero-shot-and-prompt-engineering" title="Permalink to this heading">#</a></h1>
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/entelecheia_Zero_Shot.png"><img alt="Zero Shot, Prompt, and Search Strategies" class="bg-primary mb-1 align-center" src="../../_images/entelecheia_Zero_Shot.png" style="width: 50%;" /></a>
<section id="zero-shot-and-few-shot-learners">
<h2>Zero Shot and Few Shot Learners<a class="headerlink" href="#zero-shot-and-few-shot-learners" title="Permalink to this heading">#</a></h2>
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/entelecheia_a_robot.png"><img alt="Zero Shot and Few Shot Learners" class="bg-primary mb-1 align-center" src="../../_images/entelecheia_a_robot.png" style="width: 90%;" /></a>
<p>Zero-shot and few-shot learners are machine learning techniques that allow models to generalize and adapt to new tasks with little or no training data. These approaches are particularly useful in situations where obtaining labeled data is difficult or expensive. They are inspired by the human ability to learn new concepts and skills quickly, even from a few examples.</p>
<p>Zero-shot learning (ZSL) refers to the ability of a model to perform a task without having seen any examples from that specific task during training. It relies on the model’s capacity to generalize from related tasks or to leverage auxiliary information such as relationships between classes, attributes, or semantic information. For example, a zero-shot image classification model might recognize a never-seen-before object by leveraging its understanding of similar objects or associated attributes.</p>
<p>Few-shot learning (FSL), on the other hand, involves training a model to perform a task with only a small number of examples (typically less than 10) from the target task. This is in contrast to traditional machine learning methods that often require large amounts of labeled data. Few-shot learning aims to quickly adapt to new tasks by leveraging prior knowledge and transfer learning. Meta-learning and memory-augmented neural networks are common approaches to few-shot learning, focusing on learning a model that can adapt rapidly to new tasks based on limited data.</p>
<p>Both zero-shot and few-shot learners are essential components in the pursuit of artificial general intelligence (AGI), as they enable models to learn efficiently and generalize effectively across various tasks and domains with minimal data.</p>
</section>
<section id="prompt-engineering">
<h2>Prompt Engineering<a class="headerlink" href="#prompt-engineering" title="Permalink to this heading">#</a></h2>
<p>Prompt engineering is the process of designing and refining inputs (prompts) for language models like GPT-3 or GPT-4 to obtain better, more accurate, and contextually relevant outputs. Since large-scale language models are trained to generate text based on the input they receive, crafting effective prompts can significantly influence the quality of the generated responses.</p>
<p>The main goal of prompt engineering is to maximize the usefulness and relevance of a language model’s output by considering factors such as clarity, specificity, context, and constraints. It often involves an iterative process of experimentation and fine-tuning to arrive at the optimal prompt. Some strategies used in prompt engineering include:</p>
<ul class="simple">
<li><p>Clarity: Ensure that the prompt is clear and unambiguous, which helps the language model understand the question or context better.</p></li>
<li><p>Specificity: Make the prompt more specific by including relevant details or asking for specific information. This can help narrow down the potential responses and avoid generic answers.</p></li>
<li><p>Context: Provide sufficient context to guide the model in generating responses that are relevant to the given situation or domain.</p></li>
<li><p>Constraints: Apply constraints on the response format or content, such as specifying the desired answer type (e.g., a list or a single word) or limiting the length of the response.</p></li>
<li><p>Redundancy: Ask the same question in multiple ways or incorporate different perspectives to increase the likelihood of obtaining accurate and comprehensive answers.</p></li>
<li><p>Instructiveness: Use explicit instructions or guiding questions to direct the model’s attention to the relevant aspects of the problem or task.</p></li>
</ul>
<p>Prompt engineering is an essential skill in working with language models as it helps to bridge the gap between the model’s training data and the desired output for specific use cases. By carefully crafting and refining prompts, users can improve the reliability and usefulness of generated responses, making the models more effective across a wide range of applications.</p>
</section>
<section id="prompting-on-llms">
<h2>Prompting on LLMs<a class="headerlink" href="#prompting-on-llms" title="Permalink to this heading">#</a></h2>
<p>Large Language Models (LLMs) like GPT-3 or GPT-4 can be used for various tasks, including zero-shot, one-shot, and few-shot learning. Here are some examples using text prompts for each case:</p>
<section id="zero-shot-learning">
<h3>Zero-shot learning:<a class="headerlink" href="#zero-shot-learning" title="Permalink to this heading">#</a></h3>
<p>Task: Sentiment analysis (positive or negative) for a movie review</p>
<p>Prompt:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Determine</span> <span class="n">the</span> <span class="n">sentiment</span> <span class="n">of</span> <span class="n">the</span> <span class="n">following</span> <span class="n">movie</span> <span class="n">review</span><span class="p">:</span> <span class="s2">&quot;I absolutely loved the movie! The storyline was captivating, and the acting was superb. I can&#39;t wait to watch it again!&quot;</span>
</pre></div>
</div>
<p>Since the LLM has been pre-trained on a diverse range of texts, it should be able to classify the sentiment of the review without any additional examples.</p>
</section>
<section id="one-shot-learning">
<h3>One-shot learning:<a class="headerlink" href="#one-shot-learning" title="Permalink to this heading">#</a></h3>
<p>Task: Animal classification based on a brief description</p>
<p>Prompt:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Based</span> <span class="n">on</span> <span class="n">the</span> <span class="n">given</span> <span class="n">description</span><span class="p">,</span> <span class="n">classify</span> <span class="n">the</span> <span class="n">animal</span><span class="p">:</span>
<span class="n">Example</span><span class="p">:</span> <span class="s2">&quot;This animal has a long neck and long legs. It mainly eats leaves from trees.&quot;</span><span class="p">:</span> <span class="n">Giraffe</span>

<span class="n">Description</span><span class="p">:</span> <span class="s2">&quot;This small creature has a bushy tail, sharp claws, and climbs trees to collect nuts.&quot;</span>
</pre></div>
</div>
<p>By providing an example in the prompt, the LLM can use this context to generate an appropriate classification for the given description.</p>
</section>
<section id="few-shot-learning">
<h3>Few-shot learning:<a class="headerlink" href="#few-shot-learning" title="Permalink to this heading">#</a></h3>
<p>Task: Convert a sentence from active to passive voice</p>
<p>Prompt:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Transform</span> <span class="n">the</span> <span class="n">following</span> <span class="n">sentences</span> <span class="kn">from</span> <span class="nn">active</span> <span class="n">to</span> <span class="n">passive</span> <span class="n">voice</span><span class="p">:</span>
<span class="n">Example</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;John painted the house.&quot;</span> <span class="o">-&gt;</span> <span class="s2">&quot;The house was painted by John.&quot;</span>
<span class="n">Example</span> <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;She baked a cake.&quot;</span> <span class="o">-&gt;</span> <span class="s2">&quot;A cake was baked by her.&quot;</span>

<span class="n">Sentence</span><span class="p">:</span> <span class="s2">&quot;The cat chased the mouse.&quot;</span>
</pre></div>
</div>
<p>By providing multiple examples, the LLM can better understand the desired transformation and apply it to the input sentence.</p>
<p>In each case, the prompt is designed to guide the LLM to perform the desired task with varying amounts of examples. The specific syntax for providing these prompts to an LLM like GPT-3 or GPT-4 depends on the API or library you are using, but the core idea remains the same: designing effective prompts to maximize the usefulness and relevance of the generated outputs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Zero-shot Learning with Hugging Face&#39;s Transformers</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">review</span> <span class="o">=</span> <span class="s2">&quot;I absolutely loved the movie! The storyline was captivating, and the acting was superb. I can&#39;t wait to watch it again!&quot;</span>

<span class="c1"># Initialize the zero-shot classification pipeline</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;zero-shot-classification&quot;</span><span class="p">)</span>

<span class="c1"># Classify sentiment using the pipeline</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">review</span><span class="p">,</span> <span class="n">candidate_labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;positive&quot;</span><span class="p">,</span> <span class="s2">&quot;negative&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
2023-03-30 00:19:57.858469: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).
Using a pipeline without specifying a model name and revision in production is not recommended.
Downloading pytorch_model.bin: 100%|██████████| 1.63G/1.63G [00:23&lt;00:00, 68.1MB/s]
Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00&lt;00:00, 10.3kB/s]
Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:01&lt;00:00, 874kB/s]
Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00&lt;00:00, 553kB/s]
Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:01&lt;00:00, 1.10MB/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>positive
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># One-shot Learning</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Answer the following geography-related question:</span>
<span class="s2">Question: &quot;What is the capital city of France?&quot;: </span>
<span class="s2">Answer: Paris</span>

<span class="s2">Question: &quot;What is the highest mountain in the world?&quot;</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="c1"># Initialize the GPT-2 model and tokenizer</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;gpt2&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Generate a response using the model</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input&#39;s `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Answer the following geography-related question:
Question: &quot;What is the capital city of France?&quot;: 
Answer: Paris

Question: &quot;What is the highest mountain in the world?&quot;

Answer: Mount Everest

Question
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Few-shot Learning</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Transform the following sentences from active to passive voice:</span>
<span class="s2">Example 1: &quot;John painted the house.&quot; -&gt; &quot;The house was painted by John.&quot;</span>
<span class="s2">Example 2: &quot;She baked a cake.&quot; -&gt; &quot;A cake was baked by her.&quot;</span>

<span class="s2">Example 3: &quot;The cat chased the mouse.&quot;</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="c1"># Initialize the GPT-2 model and tokenizer</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;gpt2&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Generate a response using the model</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input&#39;s `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Transform the following sentences from active to passive voice:
Example 1: &quot;John painted the house.&quot; -&gt; &quot;The house was painted by John.&quot;
Example 2: &quot;She baked a cake.&quot; -&gt; &quot;A cake was baked by her.&quot;

Example 3: &quot;The cat chased the mouse.&quot;

Example 4: &quot;The cat was a cat.&quot; -&gt; &quot;The cat was a cat.&quot;

Example 5: &quot;The cat was a cat.&quot; -&gt; &quot;The cat was a cat
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="zero-shot-reasoners-and-chain-of-thought-prompting">
<h2>Zero-Shot Reasoners and Chain-of-Thought Prompting<a class="headerlink" href="#zero-shot-reasoners-and-chain-of-thought-prompting" title="Permalink to this heading">#</a></h2>
<p>The <a class="reference external" href="https://arxiv.org/abs/2205.11916">University of Tokyo and Google Brain team</a> discovered that large language models (LLMs) possess inherent zero-shot abilities in high-level cognitive tasks. These abilities can be extracted using a method called Chain-of-Thought (CoT) prompting.</p>
<p>Further research by the <a class="reference external" href="https://arxiv.org/abs/2201.11903">Google Brain team</a> delved into CoT prompting and found that generating a series of intermediate reasoning steps, or a “chain-of-thought,” significantly improves LLMs’ complex reasoning capabilities. Experiments conducted on three LLMs showed that CoT prompting enhances performance in arithmetic, common sense, and symbolic reasoning tasks.</p>
<p>Here’s an example to illustrate CoT prompting:</p>
<ul class="simple">
<li><p>Q: Jane has 7 books on her shelf. She borrowed 4 books from the library. How many books does she have now?</p></li>
<li><p>A: Jane had 7 books initially. She borrowed 4 books from the library. So, 7 + 4 = 11. The answer is 11.</p></li>
</ul>
<p>And another example:</p>
<ul class="simple">
<li><p>Q: A shop had 15 umbrellas. 8 umbrellas were sold, and they restocked 5 more. How many umbrellas do they have now?</p></li>
<li><p>A: The shop had 15 umbrellas originally. They sold 8 umbrellas, leaving 15 - 8 = 7. They restocked 5 more umbrellas, so they have 7 + 5 = 12. The answer is 12.</p></li>
</ul>
<p>In summary, chain-of-thought reasoning enables models to break down complex problems into smaller, manageable steps that can be solved individually. The language-based nature of CoT prompting makes it applicable to any task that can be solved through language. Empirical experiments have shown that CoT prompting improves performance across various reasoning tasks, and successful chain-of-thought reasoning emerges as models scale up.</p>
<p>Here’s a Python example using Hugging Face Transformers to demonstrate Zero-Shot Reasoners and Chain-of-Thought Prompting with the GPT-2 model:</p>
<p><strong>Import libraries and prepare the model</strong></p>
<p>The following code imports necessary libraries, TensorFlow and Hugging Face Transformers, to work with the GPT-2 model. It initializes a GPT-2 tokenizer and loads the pre-trained GPT-2 model from Hugging Face’s model hub. The EOS token is set as the PAD token to avoid warnings during tokenization and padding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFGPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>

<span class="k">def</span> <span class="nf">generate_chain_of_thought</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;tf&quot;</span><span class="p">)</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">generated_text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFGPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>

<span class="c1"># Example problem</span>
<span class="n">problem</span> <span class="o">=</span> <span class="s2">&quot;Jane has 7 books on her shelf. She borrowed 4 books from the library. How many books does she have now?&quot;</span>

<span class="c1"># Chain-of-Thought Prompt</span>
<span class="n">cot_prompt</span> <span class="o">=</span> <span class="s2">&quot;To find out the total number of books Jane has now, we can first count the number of books on her shelf, which is 7. Then we count the number of books she borrowed from the library, which is 4. Now, we add these two quantities together. What is 7 + 4?&quot;</span>

<span class="c1"># Generate the answer</span>
<span class="n">answer</span> <span class="o">=</span> <span class="n">generate_chain_of_thought</span><span class="p">(</span><span class="n">cot_prompt</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>All model checkpoint layers were used when initializing TFGPT2LMHeadModel.

All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>To find out the total number of books Jane has now, we can first count the number of books on her shelf, which is 7. Then we count the number of books she borrowed from the library, which is 4. Now, we add these two quantities together. What is 7 + 4? Well, it&#39;s the number of books she borrowed from the library. So, if you have a library of about 500 books, you have about 7 books on your shelf. So, if you have
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># You have to install the OpenAI Python library by running the following command:</span>
<span class="c1"># pip install openai</span>
<span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="s2">&quot;API_KEY&quot;</span>

<span class="k">def</span> <span class="nf">generate_chain_of_thought</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">model_engine</span><span class="o">=</span><span class="s2">&quot;text-davinci-003&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Completion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">engine</span><span class="o">=</span><span class="n">model_engine</span><span class="p">,</span>
        <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
        <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">stop</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="c1"># Example problem</span>
<span class="n">problem</span> <span class="o">=</span> <span class="s2">&quot;Jane has 7 books on her shelf. She borrowed 4 books from the library. How many books does she have now?&quot;</span>

<span class="c1"># Chain-of-Thought Prompt</span>
<span class="n">cot_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Jane had 7 books initially. She borrowed 4 books from the library. What is the total number of books Jane has now? 7 + 4 =&quot;</span>

<span class="c1"># Generate the answer</span>
<span class="n">answer_text</span> <span class="o">=</span> <span class="n">generate_chain_of_thought</span><span class="p">(</span><span class="n">cot_prompt</span><span class="p">)</span>
<span class="n">answer</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\d+&quot;</span><span class="p">,</span> <span class="n">answer_text</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\d+&quot;</span><span class="p">,</span> <span class="n">answer_text</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;unknown&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/nlp_deep"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="llms.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Large Language Models</p>
      </div>
    </a>
    <a class="right-next"
       href="decoding.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Decoding and Search Strategies</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-and-few-shot-learners">Zero Shot and Few Shot Learners</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-engineering">Prompt Engineering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting-on-llms">Prompting on LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-learning">Zero-shot learning:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-shot-learning">One-shot learning:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#few-shot-learning">Few-shot learning:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-reasoners-and-chain-of-thought-prompting">Zero-Shot Reasoners and Chain-of-Thought Prompting</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://entelecheia.me">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>