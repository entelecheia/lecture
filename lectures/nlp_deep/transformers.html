

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Transformers &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/js/hoverxref.js"></script>
    <script src="../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/chatgpt.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/nlp_deep/transformers';</script>
    <link rel="canonical" href="https://lecture.entelecheia.ai/lectures/nlp_deep/transformers.html" />
    <link rel="shortcut icon" href="../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Reinforcement Learning with Human Feedback (RLHF)" href="rlhf.html" />
    <link rel="prev" title="Decoding and Search Strategies" href="decoding.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content"><p>
  <strong>Announcement:</strong>
  You can install the accompanying AI tutor <a href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd">here</a>.
</p>
</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_intro/index.html">Introduction to NLP</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/research.html">Research Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/language_models.html">Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic_modeling.html">Topic Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/tomotopy.html">Topic Modeling Tools (Tomotopy)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic_coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/sentiments.html">Sentiment Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/word_segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/vectorization.html">Vector Semantics and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/word_embeddings.html">Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab2-corpus-eda.html">Lab 2: EDA on Corpora</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Deep Learning for NLP</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="llms.html">Large Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="zeroshot.html">Zero Shot and Prompt Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="decoding.html">Decoding and Search Strategies</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Transformers </a></li>
<li class="toctree-l2"><a class="reference internal" href="rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="detectGPT.html">How to Spot Machine-Written Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
<li class="toctree-l2"><a class="reference internal" href="plms.html">Pretrained Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab3-train-tokenizers.html">Lab 3: Training Tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab4-pretraining-lms.html">Lab 4: Pretraining Language Models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_advances/index.html">Advances in AI and NLP</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/thesis.html">Writing a Thesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/detectGPT.html">DetectGPT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/gpt4.html">GPT-4</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../aiart/index.html">AI Art (Generative AI)</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../aiart/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/aiart.html">Art and Music in Light of AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/imagen.html">Imagen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/textual-inversion.html">Textual Inversion (Dreambooth)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/whisper.html">Automatic Speech Recognition (Whisper)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/text2music.html">Text to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/image2music.html">Image to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/robot_drawings.html">Robot Drawing Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/project-themes.html">Project Themes - A Brave New World</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/devops.html">DevOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/gitops.html">GitOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/devsecops.html">DevSecOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/dotfiles.html">Dotfiles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/auth-enc-sign.html">Authentication, Encryption, and Signing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/age-gpg-ssh.html">SSH, GPG, and AGE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/github.html">Github’s Fork &amp; Pull Workflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ds/index.html">Data Science for Economics and Finance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../about/lecture-bot.html">LectureBot for ἐντελέχεια.άι</a></li>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/index.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/entelecheia/lecture/blob/main/book/lectures/nlp_deep/transformers.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/nlp_deep/transformers.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Transformers </h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-transformers">Why do we need transformers?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-is-all-you-need">Attention is all you need?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-implement-this-attention-mechanism">How do we implement this attention mechanism?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-head attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tranformer-architecture">Tranformer Architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-the-transformer-architecture">Problems with the Transformer architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-is-not-all-you-need">Attention Is Not All You Need</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#so-why-are-transformers-so-powerful">So why are transformers so powerful?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vision-transformers">Vision Transformers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-input-images-into-a-transformer">How to input images into a transformer?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vision-transformer-architecture">Vision Transformer Architecture</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-in-transformer">Transformer in Transformer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#timesformers">TimeSformers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-machine-learning">Multimodal Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vatt-transformers-for-multimodal-self-supervised-learning">VATT: Transformers for Multimodal Self-Supervised Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gato-a-generalist-agent">GATO: A Generalist Agent</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="transformers-jupyter-book-badge">
<h1>Transformers <a class="reference external" href="https://entelecheia.github.io/ekorpkit-book/"><img alt="Jupyter Book Badge" src="https://jupyterbook.org/badge.svg" /></a><a class="headerlink" href="#transformers-jupyter-book-badge" title="Permalink to this heading">#</a></h1>
<p><a class="reference external" href="https://www.topbots.com/transformers-timesformers-and-attention/">On Transformers, TimeSformers, And Attention</a></p>
<p><img alt="" src="../../_images/entelecheia_transformers.png" /></p>
<p>Transformers are a very powerful Deep Learning model that has been able to become a standard in many Natural Language Processing tasks and is poised to revolutionize the field of Computer Vision as well.</p>
<ul class="simple">
<li><p>Google Brain published the paper “Attention Is All You Need” in 2017 <span id="id1">[<a class="reference internal" href="../../about/index.html#id5" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017.">Vaswani <em>et al.</em>, 2017</a>]</span>.</p></li>
<li><p>The paper introduced the Transformer model, which is a deep learning model that is able to perform well on many NLP tasks.</p></li>
<li><p>The model is based on the idea of attention, which is a way of focusing on certain parts of the input.</p></li>
<li><p>The model is able to perform well on many NLP tasks, including machine translation, summarization, and question answering.</p></li>
<li><p>The model is also able to perform well on many other tasks, including image classification and speech recognition.</p></li>
</ul>
<p><img alt="" src="../../_images/transformers-history.jpeg" /></p>
<p>In 2020, Google Brain asks “will they be as effective on images?”</p>
<ul class="simple">
<li><p>The paper “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” <span id="id2">[<a class="reference internal" href="../../about/index.html#id7" title="Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and others. An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.">Dosovitskiy <em>et al.</em>, 2020</a>]</span> was published in 2020.</p></li>
<li><p>The model is able to perform well on many Computer Vision tasks, including image classification and object detection.</p></li>
</ul>
<p>At the beginning of 2021, Facebook researchers published a new version of the Transformers model, called TimeSformer.</p>
<ul class="simple">
<li><p>The paper “Is space-time attention all you need for video understanding?” <span id="id3">[<a class="reference internal" href="../../about/index.html#id8" title="Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, volume 2, 4. 2021.">Bertasius <em>et al.</em>, 2021</a>]</span> was published in 2021.</p></li>
</ul>
<section id="why-do-we-need-transformers">
<h2>Why do we need transformers?<a class="headerlink" href="#why-do-we-need-transformers" title="Permalink to this heading">#</a></h2>
<p>What are the problems with the previous models?</p>
<ul class="simple">
<li><p>The previous models were based on Recurrent Neural Networks (RNNs).</p></li>
<li><p>RNNs are able to process sequences of data, such as text or audio.</p></li>
<li><p>One of the main problems is its sequential operation.</p></li>
<li><p>The model needs to process the input sequentially, which means that it needs to process the first word, then the second word, and so on.</p></li>
<li><p>This sequential operation makes it difficult to parallelize the model.</p></li>
<li><p>There are also other problems such as gradient explosion, inability to detect dependencies between distant words in the same sentence, and so on.</p></li>
</ul>
<p>For example, to translate a sentence from English to Italian, with this type of networks, the first word of the sentence to be translated was passed into an encoder together with an initial state, and the next state was then passed into a second encoder with the second word of the sentence, and so on until the last word. The resulting state from the last encoder is then passed to a decoder that returns as output both the first translated word and a subsequent state, which is passed to another decoder, and so on.</p>
<p><img alt="" src="../../_images/problem-rnn.gif" /></p>
</section>
<section id="attention-is-all-you-need">
<h2>Attention is all you need?<a class="headerlink" href="#attention-is-all-you-need" title="Permalink to this heading">#</a></h2>
<p>Is there a mechanism that we can compute in a parallelized manner that allows us to extract the information we need from the sentence?</p>
<p><img alt="" src="../../_images/attention.gif" /></p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">I</span> <span class="pre">gave</span> <span class="pre">my</span> <span class="pre">dog</span> <span class="pre">Charlie</span> <span class="pre">some</span> <span class="pre">food.</span></code></p>
</div></blockquote>
<ul class="simple">
<li><p>Focusing on the word “gave,” what other words in the sentence should we pay attention on to add context to the word “gave”?</p></li>
<li><p>You might ask yourself, “Who gave the food to the dog?”</p></li>
<li><p>In this case, the attention mechanism would focus on the words “I.”</p></li>
<li><p>If you were to ask yourself, “To whom did I give the food?”</p></li>
<li><p>The attention mechanism would focus on the words “dog” and “Charlie.”</p></li>
<li><p>If you were to ask yourself, “What did I give to the dog?”</p></li>
<li><p>The attention mechanism would focus on the words “food.”</p></li>
</ul>
<section id="how-do-we-implement-this-attention-mechanism">
<h3>How do we implement this attention mechanism?<a class="headerlink" href="#how-do-we-implement-this-attention-mechanism" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>To understand the computation of attention we can draw parallels to the world of databases.</p></li>
<li><p>When we do a search in the database we submit a query (Q) and we search among the available data for one or more keys that satisfy the query.</p></li>
<li><p>The keys are the words in the sentence and the query is the word we want to focus on.</p></li>
<li><p>The result of the search is the value of the key that satisfies the query.</p></li>
</ul>
<p><img alt="" src="../../_images/attention-calculate.gif" /></p>
<ul class="simple">
<li><p>We begin by looking at the sentence on which to compute attention as a set of vectors.</p></li>
<li><p>Each word, via a word embedding mechanism, is encoded into a vector K.</p></li>
<li><p>These vectors are the keys to search for with respect to a query.</p></li>
<li><p>A query is a vector Q that represents the word we want to focus on.</p></li>
<li><p>The query could be a word from the same sentence (self-attention) or a word from another sentence (cross-attention).</p></li>
<li><p>When then compute the similarity between the query Q and each of the available keys K.</p></li>
<li><p>The similarity is computed by multiplying the query Q by the transpose of the keys K.</p></li>
<li><p>The result of this operation is a vector of scores that represent the similarity between the query and each of the keys.</p></li>
<li><p>The scores are then normalized to obtain a probability distribution by applying the softmax function.</p></li>
<li><p>The result of the softmax function is a vector of probabilities that represent the attention weights.</p></li>
<li><p>The attention weights are then multiplied by the sentence vector, which is a vector of the same dimension as the keys, where each value represents the word in the sentence.</p></li>
<li><p>The result of this operation is a vector that represents the context of the word we want to focus on.</p></li>
<li><p>The context vector C is a vector of the same dimensionality as the keys K, where each element is a weighted sum of the keys K.</p></li>
<li><p>The context vector C is then passed to a linear layer, which is a fully connected layer, to obtain the final result of the attention mechanism.</p></li>
</ul>
<p><img alt="" src="../../_images/attention-focus.jpeg" /></p>
<ul class="simple">
<li><p>Each vector represents a word in the sentence.</p></li>
<li><p>The word we want to focus on is represented by the vector Q.</p></li>
<li><p>We then compute the similarity between the vector Q and each of the vectors in the sentence.</p></li>
<li><p>The similarity is computed by multiplying the vector Q by the vector of each word in the sentence.</p></li>
<li><p>The result of the multiplication is a scalar value that represents the similarity between the vector Q and the vector of the word in the sentence.</p></li>
<li><p>The scalar value is then passed through a softmax function, which normalizes the values between 0 and 1.</p></li>
<li><p>The result of the softmax function is the attention vector.</p></li>
<li><p>The attention vector is a vector of the same size as the sentence, where each value represents the attention that should be given to each word in the sentence.</p></li>
<li><p>The attention vector is then multiplied by the sentence vector, which is a vector of the same size as the sentence, where each value represents the word in the sentence.</p></li>
<li><p>The result of the multiplication is a vector of the same size as the sentence, where each value represents the weighted sum of the words in the sentence.</p></li>
<li><p>The weighted sum is then passed through a linear layer, which is a fully connected layer, to obtain the final result.</p></li>
</ul>
</section>
<section id="multi-head-attention">
<h3>Multi-head attention<a class="headerlink" href="#multi-head-attention" title="Permalink to this heading">#</a></h3>
<p>This mechanism would be sufficient if we wanted to focus on a single word. However, we want to focus on from several points of view.</p>
<ul class="simple">
<li><p>With a simliar mechanism, we can use multiple keys to focus on different words in the sentence.</p></li>
<li><p>The results are then concatenated to obtain a single, summarized vector of all the attention mechanisms.</p></li>
<li><p>This mechanism is called multi-head attention.</p></li>
</ul>
<p><img alt="" src="../../_images/attention-multihead.jpeg" /></p>
</section>
</section>
<section id="tranformer-architecture">
<h2>Tranformer Architecture<a class="headerlink" href="#tranformer-architecture" title="Permalink to this heading">#</a></h2>
<p>The Transformer architecture is composed of two main components: the encoder and the decoder.</p>
<p><img alt="" src="../../_images/transformer-architecture.gif" /></p>
<p>Consider a transformer model that is trained to translate a sentence from English to Italian.</p>
<ul class="simple">
<li><p>The encoder is responsible for encoding the input sentence into a vector.</p></li>
<li><p>The encoder takes as input a sentence in English and returns a vector.</p></li>
<li><p>Unlike the RNN, the encoder does not need to process the sentence sequentially.</p></li>
<li><p>Before passing the sentence to the encoder, the sentence is first tokenized into words.</p></li>
<li><p>Each word is then passed through a word embedding mechanism to obtain a vector.</p></li>
<li><p>Before proceeding with the attention computation, the vectors are combined with a positional encoding vector.</p></li>
<li><p>The positional encoding vector is a vector of the same size as the word embedding vector, where each value represents the position of the word in the sentence based on sine and cosine functions.</p></li>
<li><p>This is important because the order of the words in the sentence is more than relevant and this information is required to understand the meaning of the sentence.</p></li>
<li><p>The result of the addition is then passed to the multi-head attention mechanism.</p></li>
<li><p>The result is normalized and passed to a feed-forward neural network.</p></li>
<li><p>The encoding can be repeated multiple times to obtain a more detailed representation of the sentence.</p></li>
</ul>
<ul class="simple">
<li><p>The decoder is responsible for decoding the vector into a sentence in Italian.</p></li>
<li><p>The decoder takes as input the output of the encoder and returns a sentence in Italian.</p></li>
<li><p>Assume that we have already translated the first two words and we want to predict the third word of the sentence in Italian.</p></li>
<li><p>The decoder takes as input the first two words of the sentence in Italian and the output of the encoder.</p></li>
<li><p>The positional encoding and multi-head attention mechanisms are applied to the first two words of the sentence in Italian.</p></li>
<li><p>The result is then concatenated with the output of the encoder.</p></li>
<li><p>The attention is recalculated on the concatenated vector.</p></li>
<li><p>The result is normalized and passed to a feed-forward neural network.</p></li>
<li><p>The result will be a vector of potential candidates for the third word of the sentence in Italian.</p></li>
<li><p>In the next iteration, the decoder will take as input the first three words of the sentence in Italian and the output of the encoder.</p></li>
</ul>
<p><img alt="" src="../../_images/transformer-best.jpeg" /></p>
</section>
<section id="problems-with-the-transformer-architecture">
<h2>Problems with the Transformer architecture<a class="headerlink" href="#problems-with-the-transformer-architecture" title="Permalink to this heading">#</a></h2>
<p><img alt="" src="../../_images/transformer-problem.gif" /></p>
<p>The Transformer architecture is a very powerful architecture, but it has some problems.</p>
<ul class="simple">
<li><p>One of its strengths is also its weakness, the calculation of attention is very expensive.</p></li>
<li><p>The attention mechanism is very expensive because it requires a lot of computation.</p></li>
<li><p>In order to calculate the attention of each word with respect to all the others I have to perform <span class="math notranslate nohighlight">\(N^2\)</span> calculations.</p></li>
<li><p>Graphically you can imagine a matrix that has to be filled with the attention values of each word compared to any other word.</p></li>
<li><p>Optionally and usually on the decoder, it is possible to calculate the masked attention to avoid the calculation of the attention of a word with respect to the following words.</p></li>
<li><p>The masked attention is a mechanism that allows you to calculate the attention of a word with respect to the previous words.</p></li>
</ul>
<p><img alt="" src="../../_images/transformer-N2.gif" /></p>
</section>
<section id="attention-is-not-all-you-need">
<h2>Attention Is Not All You Need<a class="headerlink" href="#attention-is-not-all-you-need" title="Permalink to this heading">#</a></h2>
<p>In March 2021, Google researchers published a paper titled, “Attention Is Not All You Need” <span id="id4">[<a class="reference internal" href="../../about/index.html#id9" title="Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning, 2793–2803. PMLR, 2021.">Dong <em>et al.</em>, 2021</a>]</span>.</p>
<ul class="simple">
<li><p>The researchers conducted experiments analyzing the behaviour of the self-attention mechanism conducted without any of the other components of the transformers.</p></li>
<li><p>They found that it converges to a rank 1 matrix with a doubly exponential rate.</p></li>
<li><p>This means that this mechanism, by itself, is practically useless.</p></li>
</ul>
<section id="so-why-are-transformers-so-powerful">
<h3>So why are transformers so powerful?<a class="headerlink" href="#so-why-are-transformers-so-powerful" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The researchers found that the self-attention mechanism is not the only component that makes transformers so powerful.</p></li>
<li><p>It is due to a tug of war between the self-attention mechanism that tends to reduce the rank of the matrix and two other components of transformers, skip connections and MLP.</p></li>
<li><p>The skip connections allow the model to diversify the distribution of paths avoiding all the same path.</p></li>
<li><p>This drastically reduces the probability of the model converging to a rank 1 matrix.</p></li>
<li><p>The MLP instead manages to increase the rank of the matrix due to the non-linearity of the activation function.</p></li>
<li><p>Therefore, attention is not all you need, but it is necessary to have skip connections and MLP to make transformers powerful.</p></li>
<li><p>The transformer architecture manages to use the self-attention mechanism to its advantage to achieve impressive results.</p></li>
</ul>
<p><img alt="" src="../../_images/transformer-tug-of-war.jpeg" /></p>
</section>
</section>
<section id="vision-transformers">
<h2>Vision Transformers<a class="headerlink" href="#vision-transformers" title="Permalink to this heading">#</a></h2>
<p>“If Transformers have been found to be so effective in the field of Natural Language Processing, how will they perform with images?”</p>
<p><img alt="" src="../../_images/vit.jpeg" /></p>
<p>If we consider a picture of a dog standing in front of a wall, we can imagine that the dog is the main subject of the picture and the wall is the background.</p>
<ul class="simple">
<li><p>This is because we are focusing on the dominant subject of the picture.</p></li>
<li><p>This is the same concept that we use to understand the meaning of a sentence.</p></li>
<li><p>This is exactly what the self-attention mechanism applied to images does.</p></li>
</ul>
<section id="how-to-input-images-into-a-transformer">
<h3>How to input images into a transformer?<a class="headerlink" href="#how-to-input-images-into-a-transformer" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>A first solution would be to use all the pixels of the image and pass them to the transformer.</p></li>
<li><p>This solution is not very efficient because it would require a lot of computation.</p></li>
<li><p>The calculation of attention has a complexity equal to <span class="math notranslate nohighlight">\(O(N^2)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of pixels.</p></li>
<li><p>This means that the calculation of attention would require <span class="math notranslate nohighlight">\(O(N^4)\)</span> calculations.</p></li>
<li><p>This is not a viable solution because it would require a lot of computation.</p></li>
</ul>
<p><img alt="" src="../../_images/vit-pixels.gif" /></p>
<p>The solution is simple.</p>
<ul class="simple">
<li><p>The image is divided into patches.</p></li>
<li><p>Each patch is converted into a vector using a linear projection.</p></li>
</ul>
<p><img alt="" src="../../_images/vit-projection.gif" /></p>
</section>
<section id="vision-transformer-architecture">
<h3>Vision Transformer Architecture<a class="headerlink" href="#vision-transformer-architecture" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Vectors obtained from a linear projection are then coupled with positional encoding vectors.</p></li>
<li><p>The result is then passed to a classic transformer architecture.</p></li>
<li><p>The result is a vector that represents the image.</p></li>
<li><p>The vector is then passed to a classifier to obtain the final result.</p></li>
</ul>
<p><img alt="" src="../../_images/vit-architecture.gif" /></p>
</section>
</section>
<section id="transformer-in-transformer">
<h2>Transformer in Transformer<a class="headerlink" href="#transformer-in-transformer" title="Permalink to this heading">#</a></h2>
<p>In the transition from patch to vector, any kind of information about the position of pixels in the patch is lost. Is it possible to find a better way to get the vectors to submit to the transformer?</p>
<p>The authors of Transformer in Transformer (TnT) <span id="id5">[<a class="reference internal" href="../../about/index.html#id10" title="Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. Advances in Neural Information Processing Systems, 34:15908–15919, 2021.">Han <em>et al.</em>, 2021</a>]</span> point out because the arrangement of pixels within a portion of the image to be analyzed is certain information we would not want to lose in order to make a quality prediction.</p>
<ul class="simple">
<li><p>Their proposal is then to take each individual patch (pxp) of the image, which are themselves images on 3 RGB channels, and transform it into a c-channel tensor.</p></li>
<li><p>This tensor is then divided into <span class="math notranslate nohighlight">\(p^\prime\)</span> parts with <span class="math notranslate nohighlight">\(p^\prime&lt;p\)</span>, in the example <span class="math notranslate nohighlight">\(p^\prime=4\)</span>.</p></li>
<li><p>This yields <span class="math notranslate nohighlight">\(p’\)</span> vectors in <span class="math notranslate nohighlight">\(c\)</span> dimensions.</p></li>
<li><p>These vectors now contain information about the arrangement of pixels within the patch.</p></li>
<li><p>They are then concatenated and linearly projected in order to make them the same size as the vector obtained from the linear projection of the original patch and combined with it.</p></li>
<li><p>By doing this the authors have managed to further improve performance on various computer vision tasks.</p></li>
</ul>
<p><img alt="" src="../../_images/tnt.gif" /></p>
<p><img alt="" src="../../_images/tnt-architecture.gif" /></p>
</section>
<section id="timesformers">
<h2>TimeSformers<a class="headerlink" href="#timesformers" title="Permalink to this heading">#</a></h2>
<p>In 2021 Facebook researchers tried to apply this architecture to video as well.</p>
<ul class="simple">
<li><p>The idea is to divide the video into frames and then apply the same procedure as for images.</p></li>
<li><p>There is only one small detail that makes them different from Vision Transformers.</p></li>
<li><p>You have to take into account the temporal dimension of the video besides the spatial dimension.</p></li>
</ul>
<p><img alt="" src="lectures/figs/deep_nlp/transformers/timesformer-video.gif" /></p>
<p>The authors have suggested several new attention mechanisms, from those that focus exclusively on space, used primarily as a reference point, to those that compute attention axially, scattered, or jointly between space and time.</p>
<p><img alt="" src="../../_images/timesformer-architecture.gif" /></p>
<p><img alt="" src="../../_images/timesformer-attentions.jpeg" /></p>
<ul class="simple">
<li><p>The method that has achieved the best results is Divided Space-Time Attention.</p></li>
<li><p>It consists, given a frame at instant t and one of its patches as a query, to compute the spatial attention over the whole frame and then the temporal attention in the same patch of the query but in the previous and next frame.</p></li>
<li><p>Why does this approach work so well?</p></li>
<li><p>The reason is that it learns more separate features than other approaches and is, therefore, better able to understand videos from different categories.</p></li>
</ul>
<p>We can see this in the following visualization where each video is represented by a point in space and its colour represents the category it belongs to.</p>
<p><img alt="" src="../../_images/timesformer-divide.jpeg" /></p>
<ul class="simple">
<li><p>The authors ound that the higher the resolution the better the accuracy of the model, up to a point.</p></li>
<li><p>As for the number of frames, again as the number of frames increases, the accuracy also increases.</p></li>
<li><p>It was not possible to make tests with a higher number of frames than that shown in the graph and therefore potentially the accuracy could still improve.</p></li>
<li><p>The upper limit of this improvement is not yet known.</p></li>
</ul>
<p><img alt="" src="../../_images/timesformer-resolution.jpeg" /></p>
<p>In Vision Transformers it is known that a larger training dataset often results in better accuracy. This was also checked by the authors on TimeSformers and again, as the number of training videos considered increases, the accuracy also increases.</p>
<p><img alt="" src="../../_images/timesformer-dataset.jpeg" /></p>
<p>Transformers have just landed in the world of computer vision and seem to be more than determined to replace traditional convolutional networks or at least carve out an important role for themselves in this area.</p>
<ul class="simple">
<li><p>Transformers are a powerful architecture that has revolutionized the field of Natural Language Processing.</p></li>
<li><p>They have been able to achieve impressive results in various tasks.</p></li>
<li><p>They have also been applied to other areas such as computer vision.</p></li>
<li><p>The results obtained are very promising and it is likely that they will continue to improve in the future.</p></li>
</ul>
</section>
<section id="multimodal-machine-learning">
<h2>Multimodal Machine Learning<a class="headerlink" href="#multimodal-machine-learning" title="Permalink to this heading">#</a></h2>
<p>Having now a single architecture capable of working with different types of data, we can now start to think about how to combine them.</p>
<ul class="simple">
<li><p>This is called multimodal machine learning.</p></li>
<li><p>People are able to combine information from several sources to draw their own inferences.</p></li>
<li><p>They simultaneously receive data by observing the world around them with their eyes, but also by smelling its scents, listening to its sounds or touching its shapes.</p></li>
<li><p>This is why we are able to understand the world around us.</p></li>
<li><p>We can now try to replicate this ability in machines.</p></li>
<li><p>The problem lies in treating all the different inputs in the same way without losing information.</p></li>
<li><p>Trnasformers are a good candidate for this task.</p></li>
<li><p>They are able to process different types of data and combine them in a single architecture.</p></li>
</ul>
</section>
<section id="vatt-transformers-for-multimodal-self-supervised-learning">
<h2>VATT: Transformers for Multimodal Self-Supervised Learning<a class="headerlink" href="#vatt-transformers-for-multimodal-self-supervised-learning" title="Permalink to this heading">#</a></h2>
<p>One of the most important applications of Transformers in the field of Multimodal Machine Learning is VATT <span id="id6">[<a class="reference internal" href="../../about/index.html#id11" title="Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: transformers for multimodal self-supervised learning from raw video, audio and text. Advances in Neural Information Processing Systems, 34:24206–24221, 2021.">Akbari <em>et al.</em>, 2021</a>]</span>.</p>
<p><img alt="" src="../../_images/vatt.gif" /></p>
<p>The proposed architecture is composed of a single Transformer Encoder on which three distinct forward calls are made.</p>
<ul class="simple">
<li><p>One call for each type of input data is always transformed into a sequence of tokens.</p></li>
<li><p>The transformer takes these sequences as input and returns three distinct sets of features.</p></li>
<li><p>Then the features are given in input to a contrastive estimation block that calculates a single loss and performs the backward.</p></li>
<li><p>In this way the loss is the result of the error committed on all the three types of data considered.</p></li>
<li><p>Therefore the model, between the epochs, will learn to reduce it by managing better the information coming from all the three different sources.</p></li>
<li><p>VATT represents the culmination of what Multimodal Machine Learning had been trying to achieve for years, a single model that handles completely different types of data together.</p></li>
</ul>
</section>
<section id="gato-a-generalist-agent">
<h2>GATO: A Generalist Agent<a class="headerlink" href="#gato-a-generalist-agent" title="Permalink to this heading">#</a></h2>
<p>Is it possible to realize a neural network capable of receiving inputs of different types and then being able to perform different tasks?</p>
<ul class="simple">
<li><p>This is the question that the authors of GATO <span id="id7">[<a class="reference internal" href="../../about/index.html#id12" title="Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, and others. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.">Reed <em>et al.</em>, 2022</a>]</span> have tried to answer.</p></li>
<li><p>GATO is a multi-modal, multi-task, multi-embodiment generalist that represents one of the most impressive achievements in this field today.</p></li>
<li><p>How does it work?</p></li>
<li><p>GATO is composed of a single Transformer Encoder that receives as input a sequence of tokens representing the different types of data.</p></li>
<li><p>Thanks to this unification of inputs and to the Transformer architecture, GATO is able to learn to combine the different types of data and to perform different tasks, achieving an unprecedented level of generalisation.</p></li>
</ul>
<p><img alt="" src="lectures/figs/deep_nlp/transformers/gato.gif" /></p>
<p><img alt="" src="lectures/figs/deep_nlp/transformers/gato-examples.gif" /></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/nlp_deep"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="decoding.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Decoding and Search Strategies</p>
      </div>
    </a>
    <a class="right-next"
       href="rlhf.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Reinforcement Learning with Human Feedback (RLHF)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-transformers">Why do we need transformers?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-is-all-you-need">Attention is all you need?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-implement-this-attention-mechanism">How do we implement this attention mechanism?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-head attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tranformer-architecture">Tranformer Architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-the-transformer-architecture">Problems with the Transformer architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-is-not-all-you-need">Attention Is Not All You Need</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#so-why-are-transformers-so-powerful">So why are transformers so powerful?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vision-transformers">Vision Transformers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-input-images-into-a-transformer">How to input images into a transformer?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vision-transformer-architecture">Vision Transformer Architecture</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-in-transformer">Transformer in Transformer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#timesformers">TimeSformers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-machine-learning">Multimodal Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vatt-transformers-for-multimodal-self-supervised-learning">VATT: Transformers for Multimodal Self-Supervised Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gato-a-generalist-agent">GATO: A Generalist Agent</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://entelecheia.me">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>