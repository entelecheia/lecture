<!doctype html>
<html class="no-js">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" /><link rel="next" title="Tokenization" href="tokenization.html" /><link rel="prev" title="Transformers" href="transformers.html" />

    <meta name="generator" content="sphinx-4.5.0, furo 2022.09.29"/>
        <title>T5: Text-To-Text Transfer Transformer - Artificial Intelligence | Lecture Notes</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=d81277517bee4d6b0349d71bb2661d4890b5617c" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">Artificial Intelligence | Lecture Notes</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  
  
  <span class="sidebar-brand-text">Artificial Intelligence | Lecture Notes</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_intro/index.html">Introduction to NLP</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/research.html">Research Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/language_models.html">Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic.html">Topic Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic_models.html">Topic Models </a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic_coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/sentiments.html">Sentiment Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/word_segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/vectorization.html">Vector Semantics and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/word_embeddings.html">Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab2-corpus-eda.html">Lab 2: EDA on Corpora</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Deep Learning for NLP</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="zeroshot.html">Zero Shot, Prompt, and Search Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers.html">Transformers </a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
<li class="toctree-l2"><a class="reference internal" href="plms.html">Pretrained Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab3-train-tokenizers.html">Lab 3: Training Tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab4-pretraining-lms.html">Lab 4: Pretraining Language Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../nlp_apps/index.html">NLP Applications</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../aiart/index.html">AI Art (Text-to-Image)</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../aiart/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/project-themes.html">Project Themes - A Brave New World</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/imagen.html">Imagen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/textual-inversion.html">Textual Inversion (Dreambooth)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/whisper.html">Automatic Speech Recognition (Whisper)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/text2music.html">Text to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/image2music.html">Image to Music</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../mlops/dev-env.html">Development Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/dotfiles.html">Dotfiles</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ds/index.html">Data Science for Economics and Finance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/index.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/cite.html">Citation</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section class="tex2jax_ignore mathjax_ignore" id="t5-text-to-text-transfer-transformer">
<h1>T5: Text-To-Text Transfer Transformer<a class="headerlink" href="#t5-text-to-text-transfer-transformer" title="Permalink to this headline">#</a></h1>
<p><img alt="" src="../../_images/t5.gif" /></p>
<p>“Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer” <span id="id1">[<a class="reference internal" href="../../about/index.html#id16" title="Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, and others. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.">Raffel <em>et al.</em>, 2020</a>]</span></p>
<ul class="simple">
<li><p>A unified framework that converts all text-based language problems into a text-to-text format by framing them as conditional text generation tasks.</p></li>
<li><p>Combining the pre-training objectives of BERT and GPT-2, T5 is trained on a very large number of tasks and is able to perform well on a wide range of tasks with minimal task-specific architecture modifications.</p></li>
<li><p>C4 (Corpus of Cleaned Web Crawled Text) is used as the training corpus, which is a large-scale dataset of 3.3 billion web pages.</p></li>
<li><p>Achieves state-of-the-art results on 11 out of 15 tasks in GLUE, SuperGLUE, and SQuAD v1.1.</p></li>
</ul>
<section id="t5-text-to-text-framework">
<h2>T5: Text-to-Text Framework<a class="headerlink" href="#t5-text-to-text-framework" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../../_images/t5-training.png" /></p>
<section id="unified-input-output-format">
<h3>Unified Input &amp; Output Format<a class="headerlink" href="#unified-input-output-format" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>T5 means “<code class="docutils literal notranslate"><span class="pre">T</span></code>ext-<code class="docutils literal notranslate"><span class="pre">T</span></code>o-<code class="docutils literal notranslate"><span class="pre">T</span></code>ext <code class="docutils literal notranslate"><span class="pre">T</span></code>ransfer <code class="docutils literal notranslate"><span class="pre">T</span></code>ransformer”.</p></li>
<li><p>Every task considered by T5 is framed as a conditional text generation task with a single input and output sequence.</p></li>
<li><p>Translation: <code class="docutils literal notranslate"><span class="pre">translate</span> <span class="pre">English</span> <span class="pre">to</span> <span class="pre">German:</span> <span class="pre">How</span> <span class="pre">old</span> <span class="pre">are</span> <span class="pre">you?</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">Wie</span> <span class="pre">alt</span> <span class="pre">bist</span> <span class="pre">du?</span></code></p></li>
<li><p>Text classification: <code class="docutils literal notranslate"><span class="pre">classify</span> <span class="pre">sentiment:</span> <span class="pre">This</span> <span class="pre">movie</span> <span class="pre">is</span> <span class="pre">so</span> <span class="pre">bad.</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">negative</span></code></p></li>
<li><p>Text summarization: <code class="docutils literal notranslate"><span class="pre">summarize:</span> <span class="pre">The</span> <span class="pre">movie</span> <span class="pre">was</span> <span class="pre">not</span> <span class="pre">good.</span> <span class="pre">The</span> <span class="pre">animation</span> <span class="pre">and</span> <span class="pre">the</span> <span class="pre">graphics</span> <span class="pre">were</span> <span class="pre">good.</span> <span class="pre">This</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">good</span> <span class="pre">movie.</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">movie</span> <span class="pre">was</span> <span class="pre">not</span> <span class="pre">good.</span></code></p></li>
<li><p>MNLI (entailment): <code class="docutils literal notranslate"><span class="pre">mnli:</span> <span class="pre">Premise:</span> <span class="pre">A</span> <span class="pre">person</span> <span class="pre">on</span> <span class="pre">a</span> <span class="pre">horse</span> <span class="pre">jumps</span> <span class="pre">over</span> <span class="pre">a</span> <span class="pre">broken</span> <span class="pre">down</span> <span class="pre">airplane.</span> <span class="pre">Hypothesis:</span> <span class="pre">The</span> <span class="pre">person</span> <span class="pre">is</span> <span class="pre">training</span> <span class="pre">his</span> <span class="pre">horse</span> <span class="pre">for</span> <span class="pre">a</span> <span class="pre">competition.</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">entailment</span></code></p></li>
<li><p>MNLI (neutral): <code class="docutils literal notranslate"><span class="pre">mnli:</span> <span class="pre">Premise:</span> <span class="pre">A</span> <span class="pre">person</span> <span class="pre">on</span> <span class="pre">a</span> <span class="pre">horse</span> <span class="pre">jumps</span> <span class="pre">over</span> <span class="pre">a</span> <span class="pre">broken</span> <span class="pre">down</span> <span class="pre">airplane.</span> <span class="pre">Hypothesis:</span> <span class="pre">The</span> <span class="pre">person</span> <span class="pre">is</span> <span class="pre">at</span> <span class="pre">the</span> <span class="pre">zoo,</span> <span class="pre">riding</span> <span class="pre">a</span> <span class="pre">horse.</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">neutral</span></code></p></li>
<li><p>Regression: <code class="docutils literal notranslate"><span class="pre">sts-b:</span> <span class="pre">The</span> <span class="pre">cat</span> <span class="pre">was</span> <span class="pre">playing</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">garden.</span> <span class="pre">The</span> <span class="pre">cat</span> <span class="pre">was</span> <span class="pre">playing</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">yard.</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">5.0</span></code></p></li>
</ul>
</section>
</section>
<section id="c4-colossal-clean-crawled-corpus">
<h2>C4: Colossal Clean Crawled Corpus<a class="headerlink" href="#c4-colossal-clean-crawled-corpus" title="Permalink to this headline">#</a></h2>
<section id="common-crawl">
<h3><a class="reference external" href="https://commoncrawl.org/">Common Crawl</a><a class="headerlink" href="#common-crawl" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Common Crawl is a web crawl data repository that contains petabytes of data collected from the public web.</p></li>
<li><p>It is a non-profit organization that collects data from the public web and makes it freely available to the research community.</p></li>
<li><p>It produces around 30TB of data per month.</p></li>
<li><p>However, Common Crawl contains large amounts of noisy data, such as error messages, spam, and duplicate content.</p></li>
<li><p>To address this problem, the authors of T5 use the C4 dataset, which is a cleaned version of Common Crawl.</p></li>
</ul>
</section>
<section id="colossal-clean-crawled-corpus">
<h3>Colossal Clean Crawled Corpus<a class="headerlink" href="#colossal-clean-crawled-corpus" title="Permalink to this headline">#</a></h3>
<p>For C4, the authors took Common Crawl scrape from April 2019 and applied some cleansing filters on it:</p>
<ol class="simple">
<li><p>Only retained lines that ended in a terminal punctuation mark (i.e. a period, exclamation mark, question mark, or end quotation mark).</p></li>
<li><p>Discarded any page with fewer than 5 sentences and only retained lines that contained at least 3 words.</p></li>
<li><p>Removed any page that contained any word on the <code class="docutils literal notranslate"><span class="pre">List</span> <span class="pre">of</span> <span class="pre">Dirty,</span> <span class="pre">Naughty,</span> <span class="pre">Obscene</span> <span class="pre">or</span> <span class="pre">Otherwise</span> <span class="pre">Bad</span> <span class="pre">Words</span></code>.</p></li>
<li><p>Removed any line with the word <code class="docutils literal notranslate"><span class="pre">Javascript</span></code>.</p></li>
<li><p>Removed any page where the phrase <code class="docutils literal notranslate"><span class="pre">lorem</span> <span class="pre">ipsum</span></code> appeared.</p></li>
<li><p>Removed any pages that contained a curly bracket.</p></li>
<li><p>Deduplicate the data set, discarded all but one of any three-sentence span occurring more than once in the data set.</p></li>
<li><p>Since most of the downstream tasks are focused on English-language text, langdetect is used to filter out any pages that were not classified as English with a probability of at least 0.99.</p></li>
</ol>
<ul class="simple">
<li><p>The filtered dataset is larger than most data sets used for pre-training (about 750 GB).</p></li>
<li><p>It also comprises reasonably clean and natural English text.</p></li>
<li><p>This data set is called the “Colossal Clean Crawled Corpus” (or C4 for short) and released as part of TensorFlow Datasets.</p></li>
</ul>
</section>
</section>
<section id="the-model">
<h2>The Model<a class="headerlink" href="#the-model" title="Permalink to this headline">#</a></h2>
<section id="encoder-decoder-transformer-model">
<h3>Encoder-Decoder Transformer Model<a class="headerlink" href="#encoder-decoder-transformer-model" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>T5 uses the same encoder-decoder Transformer architecture as BERT.</p></li>
<li><p>However, a simplified layer normalization is used the activations are only rescaled and no additive bias is applied.</p></li>
<li><p>After layer normalization, a residual skip connection, originated from ResNet, adds each subcomponent’s input to its output.</p></li>
<li><p>Also, instead of using a fixed positional encoding, T5 uses a relative positional encoding.</p></li>
</ul>
</section>
<section id="baseline">
<h3>Baseline<a class="headerlink" href="#baseline" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The baseline model is designed so that the encoder and decoder are each similar in size and configuration to a BERT-base model.</p></li>
<li><p>Specifically, the encoder and decoder each have 12 layers, with about 220 million parameters.</p></li>
<li><p>The objective of the baseline model is to predict the dropped-out tokens in the input sequence.</p></li>
</ul>
<p><img alt="T5 Baseline" src="../../_images/t5-baseline.png" /></p>
</section>
</section>
<section id="a-systematic-study-of-transfer-learning-methodology">
<h2>A Systematic Study of Transfer Learning Methodology<a class="headerlink" href="#a-systematic-study-of-transfer-learning-methodology" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../../_images/t5-strategies.png" /></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">architectures</span></code>, where we found that encoder-decoder models generally outperformed “decoder-only” language models;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pre-training</span> <span class="pre">objectives</span></code>, where we confirmed that fill-in-the-blank-style denoising objectives (where the model is trained to recover missing words in the input) worked best and that the most important factor was the computational cost;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">unlabeled</span> <span class="pre">datasets</span></code>, where we showed that training on in-domain data can be beneficial but that pre-training on smaller datasets can lead to detrimental overfitting;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">training</span> <span class="pre">strategies</span></code>, where we found that multitask learning could be close to competitive with a pre-train-then-fine-tune approach but requires carefully choosing how often the model is trained on each task;</p></li>
<li><p>and <code class="docutils literal notranslate"><span class="pre">scale</span></code>, where we compare scaling up the model size, the training time, and the number of ensembled models to determine how to make the best use of fixed compute power.</p></li>
</ul>
<section id="architectures">
<h3>Architectures<a class="headerlink" href="#architectures" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../../_images/t5-structure.png" /></p>
<p>At an architectural level, there are several options in selecting the training approach:</p>
<ol class="simple">
<li><p>Encoder-Decoder:</p>
<ul class="simple">
<li><p>This is the standard encoder-decoder Transformer architecture.</p></li>
<li><p>Encoder is trained a BERT-style, fully visible language modeling objective. (i.e. every token contributes to the attention calculation of every other token)</p></li>
<li><p>Decoder is trained in a GPT-style, causal language modeling objective. (i.e. every token contributes to the attention calculation of every token that appears before it)</p></li>
</ul>
</li>
<li><p>Language Model: This is essentially the causal attention language modeling objective.</p></li>
<li><p>Prefix Language Model: This is a combination of the BERT-style and language model approaches. For example, the taks of translating from English to German is framed as a prefix language model task, where the input <code class="docutils literal notranslate"><span class="pre">translate</span> <span class="pre">English</span> <span class="pre">to</span> <span class="pre">German:</span></code> attended in BERT-style, and the output <code class="docutils literal notranslate"><span class="pre">Wie</span> <span class="pre">alt</span> <span class="pre">bist</span> <span class="pre">du?</span></code> is attended autoregressively.</p></li>
</ol>
<p><img alt="" src="../../_images/t5-mask-patterns.png" /></p>
<p><img alt="" src="../../_images/t5-architectures.png" /></p>
</section>
<section id="unsupervised-objective">
<h3>Unsupervised Objective<a class="headerlink" href="#unsupervised-objective" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../../_images/t5-unsupervised.png" /></p>
<p>With respect to the pre-training objective, authors have tested the following:</p>
<ol class="simple">
<li><p>Language Modeling: This is the standard language modeling objective, where the model is trained to predict the next token in a sequence.</p></li>
<li><p>Deshuffling: This is a variant of language modeling, where the model is trained to predict the original order of a shuffled sequence.</p></li>
<li><p>Corrupting Spans: Masking a sequence of tokens and training the model to predict these masked tokens.</p></li>
</ol>
<p><img alt="" src="../../_images/t5-unsupervised-obj.png" /></p>
<p>The BERT-style objective performs best, though the prefix language modeling objective attains similar performance on the translation tasks.</p>
<p><img alt="" src="../../_images/t5-unsupervised-obj-perf1.png" /></p>
<p><img alt="" src="../../_images/t5-unsupervised-obj-perf2.png" /></p>
</section>
<section id="corruption-rates">
<h3>Corruption Rates<a class="headerlink" href="#corruption-rates" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Using a larger corruption rate results in longer targets, which can potentially slow down training.</p></li>
<li><p>Based on the results and the historical precedent set by BERT, a corruption rate of 15% is used going forward.</p></li>
</ul>
<p><img alt="" src="../../_images/t5-corruption-rates.png" /></p>
</section>
<section id="corrupted-span-length">
<h3>Corrupted Span Length<a class="headerlink" href="#corrupted-span-length" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>When multiple consecutive tokens have been corrupted, they are treated as a “span” and a single unique mask token is used to replace the entire span.</p></li>
<li><p>A limited difference is found between these objectives, though the version with an average span length of 10 slightly underperforms the other values in some cases.</p></li>
</ul>
<p><img alt="" src="../../_images/t5-corrupted-span-length.png" /></p>
</section>
<section id="unlabeled-datasets">
<h3>Unlabeled Datasets<a class="headerlink" href="#unlabeled-datasets" title="Permalink to this headline">#</a></h3>
<p>Different pretraining datasets are tried.</p>
<ul class="simple">
<li><p>C4: The one mentioned in Section 2 in this story article.</p></li>
<li><p>C4, unfiltered: C4 but without filtering, to know the effect of the heuristic filtering</p></li>
<li><p>RealNews-like: C4 but only include content from one of the domains used in the “RealNews” data set.</p></li>
<li><p>WebText-like: Similarly, the WebText data set only uses content from webpages.</p></li>
<li><p>Wikipedia: English Wikipedia text data from TensorFlow Datasets.</p></li>
<li><p>Wikipedia+TBC: Toronto Books Corpus (TBC) contains text extracted from eBooks, combining with Wikipedia following BERT.</p></li>
</ul>
<p>Pre-training on in-domain unlabeled data can improve performance on downstream tasks. (e.g.: unlabled news data helps downstream news dataset.) But this is unsatisfying if the goal is to pre-train a model that can rapidly adapt to language tasks from arbitrary domains.</p>
<p><img alt="" src="../../_images/t5-unlabeled-datasets.png" /></p>
</section>
<section id="unlabeled-dataset-sizes">
<h3>Unlabeled Dataset Sizes<a class="headerlink" href="#unlabeled-dataset-sizes" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>C4 has <span class="math notranslate nohighlight">\(2^{35}\)</span>= 34B tokens.</p></li>
<li><p>Truncated variants of C4 consisting of <span class="math notranslate nohighlight">\(2^{29}\)</span>, <span class="math notranslate nohighlight">\(2^{27}\)</span>, <span class="math notranslate nohighlight">\(2^{25}\)</span> and <span class="math notranslate nohighlight">\(2^{23}\)</span> tokens.</p></li>
<li><p>These sizes correspond to repeating the data set 64, 256, 1,024, and 4,096 times respectively over the course of pre-training.</p></li>
<li><p>As expected, performance degrades as the data set size shrinks.</p></li>
</ul>
<p><img alt="" src="../../_images/t5-unlabeled-dataset-sizes.png" /></p>
</section>
<section id="fine-tuning-methods">
<h3>Fine-Tuning Methods<a class="headerlink" href="#fine-tuning-methods" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Fine-tuning all parameters.</p></li>
<li><p>Adapter Layers: keeping most of the original model fixed while fine-tuning. Adapter layers are additional dense-ReLU-dense blocks that are added after each of the preexisting feed-forward networks in each block of the Transformer.</p></li>
<li><p>Gradual Freezing: More and more of the model’s parameters are finetuned over time. At the start of fine-tuning, only the parameters of the final layer are updated, then after training for a certain number of updates the parameters of the second-to-last layer are also included, and so on until the entire network’s parameters are being fine-tuned.</p></li>
</ul>
<p>It is found that gradual unfreezing caused a minor degradation in performance across all tasks, though it did provide some speedup during fine-tuning.</p>
<p><img alt="" src="../../_images/t5-fine-tuning-methods.png" /></p>
</section>
<section id="multi-task-learning">
<h3>Multi-Task Learning<a class="headerlink" href="#multi-task-learning" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Multi-task learning is to train the model on multiple tasks at a time.</p></li>
<li><p>Multi-task learning underperforms pre-training followed by fine-tuning on most tasks.</p></li>
</ul>
<p><img alt="" src="../../_images/t5-multi-task-learning.png" /></p>
</section>
<section id="scaling">
<h3>Scaling<a class="headerlink" href="#scaling" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>There are a variety of possible ways to scale, including using a bigger model, training the model for more steps, and ensembling.</p></li>
<li><p>There is no large difference between training a 2x bigger model for 2x as long and training a 4x bigger model on any of the tasks.</p></li>
<li><p>This suggests that increasing the training time and increasing the model size can be complementary means of improving performance.</p></li>
<li><p>The results also suggest that ensembling provides an orthogonal and effective means of improving performance through scale.</p></li>
</ul>
<p><img alt="" src="../../_images/t5-scaling.png" /></p>
</section>
<section id="sota-comparisons">
<h3>SOTA Comparisons<a class="headerlink" href="#sota-comparisons" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Small, Base, Large, 3B, and 11B refer to model configurations with 60 million, 220 million, 770 million, 3 billion, and 11 billion parameters, respectively. (by tuning different hyperparameters.)</p></li>
<li><p>Overall, state-of-the-art performance is achieved on 18 out of the 24 tasks.</p></li>
<li><p>T5–3B model variant did beat the previous state of the art in a few tasks, but scaling the model size to 11 billion parameters was the most important ingredient for achieving the best performance.</p></li>
<li><p>For SQuAD, T5 outperformed the previous state-of-the-art ALBERT by over one point on the Exact Match score.</p></li>
<li><p>For SuperGLUE, T5 improved upon the state-of-the-art RoBERTa by a large margin from an average score of 84.6 to 88.9.</p></li>
</ul>
<p><img alt="" src="../../_images/t5-sota-comparisons.png" /></p>
<p>Further experiment is performed on three configurations as above:</p>
<ol class="simple">
<li><p>The standard baseline model, which was pre-trained on 2²⁵=34B tokens.</p></li>
<li><p>Baseline-1T: The baseline trained instead for about 1 trillion tokens (i.e. the same amount of pre-training used for T5).</p></li>
<li><p>T5-Base.</p></li>
</ol>
<p>T5-Base performs substantially better than Baseline-1T, suggesting that scale is not the only factor that contributes to T5’s success.</p>
<p><img alt="" src="../../_images/t5-sota-comparisons-2.png" /></p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/t5">T5 by huggingface</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/nlp_deep"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="tokenization.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Tokenization</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="transformers.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Transformers </div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">T5: Text-To-Text Transfer Transformer</a><ul>
<li><a class="reference internal" href="#t5-text-to-text-framework">T5: Text-to-Text Framework</a><ul>
<li><a class="reference internal" href="#unified-input-output-format">Unified Input &amp; Output Format</a></li>
</ul>
</li>
<li><a class="reference internal" href="#c4-colossal-clean-crawled-corpus">C4: Colossal Clean Crawled Corpus</a><ul>
<li><a class="reference internal" href="#common-crawl">Common Crawl</a></li>
<li><a class="reference internal" href="#colossal-clean-crawled-corpus">Colossal Clean Crawled Corpus</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-model">The Model</a><ul>
<li><a class="reference internal" href="#encoder-decoder-transformer-model">Encoder-Decoder Transformer Model</a></li>
<li><a class="reference internal" href="#baseline">Baseline</a></li>
</ul>
</li>
<li><a class="reference internal" href="#a-systematic-study-of-transfer-learning-methodology">A Systematic Study of Transfer Learning Methodology</a><ul>
<li><a class="reference internal" href="#architectures">Architectures</a></li>
<li><a class="reference internal" href="#unsupervised-objective">Unsupervised Objective</a></li>
<li><a class="reference internal" href="#corruption-rates">Corruption Rates</a></li>
<li><a class="reference internal" href="#corrupted-span-length">Corrupted Span Length</a></li>
<li><a class="reference internal" href="#unlabeled-datasets">Unlabeled Datasets</a></li>
<li><a class="reference internal" href="#unlabeled-dataset-sizes">Unlabeled Dataset Sizes</a></li>
<li><a class="reference internal" href="#fine-tuning-methods">Fine-Tuning Methods</a></li>
<li><a class="reference internal" href="#multi-task-learning">Multi-Task Learning</a></li>
<li><a class="reference internal" href="#scaling">Scaling</a></li>
<li><a class="reference internal" href="#sota-comparisons">SOTA Comparisons</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/js/hoverxref.js"></script>
    <script src="../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script src="../../_static/scripts/furo.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>