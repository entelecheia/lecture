<!doctype html>
<html class="no-js">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" /><link rel="next" title="Pretrained Language Models" href="plms.html" /><link rel="prev" title="SentencePiece Tokenizer" href="sentencepiece.html" />

    <meta name="generator" content="sphinx-4.5.0, furo 2022.09.29"/>
        <title>ByT5: Towards a token-free future with pre-trained byte-to-byte models - Artificial Intelligence | Lecture Notes</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=d81277517bee4d6b0349d71bb2661d4890b5617c" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">Artificial Intelligence | Lecture Notes</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  
  
  <span class="sidebar-brand-text">Artificial Intelligence | Lecture Notes</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_intro/index.html">Introduction to NLP</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/research.html">Research Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/language_models.html">Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic.html">Topic Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic_models.html">Topic Models </a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic_coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/sentiments.html">Sentiment Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/word_segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/vectorization.html">Vector Semantics and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/word_embeddings.html">Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab2-corpus-eda.html">Lab 2: EDA on Corpora</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Deep Learning for NLP</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="zeroshot.html">Zero Shot, Prompt, and Search Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers.html">Transformers </a></li>
<li class="toctree-l2"><a class="reference internal" href="t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
<li class="toctree-l2"><a class="reference internal" href="plms.html">Pretrained Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab3-train-tokenizers.html">Lab 3: Training Tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab4-pretraining-lms.html">Lab 4: Pretraining Language Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../nlp_apps/index.html">NLP Applications</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../aiart/index.html">AI Art (Text-to-Image)</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../aiart/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/project-themes.html">Project Themes - A Brave New World</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/imagen.html">Imagen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/textual-inversion.html">Textual Inversion (Dreambooth)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/whisper.html">Automatic Speech Recognition (Whisper)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/text2music.html">Text to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/image2music.html">Image to Music</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../mlops/dev-env.html">Development Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/dotfiles.html">Dotfiles</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ds/index.html">Data Science for Economics and Finance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/index.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/cite.html">Citation</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section class="tex2jax_ignore mathjax_ignore" id="byt5-towards-a-token-free-future-with-pre-trained-byte-to-byte-models">
<h1>ByT5: Towards a token-free future with pre-trained byte-to-byte models<a class="headerlink" href="#byt5-towards-a-token-free-future-with-pre-trained-byte-to-byte-models" title="Permalink to this headline">#</a></h1>
<p><img alt="" src="../../_images/byt5-intro.png" /></p>
<p>The ByT5 model was presented in <span id="id1">[<a class="reference internal" href="../../about/index.html#id22" title="Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. Byt5: towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291–306, 2022. URL: https://arxiv.org/pdf/2105.13626v1.pdf.">Xue <em>et al.</em>, 2022</a>]</span>.</p>
<p>The abstract from the paper is the following:</p>
<blockquote>
<div><p>Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.</p>
</div></blockquote>
<section id="what-is-a-token-in-machine-learning">
<h2>What Is A Token In Machine Learning?<a class="headerlink" href="#what-is-a-token-in-machine-learning" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>A token is a sequence of characters that is considered as a single entity during processing.</p></li>
<li><p>Tokens are usually derived from words, but they can also be derived from subwords, characters, or even bytes.</p></li>
<li><p>For example, the sentence “The quick brown fox jumps over the lazy dog” can be tokenized into the following tokens: [“The”, “quick”, “brown”, “fox”, “jumps”, “over”, “the”, “lazy”, “dog”].</p></li>
<li><p>Some words can be tokenized into multiple tokens, for example, “don’t” can be tokenized into [“do”, “n’t”].</p></li>
<li><p>At the byte or chracter level, the sentence can be tokenized into the 43 character tokens.</p></li>
<li><p>In transformer models, tokens are usually represented as vectors of fixed length, for example, 512-dimensional vectors to limit the cost of computation.</p></li>
<li><p>Attention mechanisms are expensive, and the cost of computation increases with the order of <span class="math notranslate nohighlight">\(N^2\)</span> where <span class="math notranslate nohighlight">\(N\)</span> is the number of tokens in the sequence.</p></li>
<li><p>This explains why tokenization is important, it dramatically reduces the number of tokens in a sequence, and thus the cost of computation.</p></li>
</ul>
</section>
<section id="what-is-byt5">
<h2>What Is ByT5?<a class="headerlink" href="#what-is-byt5" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>ByT5 is a token-free model that operates directly on raw text (bytes or characters).</p></li>
<li><p>Therefore, it does not require a tokenizer, and it can process text in any language out of the box.</p></li>
<li><p>One advantage of token-free models is that they are more robust to noise.</p></li>
<li><p>Also, out-of-vocabulary words are not a problem for token-free models. It is <code class="docutils literal notranslate"><span class="pre">&lt;UNK&gt;</span></code>-free.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;UNK&gt;</span></code> is the token used to represent out-of-vocabulary words in token-based models.</p></li>
<li><p>The proposed ByT5 is based on Google’s recent token-based mT5 (Massively Multilingual Text-to-Text Transfer Transformer)</p></li>
</ul>
<p><img alt="" src="../../_images/byt5-vs-mt5.png" /></p>
</section>
<section id="key-changes-to-the-mt5-architecture">
<h2>Key Changes To the mT5 Architecture<a class="headerlink" href="#key-changes-to-the-mt5-architecture" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Feeding UTF-8 bytes of the SentencePiece vocabulary directly to the embedding layer without any text preprocessing.</p></li>
<li><p>An additional 3 IDs are reserved for the special tokens: padding, end-of-sequence, and an unused token.</p></li>
<li><p>The team then modifies the pretrained task such that instead of adding 100 new tokens for the sentinels, they reuse the final 100 byte IDs.</p></li>
<li><p>Rather than using an average span length of 3 subwords, they mask longer byte-spans with a mean length of 20 bytes.</p></li>
<li><p>They found that byte-level models with a heavier encoder perform better on both classification and generation tasks.</p></li>
<li><p>They dropped any illegal bytes in the model’s ouput to keep the output valid UTF-8.</p></li>
</ul>
</section>
<section id="how-tokens-are-used">
<h2>How Tokens Are Used<a class="headerlink" href="#how-tokens-are-used" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>The mT5 tokens are much shorter than the ByT5 tokens because every character is a token in ByT5.</p></li>
<li><p>The model is designed to remove <span class="math notranslate nohighlight">\(n\)</span> tokens from the input and replace with placeholder tokens.</p></li>
<li><p>The encoder is given the content with the sets of tokens missing and the placeholder to understand where the tokens are missing.</p></li>
<li><p>The decoder is given the missing tokens and the placeholder assigned to the content that is hidden to it.</p></li>
<li><p>Essentially, the encoder knows the content with the tokens missing and the decoder knows the missing tokens.</p></li>
<li><p>On training, accuracy is measured by how well the decoder can predict the missing tokens.</p></li>
<li><p>Accuracy of 80% means that the decoder can predict 80% of the missing tokens correctly.</p></li>
</ul>
<p><img alt="" src="../../_images/byt5-mc4.png" /></p>
</section>
<section id="the-technical-details">
<h2>The Technical Details<a class="headerlink" href="#the-technical-details" title="Permalink to this headline">#</a></h2>
<blockquote>
<div><p>“We release ByT5 in five sizes analogous to T5 and mT5 (Small, Base, Large, XL, XXL). We aim for ByT5 to cover the same use cases as mT5: it is a general-purpose pre-trained text-to-text model covering 100+ languages. We expect ByT5 will be particular useful for tasks operating on short-to-medium length text sequences (a few sentences or less), as these will incur less slowdown in fine-tuning and inference.”</p>
</div></blockquote>
<ul class="simple">
<li><p>There are five sizes of ByT5 models covering 100+ languages.</p></li>
<li><p>The prediction is that ByT5 will be better for tasks operating on shorter text sequences.</p></li>
<li><p>A marginally better model that takes 100x more time to train and run is not a good trade-off.</p></li>
<li><p>ByT5 requires ~5x more tokens for the same text length compared to mT5.</p></li>
</ul>
<blockquote>
<div><p>“Second, we modify the pre-training task. mT5 uses the “span corruption” pre-training objective first proposed by Raffel et al. (2020) where spans of tokens in unlabeled text data are replaced with a single “sentinel” ID and the model must fill in the missing spans. Rather than adding 100 new tokens for the sentinels, we find it sufficient to reuse the final 100 byte IDs. While mT5 uses an average span length of 3 subword tokens, we find that masking longer byte-spans is valuable.”</p>
</div></blockquote>
<ul class="simple">
<li><p>mT5 replaces 3 tokens with a sentinel token.</p></li>
<li><p>ByT5 replaces 20 bytes with a sentinel token.</p></li>
</ul>
<blockquote>
<div><p>“Third, we find that ByT5 performs best when we decouple the depth of the encoder and decoder transformer stacks. While T5 and mT5 used “balanced” architectures, we find byte-level models benefit significantly from a “heavier” encoder. Specifically, we set our encoder depth to 3 times that of the decoder.”</p>
</div></blockquote>
<ul class="simple">
<li><p>mT5 used balances architectures with the same number of layers in the encoder and decoder.</p></li>
<li><p>ByT5 uses a heavier encoder with 3 times the number of layers as the decoder.</p></li>
</ul>
</section>
<section id="the-pros-and-cons-of-byt5">
<h2>The Pros And Cons Of ByT5<a class="headerlink" href="#the-pros-and-cons-of-byt5" title="Permalink to this headline">#</a></h2>
<section id="pros">
<h3>Pros<a class="headerlink" href="#pros" title="Permalink to this headline">#</a></h3>
<ul>
<li><p>In a model with a large vocabulary, the vocaboulary matrix can take up a substantial proportion of the model’s parameters.</p></li>
<li><p>Vocabulary of mT5 takes up 85% ~ 16% of the model’s parameters depending on the model size.</p></li>
<li><p>By switching to a byte-level model, the saved parameters can be used for other purposes, such as adding more layers or making the layers wider.</p>
<p><img alt="" src="../../_images/byt5-vocab.png" /></p>
</li>
</ul>
</section>
<section id="cons">
<h3>Cons<a class="headerlink" href="#cons" title="Permalink to this headline">#</a></h3>
<ul>
<li><p>Byte-level sequences are much longer than token-level sequences, and this increases the cost of computation.</p></li>
<li><p>If the decoder is particularly large, autoregressive sampling can be expensive.</p></li>
<li><p>In terms of FLOPs (floating-point operations per second), ByT5 requires ~1.2x more operations for the pre-training.</p></li>
<li><p>On word-level tasks, ByT5 is fairly competitive with mT5.</p></li>
<li><p>On tasks with longer input sequences, the slowdown is more pronounced.</p>
<p><img alt="" src="../../_images/byt5-flops-pretrain.png" /></p>
</li>
</ul>
</section>
</section>
<section id="the-results">
<h2>The Results<a class="headerlink" href="#the-results" title="Permalink to this headline">#</a></h2>
<blockquote>
<div><p>“ByT5 is competitive with mT5 on standard English and multilingual NLP benchmarks and outperforms mT5 at small model sizes. Additionally ByT5 excels on free-form generation tasks and transliteration.”</p>
</div></blockquote>
<section id="english-classification-tasks-glue-superglue">
<h3>English Classification Tasks (GLUE, SuperGLUE)<a class="headerlink" href="#english-classification-tasks-glue-superglue" title="Permalink to this headline">#</a></h3>
<ul>
<li><p>ByT5 outperforms mT5 on small and base model sizes by sizable margins, and loses close battles on larger models.</p>
<p><img alt="" src="../../_images/byt5-glue.png" /></p>
</li>
</ul>
</section>
<section id="english-generation-tasks-xsum-tweetqa-drop">
<h3>English Generation Tasks (XSum, TweetQA, DROP)<a class="headerlink" href="#english-generation-tasks-xsum-tweetqa-drop" title="Permalink to this headline">#</a></h3>
<ul>
<li><p>XSum gets the model to summarize a news article in a single sentence, and TweetQA is question answering from Tweets. DROP is a challenging reading comprehension task that requires numerical reasoning.</p></li>
<li><p>ByT5 outperforms mT5 on all three tasks across all model sizes.</p>
<p><img alt="" src="../../_images/byt5-gen.png" /></p>
</li>
</ul>
</section>
<section id="cross-lingual-benchmarks">
<h3>Cross-lingual Benchmarks<a class="headerlink" href="#cross-lingual-benchmarks" title="Permalink to this headline">#</a></h3>
<ul>
<li><p>Changes to vocabulary and tokenization are likely to affect different languages in different ways.</p></li>
<li><p>On the most realistic in-language setting, where some gold training data is available in all languages, ByT5 surpasses the previous state-of-art mT5 on all tasks and model sizes.</p></li>
<li><p>One might expect languages with rich inflectional morphology (e.g. Turkish) to benefit most from the move away from a fixed vocabulary.</p></li>
<li><p>Languages with a higher SentencePiece token compression rate (e.g. Thai and Telugu) tend to favor mT5, whereas those with a lower compression rate (e.g. Indonesian and Vietnamese) tend to favor ByT5.</p>
<p><img alt="" src="../../_images/byt5-xling.png" /></p>
</li>
</ul>
</section>
<section id="word-level-tasks">
<h3>Word-Level Tasks<a class="headerlink" href="#word-level-tasks" title="Permalink to this headline">#</a></h3>
<ul>
<li><p>Unsurprisingly, “characteraware” models excel on tasks around word-internal phenonema.</p></li>
<li><p>These core NLP tasks have often been overlooked in evaluating general-purpose NLP models.</p>
<p><img alt="" src="../../_images/byt5-word.png" /></p>
</li>
</ul>
</section>
</section>
<section id="experiments-on-synthetic-noise">
<h2>Experiments on Synthetic Noise<a class="headerlink" href="#experiments-on-synthetic-noise" title="Permalink to this headline">#</a></h2>
<p>The authors test six types of noise on the model:</p>
<ul class="simple">
<li><p><strong>Drop</strong>: Each character has a 10% chance of being dropped.</p></li>
<li><p><strong>Add/Drop/Mutate</strong>: At each character position, there is a 10% chance of applying one of three actions, with equal likelihood: Add (inserts a random character from the input), Drop (deletes this character) or Mutate (replaces this character with a random character from the input).</p></li>
<li><p><strong>Repetitions</strong>: Each character has a 20% chance of being selected for repetition. If selected, 1–3 repetitions (with equal likelihood) are appended after the original character.</p></li>
<li><p><strong>Antspeak</strong>: Each character is capitalized and padded with spaces. For example, “abc def” becomes “ A B C D E F ”.</p></li>
<li><p><strong>Uppercase</strong>: Each character is converted to uppercase. Here, we restrict to languages whose scripts distinguish case (for XNLI: Bulgarian, English, French, German, Greek, Russian, Spanish, Swahili, Turkish, Vietnamese; for TyDiQA-GoldP: English, Finnish, Indonesian, Russian, Swahili).</p></li>
<li><p><strong>Random case</strong>: Each character is set to a random case (upper or lower). Again, only languages whose scripts distinguish case are considered.</p></li>
</ul>
<section id="results">
<h3>Results<a class="headerlink" href="#results" title="Permalink to this headline">#</a></h3>
<ul>
<li><p>Character-level models are more robust to real and synthetic noise than BPE or word-based models, across a range of morphological, syntactic and semantic tagging tasks.</p></li>
<li><p>Token-free models are more robust to noise across many tasks.</p>
<p><img alt="" src="../../_images/byt5-noise.png" /></p>
</li>
</ul>
</section>
</section>
<section id="ablation-study">
<h2>Ablation Study<a class="headerlink" href="#ablation-study" title="Permalink to this headline">#</a></h2>
<blockquote>
<div><p>“ByT5 outperforms mT5 in any of these four scenarios: (1) at model sizes under 1 billion parameters, (2) on generative tasks, (3) on multilingual tasks with in-language labels, and (4) in the presence of various types of noise.”</p>
</div></blockquote>
<blockquote>
<div><p>“… the gains we observe with ByT5 are achieved despite the fact that the model is pretrained on 4 times less text than mT5. This suggests that byte-level models could be more data efficient learners.”</p>
</div></blockquote>
<blockquote>
<div><p>“Our “hands-off” approach of feeding raw UTF-8 bytes directly into the transformer costs +33% pre-training time, as well as longer inference time (10–50% longer for small models, and 100–600% longer for our largest models). As such, there is significant room for improvement. We believe techniques such as hash embeddings, local attention and down-sampling (Clark et al., 2021), as well as sparse computation (Fedus et al., 2021) can help address latency issues, removing the remaining barriers to a token-free future.”</p>
</div></blockquote>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>While ByT5 takes longer to train, and underperforms in some tasks (zero-shot translation for example) the payoff for tasks where noise is an issue (think social and voice) is significant.</p></li>
<li><p>Token-free models and token-based models can coexist, and the best model for a given task will be assigned based on the task’s characteristics.</p></li>
<li><p>Token-free models will likely dramatically enhance voice search technologies where noise is prevalent.</p></li>
<li><p>Token-free models could collect information from noisy environments and to more quickly learn new terms being used as shorthand, slang, or jargon, and even emojis.</p></li>
<li><p>These information could be used to improve the performance of token-based models.</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/2105.13626v1.pdf">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></p></li>
<li><p><a class="reference external" href="https://wandb.ai/onlineinference/byt5/reports/ByT5-What-It-Might-Mean-For-SEO--Vmlldzo4NzY1NzE#what-is-byt5?">ByT5: What It Might Mean For SEO</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/nlp_deep"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="plms.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Pretrained Language Models</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="sentencepiece.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">SentencePiece Tokenizer</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a><ul>
<li><a class="reference internal" href="#what-is-a-token-in-machine-learning">What Is A Token In Machine Learning?</a></li>
<li><a class="reference internal" href="#what-is-byt5">What Is ByT5?</a></li>
<li><a class="reference internal" href="#key-changes-to-the-mt5-architecture">Key Changes To the mT5 Architecture</a></li>
<li><a class="reference internal" href="#how-tokens-are-used">How Tokens Are Used</a></li>
<li><a class="reference internal" href="#the-technical-details">The Technical Details</a></li>
<li><a class="reference internal" href="#the-pros-and-cons-of-byt5">The Pros And Cons Of ByT5</a><ul>
<li><a class="reference internal" href="#pros">Pros</a></li>
<li><a class="reference internal" href="#cons">Cons</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-results">The Results</a><ul>
<li><a class="reference internal" href="#english-classification-tasks-glue-superglue">English Classification Tasks (GLUE, SuperGLUE)</a></li>
<li><a class="reference internal" href="#english-generation-tasks-xsum-tweetqa-drop">English Generation Tasks (XSum, TweetQA, DROP)</a></li>
<li><a class="reference internal" href="#cross-lingual-benchmarks">Cross-lingual Benchmarks</a></li>
<li><a class="reference internal" href="#word-level-tasks">Word-Level Tasks</a></li>
</ul>
</li>
<li><a class="reference internal" href="#experiments-on-synthetic-noise">Experiments on Synthetic Noise</a><ul>
<li><a class="reference internal" href="#results">Results</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ablation-study">Ablation Study</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/js/hoverxref.js"></script>
    <script src="../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script src="../../_static/scripts/furo.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>