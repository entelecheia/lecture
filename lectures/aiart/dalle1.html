

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>DALL·E 1 &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/js/hoverxref.js"></script>
    <script src="../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/chatgpt.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/aiart/dalle1';</script>
    <link rel="canonical" href="https://lecture.entelecheia.ai/lectures/aiart/dalle1.html" />
    <link rel="shortcut icon" href="../../_static/favicon-v2-circle.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="DALL·E 2" href="dalle2.html" />
    <link rel="prev" title="Art and Music in Light of AI" href="aiart.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content"><p>
  <strong>Announcement:</strong>
  You can install the accompanying AI tutor <a href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd" target="_blank">here</a>.
  <a class="reference external" href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd" target="_blank"><img alt="chrome-web-store-image" src="https://img.shields.io/chrome-web-store/v/lfgfgbomindbccgidgalhhndggddpagd" /></a>
</p>
</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_intro/index.html">Introduction to NLP</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/research.html">Research Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/language_models.html">Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/datasets/index.html">Datasets for NLP</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../nlp_intro/topic/index.html">Topic Modeling</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../nlp_intro/topic/coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp_intro/topic/coherence-practice.html">Topic Coherence in Practice</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp_intro/topic/tomotopy.html">Topic Modeling Tools - Tomotopy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/sentiments.html">Sentiment Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/word_segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/vectorization.html">Vector Semantics and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/word_embeddings.html">Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab2-corpus-eda.html">Lab 2: EDA on Corpora</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_deep/index.html">Deep Learning for NLP</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/intro.html">Introduction</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../nlp_deep/llms/index.html">Large Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../nlp_deep/llms/zeroshot.html">Zero Shot and Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp_deep/llms/decoding.html">Decoding and Search Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp_deep/llms/plms.html">Pretrained Language Models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../nlp_deep/transformers/index.html">Transformers</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../nlp_deep/transformers/bert.html">BERT: Bidirectional Encoder Representations from Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp_deep/transformers/bertviz.html">BERT: Visualizing Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp_deep/transformers/t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp_deep/transformers/byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/datasets/index.html">Datasets for NLP</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../nlp_deep/tokenization/index.html">Tokenization</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../nlp_deep/tokenization/sentencepiece.html">SentencePiece Tokenizer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/detectGPT.html">How to Spot Machine-Written Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/lab3-train-tokenizers.html">Lab 3: Training Tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/lab4-pretraining-lms.html">Lab 4: Pretraining Language Models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_advances/index.html">Advances in AI and NLP</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/thesis.html">Writing a Thesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/detectGPT.html">DetectGPT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/gpt4.html">GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/camelids.html">Meet the Camelids: A Family of LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/sam/index.html">Segment Everything</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">AI Art (Generative AI)</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="aiart.html">Art and Music in Light of AI</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">DALL·E 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="dalle2.html">DALL·E 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="imagen.html">Imagen</a></li>
<li class="toctree-l2"><a class="reference internal" href="textual-inversion.html">Textual Inversion (Dreambooth)</a></li>
<li class="toctree-l2"><a class="reference internal" href="whisper.html">Automatic Speech Recognition (Whisper)</a></li>
<li class="toctree-l2"><a class="reference internal" href="text2music.html">Text to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="image2music.html">Image to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="robot_drawings.html">Robot Drawing Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="project-themes.html">Project Themes - A Brave New World</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/devops/index.html">DevOps</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/devops/gitops.html">GitOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/devops/devsecops.html">DevSecOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/devops/llmops.html">LLMOps - Large Language Model Operations</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/dotfiles/index.html">Dotfiles</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai">Dotfiles in Practice</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/dotfiles/dotdrop.html">Dotdrop: A Powerful Tool for Managing Dotfiles</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/security/index.html">Security Management</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/security/auth-enc-sign.html">Authentication, Encryption, and Signing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/security/age-gpg-ssh.html">SSH, GPG, and AGE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/security/pass.html">Unix Password Managers</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/github/index.html">Github’s Fork &amp; Pull Workflow</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/github/template.html">Project Templating Tools</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/containerization/index.html">Containerization - Docker and containerd</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/containerization/containerd.html"><code class="docutils literal notranslate"><span class="pre">containerd</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/simple-pipeline/index.html">Simple MLOps Pipeline</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ds/index.html">Data Science for Economics and Finance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../about/lecture-bot.html">LectureBot for ἐντελέχεια.άι</a></li>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/index.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/aiart/dalle1.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>DALL·E 1</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dalle-1-charateristics">DALL·E 1 Charateristics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#controlling-attributes">Controlling Attributes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#drawing-multiple-objects">Drawing Multiple Objects</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-perspective-and-three-dimensionality">Visualizing Perspective and Three-Dimensionality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-internal-and-external-structure">Visualizing Internal and External Structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inferring-contextual-details">Inferring Contextual Details</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dalle-1-architecture">DALL·E 1 Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vq-vae">VQ-VAE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoders">Autoencoders</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-autoencoders-vae">Variational Autoencoders (VAE)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-spaces">Discrete Spaces</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty-in-the-posterior">Uncertainty in the Posterior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-from-a-trained-dall-e">Sampling From a Trained DALL-E</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dalle-1-results">DALL·E 1 Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="dalle-1">
<h1>DALL·E 1<a class="headerlink" href="#dalle-1" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p>Paper: <a class="reference external" href="https://arxiv.org/abs/2102.12092">https://arxiv.org/abs/2102.12092</a></p></li>
<li><p>Blog post: <a class="reference external" href="https://openai.com/blog/dall-e/">https://openai.com/blog/dall-e/</a></p></li>
<li><p>Code: <a class="github reference external" href="https://github.com/openai/dall-e">openai/dall-e</a></p></li>
<li><p>Model: Not available</p></li>
<li><p>Alternative code (PyTorch): <a class="github reference external" href="https://github.com/lucidrains/DALLE-pytorch">lucidrains/DALLE-pytorch</a></p></li>
<li><p>Alternative code (JAX/Flax): <a class="github reference external" href="https://github.com/borisdayma/dalle-mini">borisdayma/dalle-mini</a></p></li>
</ul>
<p>The DALL·E 1 paper proposes a simple approach for text-to-image generation using a transformer model that autoregressively models text and image tokens as a single stream of data. Unlike traditional approaches that require complex architectures and auxiliary losses, DALL·E 1’s approach relies on data and scale to be competitive with previous domain-specific models in zero-shot evaluations. This approach eliminates the need for training on a fixed dataset and enables generating images from previously unseen text descriptions.</p>
<figure class="align-default" id="fig-dalle1">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1.png"><img alt="../../_images/aiart_1_dalle1.png" src="../../_images/aiart_1_dalle1.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 104 </span><span class="caption-text">DALL·E 1</span><a class="headerlink" href="#fig-dalle1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>DALL·E’s initial version is a transformer decoder that generates a 256×256 image from text input and an optional beginning of the image.</p></li>
<li><p>Text is encoded by BPE-tokens with a maximum of 256, while images are encoded by special image tokens with 1024 options, using a discrete variational autoencoder (dVAE).</p></li>
<li><p>The dVAE encodes 256×256 images into a grid of 32×32 tokens with an 8192 possible value vocabulary.</p></li>
<li><p>Generated images by DALL·E may have blurriness and smoothness due to the loss of high-frequency features and details resulting from the dVAE encoding process.</p></li>
</ul>
<section id="dalle-1-charateristics">
<h2>DALL·E 1 Charateristics<a class="headerlink" href="#dalle-1-charateristics" title="Permalink to this heading">#</a></h2>
<section id="controlling-attributes">
<h3>Controlling Attributes<a class="headerlink" href="#controlling-attributes" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Controlling attributes of DALL·E 1 refers to generating images with specific features according to textual input.</p></li>
<li><p>The model can control various attributes, such as shape, color, position, style, and composition of objects in the generated image.</p></li>
<li><p>To control attributes, the model uses prompts with flexible and open-ended descriptions.</p></li>
<li><p>During training, the model learns to associate textual input with specific visual attributes.</p></li>
<li><p>The conditioning mechanism enables the model to generalize to new attribute combinations not present in the training data.</p></li>
</ul>
<figure class="align-default" id="fig-dalle1-attributes">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_attributes.png"><img alt="../../_images/aiart_1_dalle1_attributes.png" src="../../_images/aiart_1_dalle1_attributes.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 105 </span><span class="caption-text">Controlling Attributes</span><a class="headerlink" href="#fig-dalle1-attributes" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="drawing-multiple-objects">
<h3>Drawing Multiple Objects<a class="headerlink" href="#drawing-multiple-objects" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>DALL·E faces a challenge of controlling multiple objects, their attributes, and spatial relationships simultaneously.</p></li>
<li><p>Variable binding is crucial for this task as it associates attributes with specific objects.</p></li>
<li><p>DALL·E’s ability to handle variable binding is tested with examples of relative positioning, stacking objects, and controlling multiple attributes.</p></li>
<li><p>DALL·E offers some level of controllability but the success rate is dependent on how the caption is phrased.</p></li>
<li><p>As more objects are introduced, DALL·E’s success rate sharply decreases.</p></li>
<li><p>DALL·E is found to be brittle with respect to rephrasing the caption, and alternative captions often yield no correct interpretations.</p></li>
</ul>
<figure class="align-default" id="fig-dalle1-objects">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_objects.png"><img alt="../../_images/aiart_1_dalle1_objects.png" src="../../_images/aiart_1_dalle1_objects.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 106 </span><span class="caption-text">Drawing Multiple Objects</span><a class="headerlink" href="#fig-dalle1-objects" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="visualizing-perspective-and-three-dimensionality">
<h3>Visualizing Perspective and Three-Dimensionality<a class="headerlink" href="#visualizing-perspective-and-three-dimensionality" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>DALL·E allows for control over the viewpoint of a scene and the 3D style in which a scene is rendered.</p></li>
<li><p>DALL·E’s ability to control viewpoint and 3D style is demonstrated with examples of an extreme close-up view of a capybara sitting in a field and a capybara made of voxels sitting in a field.</p></li>
<li><p>DALL·E’s ability to repeatedly draw the head of a well-known figure at each angle is tested and found to produce a smooth animation of the rotating head.</p></li>
<li><p>DALL·E can apply some types of optical distortions to scenes, as seen with the options “fisheye lens view” and “a spherical panorama.”</p></li>
<li><p>DALL·E’s ability to generate reflections is explored.</p></li>
</ul>
<figure class="align-default" id="fig-dalle1-perspective">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_perspective.png"><img alt="../../_images/aiart_1_dalle1_perspective.png" src="../../_images/aiart_1_dalle1_perspective.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 107 </span><span class="caption-text">Visualizing Perspective and Three-Dimensionality</span><a class="headerlink" href="#fig-dalle1-perspective" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="visualizing-internal-and-external-structure">
<h3>Visualizing Internal and External Structure<a class="headerlink" href="#visualizing-internal-and-external-structure" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>DALL·E’s ability to render internal and external structure is explored through cross-sectional views and macro photographs.</p></li>
<li><p>The samples from the “extreme close-up view” and “x-ray” style led to further exploration of DALL·E’s capabilities in rendering internal and external structures.</p></li>
<li><p>The cross-sectional views and macro photographs highlight DALL·E’s ability to generate images with fine details and textures.</p></li>
</ul>
<figure class="align-default" id="fig-dalle1-structure">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_structure.png"><img alt="../../_images/aiart_1_dalle1_structure.png" src="../../_images/aiart_1_dalle1_structure.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 108 </span><span class="caption-text">Visualizing Internal and External Structure</span><a class="headerlink" href="#fig-dalle1-structure" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="inferring-contextual-details">
<h3>Inferring Contextual Details<a class="headerlink" href="#inferring-contextual-details" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The task of translating text to images is underspecified, leading to multiple plausible images for a single caption.</p></li>
<li><p>DALL·E’s ability to resolve underspecification is explored in cases of changing style, setting, and time; drawing the same object in various situations; and generating an image of an object with specific text written on it.</p></li>
<li><p>DALL·E provides access to a subset of 3D rendering engine capabilities through natural language, controlling attributes of a small number of objects, their arrangement, and the location and angle of the scene.</p></li>
<li><p>DALL·E can “fill in the blanks” and generate certain details not explicitly stated in the caption, unlike a 3D rendering engine that requires unambiguous and detailed inputs.</p></li>
</ul>
<figure class="align-default" id="fig-dalle1-context">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_context.png"><img alt="../../_images/aiart_1_dalle1_context.png" src="../../_images/aiart_1_dalle1_context.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 109 </span><span class="caption-text">Inferring Contextual Details</span><a class="headerlink" href="#fig-dalle1-context" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="dalle-1-architecture">
<h2>DALL·E 1 Architecture<a class="headerlink" href="#dalle-1-architecture" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>The DALL·E 1 architecture includes a large transformer model with 12B parameters.</p></li>
<li><p>The model consists of 64 sparse transformer blocks with various attention mechanisms, including classical text-to-text masked attention, image-to-text attention, and image-to-image sparse attention.</p></li>
<li><p>All three attention types are merged into a single attention operation.</p></li>
<li><p>The model was trained on a dataset of 250M image-text pairs.</p></li>
</ul>
<section id="vq-vae">
<h3>VQ-VAE<a class="headerlink" href="#vq-vae" title="Permalink to this heading">#</a></h3>
<p>VQ-VAE is a type of machine learning model that can be used to learn compressed representations of images or other types of data. It works by first encoding the input data into a set of discrete codes, which are then used to reconstruct the original data. Unlike other types of machine learning models, which use continuous values for their internal representations, VQ-VAE uses discrete values, which can make it more efficient and easier to work with in certain situations.</p>
<figure class="align-default" id="fig-dalle1-vqvae">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_vqvae.png"><img alt="../../_images/aiart_1_dalle1_vqvae.png" src="../../_images/aiart_1_dalle1_vqvae.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 110 </span><span class="caption-text">VQ-VAE</span><a class="headerlink" href="#fig-dalle1-vqvae" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>VQ-VAE is a type of variational autoencoder that uses vector quantization to obtain a discrete latent representation.</p></li>
<li><p>This is in contrast to the continuous latent space that other variational autoencoders have.</p></li>
<li><p>The objective function of a VQ-VAE, when trained on an image dataset, can be written as:</p>
<p><span class="math notranslate nohighlight">\(\mathcal{L} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - \beta \cdot D_{KL}[q(z|x) || p(z)]\)</span></p>
<p>where <span class="math notranslate nohighlight">\(p(x)\)</span> is the data distribution, <span class="math notranslate nohighlight">\(q\)</span> is the approximate posterior over latent variables and <span class="math notranslate nohighlight">\(D_{KL}\)</span> denotes the Kullback-Leibler divergence.</p>
</li>
<li><p>This objective function encourages the model to learn an efficient codebook that minimises reconstruction error while also matching the prior distribution over codes.</p></li>
</ul>
<p><strong>A latent space</strong></p>
<p>A latent space is obtained by encoding the input image into the nearest codebook entry. This process is called vector quantization and results in a discrete latent space.</p>
<figure class="align-default" id="fig-dalle1-latent">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_latent.png"><img alt="../../_images/aiart_1_dalle1_latent.png" src="../../_images/aiart_1_dalle1_latent.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 111 </span><span class="caption-text">A latent space</span><a class="headerlink" href="#fig-dalle1-latent" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>A latent space is a simplified version of an input image created by a machine learning model.</p></li>
<li><p>The process of transforming the input image into the latent space is called encoding.</p></li>
<li><p>Vector quantization is a type of encoding that transforms the input image into the nearest codebook entry, resulting in a discrete latent space.</p></li>
<li><p>The latent space contains only the most important information from the input image, making it easier to work with and more efficient for the machine learning model.</p></li>
</ul>
</section>
<section id="autoencoders">
<h3>Autoencoders<a class="headerlink" href="#autoencoders" title="Permalink to this heading">#</a></h3>
<p>Autoencoder is a type of neural network that learns how to represent input data in a more compact way. It works by trying to predict its own input, so it learns to compress the input data into a smaller set of values that can later be used to reconstruct the original data.</p>
<figure class="align-default" id="fig-dalle1-autoencoder">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_autoencoder.png"><img alt="../../_images/aiart_1_dalle1_autoencoder.png" src="../../_images/aiart_1_dalle1_autoencoder.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 112 </span><span class="caption-text">Autoencoders</span><a class="headerlink" href="#fig-dalle1-autoencoder" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Autoencoder is a neural network that is trained to predict its input.</p></li>
<li><p>The objective function of an autoencoder can be written as:</p>
<p><span class="math notranslate nohighlight">\(\mathcal{L} = \mathbb{E}_{p(x)}[\log p(x|z)]\)</span></p>
<p>where <span class="math notranslate nohighlight">\(p(x)\)</span> is the data distribution and <span class="math notranslate nohighlight">\(p(x|z)\)</span> is the model distribution.</p>
</li>
<li><p>This objective function encourages the model to learn a latent space that captures the underlying structure of the data.</p></li>
<li><p>A VQ-VAE can be seen as a type of autoencoder where the latent space is constrained to be discrete.</p></li>
</ul>
<section id="variational-autoencoders-vae">
<h4>Variational Autoencoders (VAE)<a class="headerlink" href="#variational-autoencoders-vae" title="Permalink to this heading">#</a></h4>
<p>Variational Autoencoders (VAE) are a type of machine learning model that can be used to learn compressed representations of data. Unlike regular autoencoders, VAEs use a continuous latent space. The objective function of a VAE encourages the model to learn a latent space that captures the underlying structure of the data while also matching the prior distribution over latent variables. This makes it easier to work with and analyze the data. VAEs are particularly useful for generating new data that is similar to the input data, such as images or text.</p>
<figure class="align-default" id="fig-dalle1-vae">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_vae.png"><img alt="../../_images/aiart_1_dalle1_vae.png" src="../../_images/aiart_1_dalle1_vae.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 113 </span><span class="caption-text">Variational Autoencoders (VAE)</span><a class="headerlink" href="#fig-dalle1-vae" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Variational Autoencoders (VAE) is a type of autoencoder where the latent space is continuous.</p></li>
<li><p>The objective function of a VAE can be written as:</p>
<p><span class="math notranslate nohighlight">\(\mathcal{L} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}[q(z|x) || p(z)]\)</span></p>
<p>where <span class="math notranslate nohighlight">\(p(x)\)</span> is the data distribution, <span class="math notranslate nohighlight">\(q\)</span> is the approximate posterior over latent variables and <span class="math notranslate nohighlight">\(D_{KL}\)</span> denotes the Kullback-Leibler divergence.</p>
</li>
<li><p>This objective function encourages the model to learn a latent space that captures the underlying structure of the data while also matching the prior distribution over latent variables.</p></li>
</ul>
</section>
</section>
<section id="discrete-spaces">
<h3>Discrete Spaces<a class="headerlink" href="#discrete-spaces" title="Permalink to this heading">#</a></h3>
<p>Discrete spaces are a type of space that is more efficient to work with than continuous spaces. This is because a discrete space can be represented with a finite number of bits, while a continuous space requires an infinite number of bits. Discrete spaces are also easier to manipulate and reason about than continuous spaces. This is why VQ-VAE, which uses a discrete latent space, is more efficient than VAE at learning compressed representations of data. By using a finite set of discrete codes, VQ-VAE can learn compressed representations of data that are easier to work with and require less memory than VAE’s continuous representations.</p>
<figure class="align-default" id="fig-dalle1-discrete">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_discrete.png"><img alt="../../_images/aiart_1_dalle1_discrete.png" src="../../_images/aiart_1_dalle1_discrete.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 114 </span><span class="caption-text">Discrete Spaces</span><a class="headerlink" href="#fig-dalle1-discrete" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Discrete spaces are more efficient to represent than continuous spaces.</p></li>
<li><p>This is because a discrete space can be represented with a finite number of bits, whereas a continuous space requires an infinite number of bits.</p></li>
<li><p>In addition, discrete spaces are easier to manipulate and reason about than continuous spaces.</p></li>
<li><p>For these reasons, VQ-VAE is more efficient than VAE at learning latent representations of data.</p></li>
</ul>
</section>
<section id="uncertainty-in-the-posterior">
<h3>Uncertainty in the Posterior<a class="headerlink" href="#uncertainty-in-the-posterior" title="Permalink to this heading">#</a></h3>
<p>Uncertainty in the posterior arises due to soft-sampling codebook vectors from the Gumbel-Softmax distribution. This soft-sampling process creates a continuous approximation of the discrete latent space, which introduces some level of uncertainty or imprecision in the model’s predictions or estimated parameters.</p>
<figure class="align-default" id="fig-dalle1-uncetainty1">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_uncetainty1.png"><img alt="../../_images/aiart_1_dalle1_uncetainty1.png" src="../../_images/aiart_1_dalle1_uncetainty1.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 115 </span><span class="caption-text">Uncertainty in the Posterior</span><a class="headerlink" href="#fig-dalle1-uncetainty1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-dalle1-uncetainty2">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_uncetainty2.png"><img alt="../../_images/aiart_1_dalle1_uncetainty2.png" src="../../_images/aiart_1_dalle1_uncetainty2.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 116 </span><span class="caption-text">Uncertainty in the Posterior</span><a class="headerlink" href="#fig-dalle1-uncetainty2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>To understand uncertainty in the posterior easily, let’s break it down into simpler terms:</p>
<ul class="simple">
<li><p>Posterior: In the context of machine learning and probabilistic models, the posterior is the updated probability distribution of a model’s parameters after taking into account observed data.</p></li>
<li><p>Uncertainty: In this context, uncertainty refers to the lack of confidence or precision in the model’s predictions or estimated parameters.</p></li>
</ul>
<p>Now, let’s talk about the Gumbel-Softmax distribution and soft-sampling codebook vectors:</p>
<ul>
<li><p>Codebook vectors: In a VQ-VAE (Vector Quantized Variational AutoEncoder) model, codebook vectors are a set of fixed, learnable vectors that form a discrete latent space. The model maps its continuous latent representations to the nearest codebook vector during the training process.</p></li>
<li><p>Soft-sampling: Soft-sampling refers to the process of sampling codebook vectors in a “soft” manner, meaning that the samples are taken from a continuous approximation of the discrete latent space. This is done using the Gumbel-Softmax distribution.</p></li>
<li><p>Gumbel-Softmax distribution: The Gumbel-Softmax distribution is a technique that allows for sampling from a discrete space while maintaining gradients for backpropagation. It involves using a temperature parameter (<span class="math notranslate nohighlight">\(\beta\)</span>) that controls the “sharpness” of the distribution. Lower temperatures lead to a more focused distribution, while higher temperatures result in a more uniform distribution.</p>
<ul>
<li><p>The Gumbel-Softmax distribution is defined as:</p>
<p><span class="math notranslate nohighlight">\(G(z;\mu,\beta) = \frac{\exp((z - \mu)/\beta)}{\sum_{k=1}^K \exp((z_k - \mu)/\beta)}\)</span></p>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the mean, <span class="math notranslate nohighlight">\(\beta\)</span> is the temperature and <span class="math notranslate nohighlight">\(K\)</span> is the number of classes.</p>
</li>
</ul>
</li>
</ul>
<p>This is useful for training models with discrete latent spaces, such as VQ-VAE.</p>
<figure class="align-default" id="fig-dalle1-dvae">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_dvae.png"><img alt="../../_images/aiart_1_dalle1_dvae.png" src="../../_images/aiart_1_dalle1_dvae.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 117 </span><span class="caption-text">Comparison of original images (top) and reconstructions from the dVAE (bottom)</span><a class="headerlink" href="#fig-dalle1-dvae" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="decoder">
<h3>Decoder<a class="headerlink" href="#decoder" title="Permalink to this heading">#</a></h3>
<p>DALL-E’s decoder is a GPT-3-like transformer that takes a sequence of text tokens and optional image tokens as input, understands their relationships, and generates a visual representation of the input text by producing a continuation or completion of the image.</p>
<figure class="align-default" id="fig-dalle1-decoder">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_decoder.png"><img alt="../../_images/aiart_1_dalle1_decoder.png" src="../../_images/aiart_1_dalle1_decoder.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 118 </span><span class="caption-text">Decoder</span><a class="headerlink" href="#fig-dalle1-decoder" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The decoder in DALL-E is a crucial component responsible for generating images based on the input sequence of text and image tokens.</p>
<ul class="simple">
<li><p>GPT-3-like transformer decoder: A transformer decoder is a neural network architecture that excels in understanding and generating sequences. In DALL-E, the decoder is similar to GPT-3, a powerful language model that has been trained on vast amounts of text.</p></li>
<li><p>Text tokens: These are the individual units of text (words or subwords) that the model processes. In DALL-E, text tokens are used to provide a textual description of the desired image.</p></li>
<li><p>Image tokens: In DALL-E, images are represented as a sequence of image tokens, which are essentially small parts of an image. These tokens are used as input to the decoder, allowing it to generate a continuation or completion of the image.</p></li>
</ul>
<p>Here’s the process in simple terms:</p>
<ul class="simple">
<li><p>The input sequence: A sequence of text tokens describing the desired image and, optionally, some initial image tokens are fed into the decoder.</p></li>
<li><p>The decoder processes the input sequence: The transformer decoder processes the input sequence, understanding the relationship between the text tokens and image tokens.</p></li>
<li><p>Generating image continuation: Based on the processed input sequence, the decoder generates a continuation of the image, adding new image tokens that visually represent the input text.</p></li>
</ul>
<p>In the example you provided, the decoder is given a sequence with text tokens and an initial image token (with ID 42). It processes this sequence and generates a continuation of the image by producing the next image token (with ID 1369).</p>
</section>
<section id="sampling-from-a-trained-dall-e">
<h3>Sampling From a Trained DALL-E<a class="headerlink" href="#sampling-from-a-trained-dall-e" title="Permalink to this heading">#</a></h3>
<p>Sampling from a trained DALL-E involves preparing an input text description, encoding it, and feeding it to the decoder. The decoder generates a sequence of image tokens based on the input, which are then converted back into an image that represents the desired text description. The sampling process can be controlled using various strategies to influence the randomness and diversity of the generated images.</p>
<figure class="align-default" id="fig-dalle1-sampling">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_sampling.png"><img alt="../../_images/aiart_1_dalle1_sampling.png" src="../../_images/aiart_1_dalle1_sampling.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 119 </span><span class="caption-text">Sampling From a Trained DALL-E</span><a class="headerlink" href="#fig-dalle1-sampling" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-dalle1-sampling2">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_sampling2.png"><img alt="../../_images/aiart_1_dalle1_sampling2.png" src="../../_images/aiart_1_dalle1_sampling2.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 120 </span><span class="caption-text">Sampling From a Trained DALL-E</span><a class="headerlink" href="#fig-dalle1-sampling2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Sampling from a trained DALL-E is the process of generating new images based on a textual description provided as input.</p>
<ul class="simple">
<li><p>Prepare the input: Create a sequence of text tokens based on the desired textual description. This sequence will be used as input to guide the image generation process.</p></li>
<li><p>Encode the input: The text tokens are passed through an encoder, which converts them into a format that can be understood by the model. This typically involves mapping the text tokens to numerical representations called embeddings.</p></li>
<li><p>Generate image tokens: The encoded input is then fed into the DALL-E decoder. The decoder processes the input and generates a sequence of image tokens that visually represent the input text. The image tokens are generated one at a time, with each new token being conditioned on the input text tokens and previously generated image tokens.</p></li>
<li><p>Sampling strategy: During the image token generation process, a sampling strategy is used to control the randomness and diversity of the generated images. Common strategies include temperature-based sampling, where a higher temperature results in more diverse and random images, and top-k sampling, which restricts the token selection to the top-k most likely candidates.</p></li>
<li><p>Convert image tokens to an image: Once the sequence of image tokens is generated, they are converted back into an image format (e.g., a grid of pixels). This reconstructed image is the final output, which should visually represent the input text description.</p></li>
</ul>
</section>
</section>
<section id="dalle-1-results">
<h2>DALL·E 1 Results<a class="headerlink" href="#dalle-1-results" title="Permalink to this heading">#</a></h2>
<figure class="align-default" id="fig-dalle1-examples">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_examples.png"><img alt="../../_images/aiart_1_dalle1_examples.png" src="../../_images/aiart_1_dalle1_examples.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 121 </span><span class="caption-text">Several image generation examples from the original paper</span><a class="headerlink" href="#fig-dalle1-examples" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The trained model generated several samples (up to 512!) based on the text provided, then all these samples were ranked by a special model called CLIP, and the top-ranked one was chosen as the result of the model.</p>
<figure class="align-default" id="fig-dalle1-eval">
<a class="reference internal image-reference" href="../../_images/aiart_1_dalle1_eval.png"><img alt="../../_images/aiart_1_dalle1_eval.png" src="../../_images/aiart_1_dalle1_eval.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 122 </span><span class="caption-text">Evaluation</span><a class="headerlink" href="#fig-dalle1-eval" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<p>The DALL-E model represents a significant breakthrough in the field of artificial intelligence, particularly in the area of image generation. By successfully combining the power of transformer-based language models with the ability to generate high-quality images from textual descriptions, DALL-E has demonstrated remarkable capabilities in the realm of creative AI applications.</p>
<p>This innovative model has the potential to impact a wide range of industries, such as advertising, entertainment, education, and art, by enabling the automatic generation of customized visual content based on specific user inputs. Furthermore, DALL-E serves as an inspiration for future research in the intersection of natural language processing and computer vision, potentially paving the way for more advanced AI systems that can understand and manipulate both textual and visual information.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/aiart"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="aiart.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Art and Music in Light of AI</p>
      </div>
    </a>
    <a class="right-next"
       href="dalle2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">DALL·E 2</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dalle-1-charateristics">DALL·E 1 Charateristics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#controlling-attributes">Controlling Attributes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#drawing-multiple-objects">Drawing Multiple Objects</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-perspective-and-three-dimensionality">Visualizing Perspective and Three-Dimensionality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-internal-and-external-structure">Visualizing Internal and External Structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inferring-contextual-details">Inferring Contextual Details</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dalle-1-architecture">DALL·E 1 Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vq-vae">VQ-VAE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoders">Autoencoders</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-autoencoders-vae">Variational Autoencoders (VAE)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-spaces">Discrete Spaces</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty-in-the-posterior">Uncertainty in the Posterior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-from-a-trained-dall-e">Sampling From a Trained DALL-E</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dalle-1-results">DALL·E 1 Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://entelecheia.me" target="_blank">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>