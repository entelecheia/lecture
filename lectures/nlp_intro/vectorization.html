<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" /><link rel="next" title="Word Embeddings" href="word_embeddings.html" /><link rel="prev" title="Word Segmentation and Association" href="word_segmentation.html" />

    <!-- Generated with Sphinx 5.0.2 and Furo 2022.12.07 -->
        <title>Vector Semantics and Representation - Lecture Notes</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=91d0f0d1c444bdcb17a68e833c7a53903343c195" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">Lecture Notes</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  
  
  <span class="sidebar-brand-text">Lecture Notes</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Introduction to NLP</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="research.html">Research Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="language_models.html">Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="topic.html">Topic Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="topic_models.html">Topic Models </a></li>
<li class="toctree-l2"><a class="reference internal" href="topic_coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="sentiments.html">Sentiment Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="word_segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Vector Semantics and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="word_embeddings.html">Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab2-corpus-eda.html">Lab 2: EDA on Corpora</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_deep/index.html">Deep Learning for NLP</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/zeroshot.html">Zero Shot, Prompt, and Search Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/transformers.html">Transformers </a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/plms.html">Pretrained Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/lab3-train-tokenizers.html">Lab 3: Training Tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/lab4-pretraining-lms.html">Lab 4: Pretraining Language Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../nlp_apps/index.html">NLP Applications</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../aiart/index.html">AI Art (Text-to-Image)</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../aiart/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/project-themes.html">Project Themes - A Brave New World</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/imagen.html">Imagen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/textual-inversion.html">Textual Inversion (Dreambooth)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/whisper.html">Automatic Speech Recognition (Whisper)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/text2music.html">Text to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/image2music.html">Image to Music</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/dev-env.html">Development Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/dotfiles.html">Dotfiles</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ds/index.html">Data Science for Economics and Finance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/index.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/cite.html">Citation</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section class="tex2jax_ignore mathjax_ignore" id="vector-semantics-and-representation">
<h1>Vector Semantics and Representation<a class="headerlink" href="#vector-semantics-and-representation" title="Permalink to this heading">#</a></h1>
<p><img alt="" src="../../_images/entelecheia_alphabet_letters.png" /></p>
<section id="vector-semantics-and-word-embeddings">
<h2>Vector Semantics and Word Embeddings<a class="headerlink" href="#vector-semantics-and-word-embeddings" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Lexical semantics is the study of the meaning of words</p></li>
<li><p>Distributional hypothesis: words that occur in similar contexts have similar meanings</p></li>
<li><p>Sparse vectors: one-hot encoding or bag-of-words</p></li>
<li><p>Dense vectors: word embeddings</p></li>
</ul>
<section id="what-do-words-mean-and-how-do-we-represent-that">
<h3>What do words mean, and how do we represent that?<a class="headerlink" href="#what-do-words-mean-and-how-do-we-represent-that" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">cassoulet</span></code></p>
</div></blockquote>
<p>Do we want to represent that …</p>
<ul class="simple">
<li><p>“cassoulet” is a French dish?</p></li>
<li><p>“cassoulet” contains meat and beans?</p></li>
<li><p>“cassoulet” is a stew?</p></li>
</ul>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">bar</span></code></p>
</div></blockquote>
<p>Do we want to represent that …</p>
<ul class="simple">
<li><p>“bar” is a place where you can drink alcohol?</p></li>
<li><p>“bar” is a long rod?</p></li>
<li><p>“bar” is to prevent something from moving?</p></li>
</ul>
<p>About words, we can say that …</p>
<ul class="simple">
<li><p>Concepts or word senses have a complex many-to-many relationship with words</p></li>
<li><p>Words have relations with each other</p>
<ul>
<li><p>Synonyms: “bar” and “pub”</p></li>
<li><p>Antonyms: “bar” and “open”</p></li>
<li><p>Similarity: “bar” and “club”</p></li>
<li><p>Relatedness: “bar” and “restaurant”</p></li>
<li><p>Superordinate: “bar” and “place”</p></li>
<li><p>Subordinate: “bar” and “pub”</p></li>
<li><p>Connotation: “bar” and “prison”</p></li>
</ul>
</li>
</ul>
</section>
<section id="different-approaches-to-lexical-semantics">
<h3>Different approaches to lexical semantics<a class="headerlink" href="#different-approaches-to-lexical-semantics" title="Permalink to this heading">#</a></h3>
<p>NLP draws on two different approaches to lexical semantics:</p>
<ul class="simple">
<li><p><strong>Lexical semantics</strong>:</p>
<ul>
<li><p>The study of the meaning of words</p></li>
<li><p>The lexicographic tradition aims to capture the information represented in lexical entries in dictionaries</p></li>
</ul>
</li>
<li><p><strong>Distributional semantics</strong>:</p>
<ul>
<li><p>The study of the meaning of words based on their distributional properties in large corpora</p></li>
<li><p>The distributional hypothesis: words that occur in similar contexts have similar meanings</p></li>
</ul>
</li>
</ul>
<section id="lexical-semantics">
<h4>Lexical semantics<a class="headerlink" href="#lexical-semantics" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Uses resources such as <code class="docutils literal notranslate"><span class="pre">lexicons</span></code>, <code class="docutils literal notranslate"><span class="pre">thesauri</span></code>, <code class="docutils literal notranslate"><span class="pre">ontologies</span></code> etc. that capture explicit knowledge about word meanings.</p></li>
<li><p>Assumes that words have <code class="docutils literal notranslate"><span class="pre">discrete</span> <span class="pre">word</span> <span class="pre">senses</span></code> that can be represented in a <code class="docutils literal notranslate"><span class="pre">lexicon</span></code>.</p>
<ul>
<li><p>bank 1 = a financial institution</p></li>
<li><p>bank 2 = a river bank</p></li>
</ul>
</li>
<li><p>May capture explicit knowledge about word meanings, but is limited in its ability to capture the meaning of words that are not in the lexicon.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">dog</span></code> is a <code class="docutils literal notranslate"><span class="pre">canine</span></code> (lexicon)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cars</span></code> have <code class="docutils literal notranslate"><span class="pre">wheels</span></code> (lexicon)</p></li>
</ul>
</li>
</ul>
</section>
<section id="distributional-semantics">
<h4>Distributional semantics<a class="headerlink" href="#distributional-semantics" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Uses <code class="docutils literal notranslate"><span class="pre">large</span> <span class="pre">corpora</span> <span class="pre">of</span> <span class="pre">raw</span> <span class="pre">text</span></code> to learn the meaning of words from the contexts in which they occur.</p></li>
<li><p>Maps words to <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">representations</span></code> that capture the <code class="docutils literal notranslate"><span class="pre">distributional</span> <span class="pre">properties</span></code> of the words in the corpus.</p></li>
<li><p>Uses neural networks to learn the dense vector representations of words, <code class="docutils literal notranslate"><span class="pre">word</span> <span class="pre">embeddings</span></code>, from large corpora of raw text.</p></li>
<li><p>If each word is mapped to a single vector, this ignores the fact that words can have multiple meanings or parts of speech.</p></li>
</ul>
</section>
</section>
<section id="how-do-we-represent-words-to-capture-word-similarities">
<h3>How do we represent words to capture word similarities?<a class="headerlink" href="#how-do-we-represent-words-to-capture-word-similarities" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>As <code class="docutils literal notranslate"><span class="pre">atomic</span> <span class="pre">symbols</span></code></p>
<ul>
<li><p>in a traditional n-gram language model</p></li>
<li><p>explicit features in a machine learning model</p></li>
<li><p>this is equivalent to very high-dimensional one-hot vectors:</p>
<ul>
<li><p>aardvark = [1,0,…,0], bear = [0,1,…,0], …, zebra = [0,0,…,1]</p></li>
<li><p>height and tall are as different as aardvark and zebra</p></li>
</ul>
</li>
</ul>
</li>
<li><p>As very high-dimensional <code class="docutils literal notranslate"><span class="pre">sparse</span> <span class="pre">vectors</span></code></p>
<ul>
<li><p>to capture the distributional properties of words</p></li>
</ul>
</li>
<li><p>As low-dimensional <code class="docutils literal notranslate"><span class="pre">dense</span> <span class="pre">vectors</span></code></p>
<ul>
<li><p>word embeddings</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-should-word-representations-capture">
<h3>What should word representations capture?<a class="headerlink" href="#what-should-word-representations-capture" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Vector representations of words were originally used to capture <code class="docutils literal notranslate"><span class="pre">lexical</span> <span class="pre">semantics</span></code> so that words with similar meanings would be represented by vectors that are close together in vector space.</p></li>
<li><p>These representations may also capture some <code class="docutils literal notranslate"><span class="pre">morphological</span></code> and <code class="docutils literal notranslate"><span class="pre">syntactic</span></code> information about words. (part of speech, inflections, stems, etc.)</p></li>
</ul>
<section id="the-distributional-hypothesis">
<h4>The Distributional Hypothesis<a class="headerlink" href="#the-distributional-hypothesis" title="Permalink to this heading">#</a></h4>
<p>Zellig Harris (1954):</p>
<ul class="simple">
<li><p>Words that occur in similar contexts have similar meanings.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">oculist</span></code> and <code class="docutils literal notranslate"><span class="pre">eye</span> <span class="pre">doctor</span></code> occur in almost the same contexts</p></li>
<li><p>If A and B have almost the same environment, then A and B are synonymous.</p></li>
</ul>
<p>John Firth (1957):</p>
<ul class="simple">
<li><p>You shall know a word by the company it keeps.</p></li>
</ul>
<blockquote>
<div><p>The <code class="docutils literal notranslate"><span class="pre">contexts</span></code> in which words occur tell us a lot about the meaning of words.</p>
<p>Words that occur in similar contexts have similar meanings.</p>
</div></blockquote>
</section>
<section id="why-do-we-care-about-word-contexts">
<h4>Why do we care about word contexts?<a class="headerlink" href="#why-do-we-care-about-word-contexts" title="Permalink to this heading">#</a></h4>
<p>What is <code class="docutils literal notranslate"><span class="pre">tezgüino</span></code>?</p>
<ul class="simple">
<li><p>A bottle of <code class="docutils literal notranslate"><span class="pre">tezgüino</span></code> is on the table.</p></li>
<li><p>Everybody likes <code class="docutils literal notranslate"><span class="pre">tezgüino</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Tezgüino</span></code> makes you drunk.</p></li>
<li><p>We make <code class="docutils literal notranslate"><span class="pre">tezgüino</span></code> out of corn.</p></li>
</ul>
<p>We don’t know what <code class="docutils literal notranslate"><span class="pre">tezgüino</span></code> is, but we can guess that it is a drink because we understand these sentences.</p>
<p>If we have the following sentences:</p>
<ul class="simple">
<li><p>A bottle of <code class="docutils literal notranslate"><span class="pre">wine</span></code> is on the table.</p></li>
<li><p>There is a <code class="docutils literal notranslate"><span class="pre">beer</span></code> bottle on the table</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Beer</span></code> makes you drunk.</p></li>
<li><p>We make <code class="docutils literal notranslate"><span class="pre">bourbon</span></code> out of corn.</p></li>
<li><p>Everybody likes <code class="docutils literal notranslate"><span class="pre">chocolate</span></code></p></li>
<li><p>Everybody likes <code class="docutils literal notranslate"><span class="pre">babies</span></code></p></li>
</ul>
<p>Could we guess that <code class="docutils literal notranslate"><span class="pre">tezgüino</span></code> is a drink like <code class="docutils literal notranslate"><span class="pre">wine</span></code> or <code class="docutils literal notranslate"><span class="pre">beer</span></code>?</p>
<p>However, there are also red herrings:</p>
<ul class="simple">
<li><p>Everybody likes <code class="docutils literal notranslate"><span class="pre">babies</span></code></p></li>
<li><p>Everybody likes <code class="docutils literal notranslate"><span class="pre">chocolate</span></code></p></li>
</ul>
</section>
</section>
<section id="two-ways-nlp-uses-context-for-semantics">
<h3>Two ways NLP uses context for semantics<a class="headerlink" href="#two-ways-nlp-uses-context-for-semantics" title="Permalink to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Distributional</span> <span class="pre">similarity</span></code>: (vector-space semantics)</p>
<ul class="simple">
<li><p>Assume that words that occur in similar contexts have similar meanings.</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">set</span> <span class="pre">of</span> <span class="pre">all</span> <span class="pre">contexts</span></code> in which a word occurs to measure the <code class="docutils literal notranslate"><span class="pre">similarity</span></code> between words.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">Word</span> <span class="pre">sense</span> <span class="pre">disambiguation</span></code>:</p>
<ul class="simple">
<li><p>Assume that if a word has multiple meanings, then it will occur in different contexts for each meaning.</p></li>
<li><p>Use the context of a particular occurrence of a word to identify the <code class="docutils literal notranslate"><span class="pre">sense</span></code> of the word in that context.</p></li>
</ul>
</section>
</section>
<section id="distributional-similarity">
<h2>Distributional Similarity<a class="headerlink" href="#distributional-similarity" title="Permalink to this heading">#</a></h2>
<section id="basic-idea">
<h3>Basic idea<a class="headerlink" href="#basic-idea" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Measure the semantic <code class="docutils literal notranslate"><span class="pre">similarities</span> <span class="pre">of</span> <span class="pre">words</span></code> by measuring the <code class="docutils literal notranslate"><span class="pre">similarity</span> <span class="pre">of</span> <span class="pre">their</span> <span class="pre">contexts</span></code> in which they occur</p></li>
</ul>
</section>
<section id="how">
<h3>How?<a class="headerlink" href="#how" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Represent words as <code class="docutils literal notranslate"><span class="pre">sparse</span> <span class="pre">vectors</span></code> such that:</p>
<ul>
<li><p>each <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">element</span></code> (dimension) represents a different <code class="docutils literal notranslate"><span class="pre">context</span></code></p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">value</span></code> of each element is the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> of the context in which the word occurs, capturing how <code class="docutils literal notranslate"><span class="pre">strongly</span></code> the word is associated with that context</p></li>
</ul>
</li>
<li><p>Compute the <code class="docutils literal notranslate"><span class="pre">semantic</span> <span class="pre">similarity</span> <span class="pre">of</span> <span class="pre">words</span></code> by measuring the <code class="docutils literal notranslate"><span class="pre">similarity</span> <span class="pre">of</span> <span class="pre">their</span> <span class="pre">context</span> <span class="pre">vectors</span></code></p></li>
</ul>
<p>Distributional similarities represent each word <span class="math notranslate nohighlight">\(w\)</span> as a vector <span class="math notranslate nohighlight">\(v_w\)</span> of context counts:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[w = (w_1 , \ldots , w_N ) \in R^N\]</div>
</div>
<p>in a vector space <span class="math notranslate nohighlight">\(R^N\)</span> where <span class="math notranslate nohighlight">\(N\)</span> is the number of contexts.</p>
<ul class="simple">
<li><p>each dimension <span class="math notranslate nohighlight">\(i\)</span> represents a different context <span class="math notranslate nohighlight">\(c_i\)</span></p></li>
<li><p>each element <span class="math notranslate nohighlight">\(v_{w,i}\)</span> captures how strongly <span class="math notranslate nohighlight">\(w\)</span> is associated with context <span class="math notranslate nohighlight">\(c_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(v_{w,i}\)</span> is the co-occurrence count of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(c_i\)</span></p></li>
</ul>
</section>
<section id="the-information-retrieval-perspective-the-term-document-matrix">
<h3>The Information Retrieval perspective: The Term-Document Matrix<a class="headerlink" href="#the-information-retrieval-perspective-the-term-document-matrix" title="Permalink to this heading">#</a></h3>
<p>In information retrieval, we search a collection of <span class="math notranslate nohighlight">\(N\)</span> documents for <span class="math notranslate nohighlight">\(M\)</span> terms:</p>
<ul class="simple">
<li><p>We can represent each <code class="docutils literal notranslate"><span class="pre">word</span></code> in the vocabulary <span class="math notranslate nohighlight">\(V\)</span> as an <span class="math notranslate nohighlight">\(N\)</span>-dimensional vector <span class="math notranslate nohighlight">\(v_w\)</span> where <span class="math notranslate nohighlight">\(v_{w,i}\)</span> is the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> of the word <span class="math notranslate nohighlight">\(w\)</span> in document <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p>Conversely, we can represent each <code class="docutils literal notranslate"><span class="pre">document</span></code> as an <span class="math notranslate nohighlight">\(M\)</span>-dimensional vector <span class="math notranslate nohighlight">\(v_d\)</span> where <span class="math notranslate nohighlight">\(v_{d,j}\)</span> is the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> of the term <span class="math notranslate nohighlight">\(t_j\)</span> in document <span class="math notranslate nohighlight">\(d\)</span>.</p></li>
</ul>
<p>Finding the <code class="docutils literal notranslate"><span class="pre">most</span> <span class="pre">relevant</span></code> documents for a query <span class="math notranslate nohighlight">\(q\)</span> is equivalent to finding the <code class="docutils literal notranslate"><span class="pre">most</span> <span class="pre">similar</span></code> documents to the query vector <span class="math notranslate nohighlight">\(v_q\)</span>.</p>
<ul class="simple">
<li><p>Queries are also documents, so we can use the same vector representation for queries and documents.</p></li>
<li><p>Use the similarity of the query vector <span class="math notranslate nohighlight">\(v_q\)</span> to the document vectors <span class="math notranslate nohighlight">\(v_d\)</span> to rank the documents.</p></li>
<li><p>Documents are similar to queries if they have similar terms.</p></li>
</ul>
</section>
</section>
<section id="term-document-matrix">
<h2>Term-Document Matrix<a class="headerlink" href="#term-document-matrix" title="Permalink to this heading">#</a></h2>
<p><img alt="" src="../../_images/214.png" /></p>
<p>A term-document matrix is a 2D matrix:</p>
<ul class="simple">
<li><p>each row represents a <code class="docutils literal notranslate"><span class="pre">term</span></code> in the vocabulary</p></li>
<li><p>each column represents a <code class="docutils literal notranslate"><span class="pre">document</span></code></p></li>
<li><p>each element is the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> of the term in the document</p></li>
</ul>
<p><img alt="" src="../../_images/38.png" /></p>
<ul class="simple">
<li><p>Each <code class="docutils literal notranslate"><span class="pre">column</span> <span class="pre">vector</span></code> = a <code class="docutils literal notranslate"><span class="pre">document</span></code></p>
<ul>
<li><p>Each entry = the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> of the term in the document</p></li>
</ul>
</li>
<li><p>Each <code class="docutils literal notranslate"><span class="pre">row</span> <span class="pre">vector</span></code> = a <code class="docutils literal notranslate"><span class="pre">term</span></code></p>
<ul>
<li><p>Each entry = the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> of the term in the document</p></li>
</ul>
</li>
</ul>
<blockquote>
<div><p>Two documents are similar if their vectors are similar.</p>
</div></blockquote>
<blockquote>
<div><p>Two words are similar if their vectors are similar.</p>
</div></blockquote>
<p>For information retrieval, the term-document matrix is useful because it allows us to represent documents as vectors and compute the similarity between documents in terms of the words they contain, or of terms in terms of the documents they occur in.</p>
<p>We can adapt this idea to implement <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">model</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">distributional</span> <span class="pre">hypothesis</span></code> if we treat each context as a column in the matrix and each word as a row.</p>
<section id="what-is-a-context">
<h3>What is a <code class="docutils literal notranslate"><span class="pre">context</span></code>?<a class="headerlink" href="#what-is-a-context" title="Permalink to this heading">#</a></h3>
<p>There are many ways to define a context:</p>
<p><strong>Contexts defined by nearby words:</strong></p>
<ul class="simple">
<li><p>How often does the word <span class="math notranslate nohighlight">\(w_i\)</span> occur within a window of <span class="math notranslate nohighlight">\(k\)</span> words of the word <span class="math notranslate nohighlight">\(w_j\)</span>?</p></li>
<li><p>Or, how often do the words <span class="math notranslate nohighlight">\(w_i\)</span> and <span class="math notranslate nohighlight">\(w_j\)</span> occur in the same document or sentence?</p></li>
<li><p>This yields fairly broad thematic similarities between words.</p></li>
</ul>
<p><strong>Contexts defined by <code class="docutils literal notranslate"><span class="pre">grammtical</span> <span class="pre">relations</span></code>:</strong></p>
<ul class="simple">
<li><p>How often does the word <span class="math notranslate nohighlight">\(w_i\)</span> occur as the <code class="docutils literal notranslate"><span class="pre">subject</span></code> of the word <span class="math notranslate nohighlight">\(w_j\)</span>?</p></li>
<li><p>This requires a <code class="docutils literal notranslate"><span class="pre">grammatical</span> <span class="pre">parser</span></code> to identify the grammatical relations between words.</p></li>
<li><p>This yields more <code class="docutils literal notranslate"><span class="pre">fine-grained</span></code> similarities between words.</p></li>
</ul>
</section>
<section id="using-nearby-words-as-contexts">
<h3>Using nearby words as contexts<a class="headerlink" href="#using-nearby-words-as-contexts" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Define a fixed vocabulary of <span class="math notranslate nohighlight">\(N\)</span> context words <span class="math notranslate nohighlight">\(c_1 , \ldots , c_N\)</span></p></li>
</ol>
<ul class="simple">
<li><p>Contexts words should occur frequently enough in the corpus that you can get reliable counts.</p></li>
<li><p>However, you should ignore very frequent words (stopwords) like <code class="docutils literal notranslate"><span class="pre">the</span></code> and <code class="docutils literal notranslate"><span class="pre">a</span></code> because they are not very informative.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Define what <code class="docutils literal notranslate"><span class="pre">nearby</span></code> means:</p></li>
</ol>
<ul class="simple">
<li><p>For example, we can define a <code class="docutils literal notranslate"><span class="pre">window</span></code> of <span class="math notranslate nohighlight">\(k\)</span> words on either side of the word <span class="math notranslate nohighlight">\(w_j\)</span>.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Count the number of times each context word <span class="math notranslate nohighlight">\(c_i\)</span> occurs within a window of <span class="math notranslate nohighlight">\(k\)</span> words of the word <span class="math notranslate nohighlight">\(w_j\)</span>.</p></li>
<li><p>Define how to transform the co-occurrence counts into a vector representation of the word <span class="math notranslate nohighlight">\(w_j\)</span>.</p></li>
</ol>
<ul class="simple">
<li><p>For example, we can use the (positive) <code class="docutils literal notranslate"><span class="pre">PMI</span></code> of the word <span class="math notranslate nohighlight">\(w_j\)</span> and the context word <span class="math notranslate nohighlight">\(c_i\)</span>.</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p>Compute the similarity between words by measuring the similarity of their context vectors.</p></li>
</ol>
<ul class="simple">
<li><p>For example, we can use the cosine similarity of the context vectors.</p></li>
</ul>
</section>
<section id="word-word-matrix">
<h3>Word-Word Matrix<a class="headerlink" href="#word-word-matrix" title="Permalink to this heading">#</a></h3>
<p><img alt="" src="../../_images/43.png" /></p>
<p>Resulting word-word matrix:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(w, c)\)</span> = how often does word w appear in context <span class="math notranslate nohighlight">\(c\)</span>:</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">information</span></code> appeared six times in the context of <code class="docutils literal notranslate"><span class="pre">data</span></code></p></li>
</ul>
<p><img alt="" src="../../_images/53.png" /></p>
</section>
<section id="defining-co-occurrences">
<h3>Defining co-occurrences:<a class="headerlink" href="#defining-co-occurrences" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Within</span> <span class="pre">a</span> <span class="pre">fixed</span> <span class="pre">window</span></code>: <span class="math notranslate nohighlight">\(c_i\)</span> occurs within <span class="math notranslate nohighlight">\(\pm n\)</span> words of <span class="math notranslate nohighlight">\(w\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Within</span> <span class="pre">the</span> <span class="pre">same</span> <span class="pre">sentence</span></code>: requires sentence boundaries</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">By</span> <span class="pre">grammatical</span> <span class="pre">relations</span></code>: <span class="math notranslate nohighlight">\(c_i\)</span> occurs as a subject/object/modifier/… of verb <span class="math notranslate nohighlight">\(w\)</span> (requires parsing - and separate features for each relation)</p></li>
</ul>
</section>
<section id="representing-co-occurrences">
<h3>Representing co-occurrences:<a class="headerlink" href="#representing-co-occurrences" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_i\)</span> as<code class="docutils literal notranslate"> <span class="pre">binary</span> <span class="pre">features</span></code> (1,0): <span class="math notranslate nohighlight">\(w\)</span> does/does not occur with <span class="math notranslate nohighlight">\(c_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f_i\)</span> as frequencies: <span class="math notranslate nohighlight">\(w\)</span> occurs <span class="math notranslate nohighlight">\(n\)</span> times with <span class="math notranslate nohighlight">\(c_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f_i\)</span> as probabilities: e.g. <span class="math notranslate nohighlight">\(f_i\)</span> is the probability that <span class="math notranslate nohighlight">\(c_i\)</span> is the subject of <span class="math notranslate nohighlight">\(w\)</span>.</p></li>
</ul>
</section>
<section id="getting-co-occurrence-counts">
<h3>Getting co-occurrence counts<a class="headerlink" href="#getting-co-occurrence-counts" title="Permalink to this heading">#</a></h3>
<p>Co-occurrence as a <code class="docutils literal notranslate"><span class="pre">binary</span></code> feature:</p>
<ul class="simple">
<li><p>Does word <span class="math notranslate nohighlight">\(w\)</span> ever appear in the context <span class="math notranslate nohighlight">\(c\)</span>? (1 = yes/0 = no)</p></li>
</ul>
<p><img alt="" src="../../_images/62.png" /></p>
<p>Co-occurrence as a frequency count:</p>
<ul class="simple">
<li><p>How often does word <span class="math notranslate nohighlight">\(w\)</span> appear in the context <span class="math notranslate nohighlight">\(c\)</span>? (0,1,2,… times)</p></li>
</ul>
<p><img alt="" src="../../_images/72.png" /></p>
</section>
<section id="counts-vs-pmi">
<h3>Counts vs PMI<a class="headerlink" href="#counts-vs-pmi" title="Permalink to this heading">#</a></h3>
<p>Sometimes, low co-occurrences counts are very informative, and high co-occurrence counts are not:</p>
<ul class="simple">
<li><p>Any word is going to have relatively high co-occurrence counts with very common contexts (e.g. <code class="docutils literal notranslate"><span class="pre">the</span></code> with <code class="docutils literal notranslate"><span class="pre">a</span></code>), but this won’t tell us much about what that word means.</p></li>
<li><p>We need to identify when co-occurrence counts are higher than we would expect by chance.</p></li>
</ul>
<p>We can use pointwise mutual information (PMI) values instead of raw frequency counts:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[ PMI(w,c) = \log \frac{p(w,c)}{p(w)p(c)} \]</div>
</div>
<p><img alt="" src="../../_images/83.png" /></p>
</section>
<section id="computing-pmi-of-w-and-c">
<h3>Computing PMI of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(c\)</span>:<a class="headerlink" href="#computing-pmi-of-w-and-c" title="Permalink to this heading">#</a></h3>
<section id="using-a-fixed-window-of-pm-k-words">
<h4>Using a fixed window of <span class="math notranslate nohighlight">\(\pm k\)</span> words<a class="headerlink" href="#using-a-fixed-window-of-pm-k-words" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span>: How many tokens does the corpus contain?</p></li>
<li><p><span class="math notranslate nohighlight">\(f(w) \le N\)</span>: How often does <span class="math notranslate nohighlight">\(w\)</span> occur?</p></li>
<li><p><span class="math notranslate nohighlight">\(f(w, c) \le f(w)\)</span>: How often does <span class="math notranslate nohighlight">\(w\)</span> occur with <span class="math notranslate nohighlight">\(c\)</span> in its window?</p></li>
<li><p><span class="math notranslate nohighlight">\(f(c) = \sum_w f(w, c)\)</span>: How many tokens have <span class="math notranslate nohighlight">\(c\)</span> in their window?</p></li>
</ul>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[p(w) = \frac{f{w}}{N}, p(c) = \frac{f(c)}{N}, p(w,c) = \frac{f(w,c)}{N}\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[PMI(w,c) = \log \frac{p(w,c)}{p(w)p(c)}\]</div>
</div>
<p>Positive Pointwise Mutual Information</p>
<p>PMI is negative when words co-occur less than expected by chance.</p>
<ul class="simple">
<li><p>This is unreliable without huge corpora:</p></li>
<li><p>With <span class="math notranslate nohighlight">\(P(w ) \approx P(w2 ) \approx 10^{−6}\)</span> , we can’t estimate whether <span class="math notranslate nohighlight">\(P(w_1 , w_2 )\)</span> is significantly different from <span class="math notranslate nohighlight">\(10^{−12}\)</span></p></li>
</ul>
<p>We often just use positive PMI values, and replace all negative PMI values with 0:</p>
<p>Positive Pointwise Mutual Information (PPMI):</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\text{PPMI}(w, c) = PMI, \text{ if } \text{PMI}(w, c) \gt 0
\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\text{PPMI}(w, c) = 0, \text{ if } \text{PMI}(w, c) \le 0
\]</div>
</div>
<p>PMI and smoothing</p>
<p>PMI is biased towards infrequent events:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(P(w, c) = P(w) = P(c)\)</span>, then <span class="math notranslate nohighlight">\(\text{PMI}(w, c) = \log (\frac{1}{P(w)})\)</span></p></li>
<li><p>So <span class="math notranslate nohighlight">\(\text{PMI}(w, c)\)</span> is larger for rare words <span class="math notranslate nohighlight">\(w\)</span> with low <span class="math notranslate nohighlight">\(P(w)\)</span>.</p></li>
</ul>
<p>Simple remedy: <code class="docutils literal notranslate"><span class="pre">Add-k</span> <span class="pre">smoothing</span></code> of <span class="math notranslate nohighlight">\(P(w, c), P(w), P(c)\)</span> pushes all PMI values towards zero.</p>
<ul class="simple">
<li><p>Add-k smoothing affects low-probability events more, and will therefore reduce the bias of PMI towards infrequent events. (Pantel &amp; Turney 2010)</p></li>
</ul>
</section>
</section>
</section>
<section id="dot-product-as-similarity">
<h2>Dot product as similarity<a class="headerlink" href="#dot-product-as-similarity" title="Permalink to this heading">#</a></h2>
<p>If the vectors consist of simple binary features (0,1), we can use the <code class="docutils literal notranslate"><span class="pre">dot</span> <span class="pre">product</span></code> as <code class="docutils literal notranslate"><span class="pre">similarity</span> <span class="pre">metric</span></code>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
sim_{dot-prod}(\vec{x}\cdot\vec{y}) = \sum_{i=1}^{N} x_i \times y_i
\]</div>
</div>
<p>The dot product is a bad metric if the vector elements are arbitrary features: it prefers <code class="docutils literal notranslate"><span class="pre">long</span></code> vectors</p>
<ul class="simple">
<li><p>If one <span class="math notranslate nohighlight">\(x_i\)</span> is very large (and <span class="math notranslate nohighlight">\(y_i\)</span> nonzero), <span class="math notranslate nohighlight">\(sim(x, y)\)</span> gets very large</p></li>
<li><p>If the number of nonzero <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(y_i\)</span> is very large, <span class="math notranslate nohighlight">\(sim(x, y)\)</span> gets very large.</p></li>
<li><p>Both can happen with frequent words.</p></li>
</ul>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\text{length of }\vec{x}: |\vec{x}|=\sqrt{\sum_{i=1}^{N}x_i^2}
\]</div>
</div>
</section>
<section id="vector-similarity-cosine">
<h2>Vector similarity: Cosine<a class="headerlink" href="#vector-similarity-cosine" title="Permalink to this heading">#</a></h2>
<p>One way to define the similarity of two vectors is to use the cosine of their angle.</p>
<p>The cosine of two vectors is their dot product, divided by the product of their lengths:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
sim_{cos}(\vec{x},\vec{y})=\frac{\sum_{i=1}^{N} x_i \times y_i}{\sqrt{\sum_{i=1}^{N}x_i^2}\sqrt{\sum_{i=1}^{N}y_i^2}} = \frac{\vec{x}\cdot\vec{y}}{|\vec{x}||\vec{y}|}
\]</div>
</div>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(sim(\mathbf{w}, \mathbf{u}) = 1\)</span>: <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> point in the same direction</p>
</div></blockquote>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(sim(\mathbf{w}, \mathbf{u}) = 0\)</span>: <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> are orthogonal</p>
</div></blockquote>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(sim(\mathbf{w}, \mathbf{u}) = -1\)</span>: <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> point in the opposite direction</p>
</div></blockquote>
<section id="visualizing-cosines">
<h3>Visualizing cosines<a class="headerlink" href="#visualizing-cosines" title="Permalink to this heading">#</a></h3>
<p><img alt="" src="../../_images/cosine-viz.png" /></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/nlp_intro"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="word_embeddings.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Word Embeddings</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="word_segmentation.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Word Segmentation and Association</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Vector Semantics and Representation</a><ul>
<li><a class="reference internal" href="#vector-semantics-and-word-embeddings">Vector Semantics and Word Embeddings</a><ul>
<li><a class="reference internal" href="#what-do-words-mean-and-how-do-we-represent-that">What do words mean, and how do we represent that?</a></li>
<li><a class="reference internal" href="#different-approaches-to-lexical-semantics">Different approaches to lexical semantics</a><ul>
<li><a class="reference internal" href="#lexical-semantics">Lexical semantics</a></li>
<li><a class="reference internal" href="#distributional-semantics">Distributional semantics</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-do-we-represent-words-to-capture-word-similarities">How do we represent words to capture word similarities?</a></li>
<li><a class="reference internal" href="#what-should-word-representations-capture">What should word representations capture?</a><ul>
<li><a class="reference internal" href="#the-distributional-hypothesis">The Distributional Hypothesis</a></li>
<li><a class="reference internal" href="#why-do-we-care-about-word-contexts">Why do we care about word contexts?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#two-ways-nlp-uses-context-for-semantics">Two ways NLP uses context for semantics</a></li>
</ul>
</li>
<li><a class="reference internal" href="#distributional-similarity">Distributional Similarity</a><ul>
<li><a class="reference internal" href="#basic-idea">Basic idea</a></li>
<li><a class="reference internal" href="#how">How?</a></li>
<li><a class="reference internal" href="#the-information-retrieval-perspective-the-term-document-matrix">The Information Retrieval perspective: The Term-Document Matrix</a></li>
</ul>
</li>
<li><a class="reference internal" href="#term-document-matrix">Term-Document Matrix</a><ul>
<li><a class="reference internal" href="#what-is-a-context">What is a <code class="docutils literal notranslate"><span class="pre">context</span></code>?</a></li>
<li><a class="reference internal" href="#using-nearby-words-as-contexts">Using nearby words as contexts</a></li>
<li><a class="reference internal" href="#word-word-matrix">Word-Word Matrix</a></li>
<li><a class="reference internal" href="#defining-co-occurrences">Defining co-occurrences:</a></li>
<li><a class="reference internal" href="#representing-co-occurrences">Representing co-occurrences:</a></li>
<li><a class="reference internal" href="#getting-co-occurrence-counts">Getting co-occurrence counts</a></li>
<li><a class="reference internal" href="#counts-vs-pmi">Counts vs PMI</a></li>
<li><a class="reference internal" href="#computing-pmi-of-w-and-c">Computing PMI of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(c\)</span>:</a><ul>
<li><a class="reference internal" href="#using-a-fixed-window-of-pm-k-words">Using a fixed window of <span class="math notranslate nohighlight">\(\pm k\)</span> words</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#dot-product-as-similarity">Dot product as similarity</a></li>
<li><a class="reference internal" href="#vector-similarity-cosine">Vector similarity: Cosine</a><ul>
<li><a class="reference internal" href="#visualizing-cosines">Visualizing cosines</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/js/hoverxref.js"></script>
    <script src="../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script src="../../_static/scripts/furo.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>