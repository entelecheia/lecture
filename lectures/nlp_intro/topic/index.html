

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Topic Modeling &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../../_static/tabs.js"></script>
    <script src="../../../_static/js/hoverxref.js"></script>
    <script src="../../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/chatgpt.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/nlp_intro/topic/index';</script>
    <link rel="canonical" href="https://lecture.entelecheia.ai/lectures/nlp_intro/topic/index.html" />
    <link rel="shortcut icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Topic Coherence Measures" href="coherence.html" />
    <link rel="prev" title="Lab: Crawling DART Data" href="../datasets/lab-dart.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content"><p>
  <strong>Announcement:</strong>
  You can install the accompanying AI tutor <a href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd" target="_blank">here</a>.
  <a class="reference external" href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd" target="_blank"><img alt="chrome-web-store-image" src="https://img.shields.io/chrome-web-store/v/lfgfgbomindbccgidgalhhndggddpagd" /></a>
</p>
</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Introduction to NLP</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../intro/index.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research/index.html">Research Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lm/index.html">Language Models</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../datasets/index.html">Datasets</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../datasets/corpus.html">Corpus and Text Data Collection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../datasets/lab-dart.html">Lab: Crawling DART Data</a></li>
</ul>
</li>
<li class="toctree-l2 current active has-children"><a class="current reference internal" href="#">Topic Modeling</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l3"><a class="reference internal" href="coherence-practice.html">Topic Coherence in Practice</a></li>
<li class="toctree-l3"><a class="reference internal" href="tomotopy.html">Topic Modeling Tools - Tomotopy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../sentiments/index.html">Sentiment Analysis</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tokenization/index.html">Tokenization</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../tokenization/segmentation.html">Word Segmentation and Association</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../embeddings/index.html">Word Embeddings</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_deep/index.html">Deep Learning for NLP</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/intro.html">Introduction</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/llms/index.html">Large Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/zeroshot.html">Zero Shot and Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/decoding.html">Decoding and Search Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/plms.html">Pretrained Language Models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/transformers/index.html">Transformers</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/bert.html">BERT: Bidirectional Encoder Representations from Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/bertviz.html">BERT: Visualizing Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/datasets/index.html">Datasets</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/datasets/lab-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/tokenization/index.html">Tokenization</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/sentencepiece.html">SentencePiece Tokenizer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/detectGPT.html">How to Spot Machine-Written Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/lab3-train-tokenizers.html">Lab 3: Training Tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/lab4-pretraining-lms.html">Lab 4: Pretraining Language Models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_advances/index.html">Advances in AI and NLP</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/thesis.html">Writing a Thesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/detectGPT.html">DetectGPT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/gpt4.html">GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/camelids.html">Meet the Camelids: A Family of LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/sam/index.html">Segment Everything</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../aiart/index.html">AI Art (Generative AI)</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/aiart.html">Art and Music in Light of AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/imagen.html">Imagen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/textual-inversion.html">Textual Inversion (Dreambooth)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/whisper.html">Automatic Speech Recognition (Whisper)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/text2music.html">Text to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/image2music.html">Image to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/robot_drawings.html">Robot Drawing Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/project-themes.html">Project Themes - A Brave New World</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/devops/index.html">DevOps</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/gitops.html">GitOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/devsecops.html">DevSecOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/llmops.html">LLMOps</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/dotfiles/index.html">Dotfiles</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai">Dotfiles Repository</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/dotfiles/dotdrop.html">Dotdrop</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dotdrop/">Dotdrop Setup Script</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/github/">GitHub Setup Scripts</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/ssh-gpg-age/">SSH, GPG, and Age Setup Scripts</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/pass/">Pass and Passage Scripts</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/doppler/">Doppler Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dockerfiles/">Dockerfiles Scripts</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/security/index.html">Security Management</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/auth-enc-sign.html">Authentication, Encryption, and Signing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/age-gpg-ssh.html">SSH, GPG, and AGE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/pass.html">Unix Password Managers</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/github/index.html">Github’s Fork &amp; Pull Workflow</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/template.html">Project Templating Tools</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-python.entelecheia.ai/">Hyperfast Python Template</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-template.entelecheia.ai/">Hyperfast Template</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/containerization/index.html">Containerization</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/docker.html">Docker</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/containerd.html"><code class="docutils literal notranslate"><span class="pre">containerd</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/simple-pipeline/index.html">Simple MLOps Pipeline</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../ds/index.html">Data Science for Economics and Finance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../about/lecture-bot.html">LectureBot for ἐντελέχεια.άι</a></li>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference external" href="https://course.entelecheia.ai">course.entelecheia.ai</a></li>
<li class="toctree-l1"><a class="reference external" href="https://research.entelecheia.ai">research.entelecheia.ai</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about/index.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/entelecheia/lecture/blob/main/book/lectures/nlp_intro/topic/index.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/lectures/nlp_intro/topic/index.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Topic Modeling</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-methodologies">Topic Modeling Methodologies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-latent-semantic-analysis-plsa">Probabilistic Latent Semantic Analysis (pLSA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-dirichlet-allocation-lda">Latent Dirichlet Allocation (LDA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-negative-matrix-factorization-nmf">Non-negative Matrix Factorization (NMF)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlated-topic-model-ctm">Correlated Topic Model (CTM)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">Applications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-topic-models-dtm">Dynamic Topic Models (DTM)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Applications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-example-using-nmf-and-svd">Topic Modeling Example using NMF and SVD</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem">The problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-processing">Data Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#singular-value-decomposition-svd">Singular Value Decomposition (SVD)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Non-negative Matrix Factorization (NMF)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nmf-applications">NMF Applications</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nmf-using-scikit-learn">NMF using scikit-learn</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#truncated-svd">Truncated SVD</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#shortcomings-of-classical-algorithms-for-decomposition">Shortcomings of classical algorithms for decomposition:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-randomized-algorithms">Advantages of randomized algorithms:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#timing-comparison">Timing comparison</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-clustering">Document Clustering</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cosine-similarity">Cosine Similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction">Dimensionality Reduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-algorithms">Clustering Algorithms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next">Next</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="topic-modeling">
<h1>Topic Modeling<a class="headerlink" href="#topic-modeling" title="Permalink to this heading">#</a></h1>
<p><img alt="" src="../../../_images/entelecheia_grouping.png" /></p>
<p>As the digital age continues to expand, the volume of unstructured text data grows exponentially. From social media posts and news articles to research papers and customer reviews, there is a wealth of information waiting to be extracted and analyzed. Topic modeling emerges as a powerful tool to summarize and make sense of this large-scale unstructured text data, enabling users to better understand, navigate, and utilize the available information.</p>
<p>At its core, topic modeling is an unsupervised machine learning technique that employs the words within a document to infer its underlying subject. By identifying patterns of word co-occurrence and frequencies, topic models can reveal the latent structure or “topics” that characterize a collection of documents. This process not only allows for a high-level summary of the document’s content but also serves as a valuable method for dimension reduction.</p>
<p>Dimension reduction is crucial for simplifying complex data and enhancing the efficiency of subsequent analysis. In the context of text data, topic modeling excels at reducing the dimensionality of the document-word matrix by representing documents as mixtures of topics and topics as distributions over words. This condensed representation not only captures the essence of the document’s content but is also more interpretable compared to other dimension reduction techniques.</p>
<p>While other methods, such as Principal Component Analysis (PCA), are also used for dimension reduction, topic models offer a more interpretable and contextually meaningful output. PCA transforms the original data into orthogonal components that explain the maximum variance, which can be difficult to relate back to the original features. In contrast, topic models generate a set of human-understandable topics that provide a clear view of the document’s thematic structure.</p>
<p>In summary,</p>
<ul class="simple">
<li><p>Summarize unstructured text: Topic modeling helps in making sense of large-scale unstructured text data, providing high-level summaries of documents’ content.</p></li>
<li><p>Infer subject using words: Analyzes patterns of word co-occurrence and frequencies to reveal latent topics characterizing a collection of documents.</p></li>
<li><p>Useful for dimension reduction: Represents documents as mixtures of topics and topics as distributions over words, simplifying complex data and enhancing analysis efficiency.</p></li>
<li><p>More interpretable than PCA: Generates human-understandable topics that provide a clear view of the document’s thematic structure, compared to PCA’s orthogonal components which can be difficult to relate back to original features.</p></li>
</ul>
<section id="topic-modeling-methodologies">
<h2>Topic Modeling Methodologies<a class="headerlink" href="#topic-modeling-methodologies" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Probabilistic Latent Semantic Analysis (pLSA)</strong>: pLSA is a precursor to LDA and is based on a generative probabilistic model. It discovers latent topics in a document collection by modeling the co-occurrence of words and documents as a mixture of multinomial distributions. Despite its success in revealing hidden topics, pLSA suffers from overfitting due to the lack of regularization.</p></li>
<li><p><strong>Latent Dirichlet Allocation (LDA)</strong>: LDA is a widely-used generative probabilistic model for topic modeling, which extends pLSA by incorporating Dirichlet priors for document-topic and topic-term distributions. LDA’s assumptions about the generative process of documents and the incorporation of Dirichlet priors help in overcoming overfitting and provide better generalization.</p></li>
<li><p><strong>Non-negative Matrix Factorization (NMF)</strong>: NMF is a linear algebraic method for dimensionality reduction, which has been applied to topic modeling. NMF factorizes the document-term matrix into two non-negative lower-dimensional matrices representing document-topic and topic-term relationships. Although NMF lacks the probabilistic foundation of LDA, it often results in more interpretable topics.</p></li>
<li><p><strong>Correlated Topic Model (CTM)</strong>: CTM is an extension of LDA that allows topics to be correlated, capturing more complex relationships between topics. In CTM, the distribution of topics within documents is modeled using a logistic normal distribution instead of a Dirichlet distribution. This approach results in more realistic topic models, especially when topics are not independent in the underlying data.</p></li>
<li><p><strong>Dynamic Topic Models (DTM)</strong>: DTM is a class of topic models designed to analyze the evolution of topics over time. By modeling topic distributions as a function of time, DTMs can capture the temporal dynamics of topics in ordered document collections. This makes DTMs suitable for analyzing text data with an inherent temporal structure, such as news articles or scientific publications.</p></li>
</ol>
<p>Each of these topic modeling methodologies offers unique advantages and drawbacks, catering to different needs and data types. By understanding the underlying principles and assumptions of these methods, researchers and practitioners can select the most appropriate approach for their specific text data analysis tasks and effectively extract meaningful insights from large-scale unstructured document collections.</p>
</section>
<section id="probabilistic-latent-semantic-analysis-plsa">
<h2>Probabilistic Latent Semantic Analysis (pLSA)<a class="headerlink" href="#probabilistic-latent-semantic-analysis-plsa" title="Permalink to this heading">#</a></h2>
<p>Probabilistic Latent Semantic Analysis (pLSA) is a topic modeling technique that aims to uncover latent topics within a collection of documents by utilizing a generative probabilistic model. Introduced by Thomas Hofmann in 1999, pLSA is a precursor to the more popular Latent Dirichlet Allocation (LDA). It models the co-occurrence of words and documents as a mixture of multinomial distributions, attempting to capture the hidden structure that governs these co-occurrences.</p>
<p>In pLSA, each document is considered a mixture of topics, and each topic is represented as a distribution over words. The underlying assumption is that the observed words in a document are generated by first selecting a topic and then sampling a word from the corresponding word distribution of the selected topic. The goal of pLSA is to learn these latent topic distributions and the document-topic relationships that best explain the observed data.</p>
<p>The pLSA model is trained using the Expectation-Maximization (EM) algorithm, which iteratively refines the estimates of topic distributions and document-topic relationships. During the Expectation step, the algorithm computes the posterior probabilities of topics given the words and the current model parameters. In the Maximization step, the model parameters are updated to maximize the likelihood of the observed data given these posterior probabilities.</p>
<p>While pLSA has been successful in revealing latent topics in text data, it has some limitations:</p>
<ol class="arabic simple">
<li><p>Overfitting: pLSA is prone to overfitting due to the lack of regularization or priors in the model. This can result in poor generalization to new, unseen documents.</p></li>
<li><p>Parameter growth: The number of parameters in pLSA grows linearly with the number of documents, making it computationally expensive for large-scale document collections.</p></li>
<li><p>No generative model for new documents: pLSA does not provide an explicit generative model for creating topic distributions for new documents, which makes it difficult to apply the learned model to new data.</p></li>
</ol>
<p>Despite these limitations, pLSA laid the foundation for more advanced topic modeling techniques, such as Latent Dirichlet Allocation (LDA), which incorporates Dirichlet priors to address the overfitting issue and provides a more comprehensive generative model for document collections.</p>
</section>
<section id="latent-dirichlet-allocation-lda">
<h2>Latent Dirichlet Allocation (LDA)<a class="headerlink" href="#latent-dirichlet-allocation-lda" title="Permalink to this heading">#</a></h2>
<figure class="align-default" id="fig-lda">
<a class="reference internal image-reference" href="../../../_images/142.png"><img alt="../../../_images/142.png" src="../../../_images/142.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 39 </span><span class="caption-text">Latent Dirichlet Allocation</span><a class="headerlink" href="#fig-lda" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Latent Dirichlet Allocation (LDA) is a popular generative probabilistic model used for topic modeling. As the name suggests, it involves discovering hidden (latent) topics within a collection of documents, where Dirichlet refers to the type of probability distribution used for modeling. LDA assumes that each document is a mixture of topics, and each topic is a distribution over words. The main goal is to find these hidden topic distributions and their relationships to the documents.</p>
<p>In LDA, given an <span class="math notranslate nohighlight">\(N \times M\)</span> document-term count matrix <span class="math notranslate nohighlight">\(X\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> represents the number of documents and <span class="math notranslate nohighlight">\(M\)</span> denotes the number of unique terms or words in the corpus, we assume there are <span class="math notranslate nohighlight">\(K\)</span> topics to be discovered. The number of topics, <span class="math notranslate nohighlight">\(K\)</span>, is a tunable hyperparameter, and its optimal value can be found using methods like coherence score analysis.</p>
<p>LDA, similar to PCA and NMF, aims to factorize the document-term matrix <span class="math notranslate nohighlight">\(X\)</span> into two lower-dimensional matrices:</p>
<ol class="arabic simple">
<li><p>An <span class="math notranslate nohighlight">\(N \times K\)</span> document-topic matrix: This matrix represents the relationship between documents and topics, with each element indicating the contribution of a particular topic to a specific document.</p></li>
<li><p>A <span class="math notranslate nohighlight">\(K \times M\)</span> topic-term matrix: This matrix represents the relationship between topics and terms, with each element denoting the probability of a word belonging to a specific topic.</p></li>
</ol>
<p>LDA operates under the assumption that documents are generated through the following process:</p>
<ol class="arabic simple">
<li><p>For each document, choose a distribution over topics.</p></li>
<li><p>For each word in the document: a. Choose a topic according to the document’s distribution over topics. b. Choose a word from the topic’s distribution over words.</p></li>
</ol>
<p>By applying LDA, we can infer the hidden topic structure of the documents, making it easier to understand, navigate, and analyze large-scale unstructured text data. LDA has been widely used for various applications, including document clustering, text summarization, and information retrieval, among others.</p>
<figure class="align-default" id="fig-lda-example">
<a class="reference internal image-reference" href="../../../_images/161.png"><img alt="../../../_images/161.png" src="../../../_images/161.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 40 </span><span class="caption-text">LDA Example</span><a class="headerlink" href="#fig-lda-example" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>When applying Latent Dirichlet Allocation (LDA) to a set of questions, the goal is to discover the hidden topic structure that characterizes these questions. For instance, consider the following four questions:</p>
<ol class="arabic simple">
<li><p>How do football players stay safe?</p></li>
<li><p>What is the most hated NFL football team of all time?</p></li>
<li><p>Who is the greatest political leader in the world and why?</p></li>
<li><p>Why do people treat politics like it’s a football team or some kind of sport?</p></li>
</ol>
<p>Fitting LDA to these questions would result in topic assignments that might look like the following:</p>
<ul class="simple">
<li><p>Question 1: 100% Topic A</p></li>
<li><p>Question 2: 90% Topic A, 10% Topic C</p></li>
<li><p>Question 3: 100% Topic B</p></li>
<li><p>Question 4: 40% Topic A, 60% Topic B</p></li>
</ul>
<p>In this example, Topic A appears to be related to sports, while Topic B is associated with politics, and Topic C might represent a more specific aspect of football, such as team rivalries or sentiments.</p>
<p>Using these topic assignments, we can then infer the topics for each original question:</p>
<ol class="arabic simple">
<li><p>How do football players stay safe? (Sport)</p></li>
<li><p>What is the most hated NFL football team of all time? (Sport, with a small contribution from team rivalries or sentiments)</p></li>
<li><p>Who is the greatest political leader in the world and why? (Politics)</p></li>
<li><p>Why do people treat politics like it’s a football team or some kind of sport? (Sport, Politics)</p></li>
</ol>
<figure class="align-default" id="fig-lda-example-topics">
<a class="reference internal image-reference" href="../../../_images/171.png"><img alt="../../../_images/171.png" src="../../../_images/171.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 41 </span><span class="caption-text">LDA Example Topics</span><a class="headerlink" href="#fig-lda-example-topics" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>LDA represents each question as a probability distribution of topics and each topic as a probability distribution of words. In other words, every question can be seen as a mixture of topics, and every topic can be viewed as a mixture of words.</p>
<figure class="align-default" id="fig-lda-topic-dist">
<a class="reference internal image-reference" href="../../../_images/181.png"><img alt="../../../_images/181.png" src="../../../_images/181.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 42 </span><span class="caption-text">LDA Example Topic Distribution</span><a class="headerlink" href="#fig-lda-topic-dist" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>When fitting LDA to a collection of questions or documents, the algorithm attempts to find the best topic mix and word mix that explain the observed data. This process uncovers the latent topic structure in the data, providing valuable insights into the thematic relationships between questions or documents and facilitating more efficient text data exploration and analysis.</p>
</section>
<section id="non-negative-matrix-factorization-nmf">
<h2>Non-negative Matrix Factorization (NMF)<a class="headerlink" href="#non-negative-matrix-factorization-nmf" title="Permalink to this heading">#</a></h2>
<figure class="align-default" id="fig-nmf">
<a class="reference internal image-reference" href="../../../_images/191.png"><img alt="../../../_images/191.png" src="../../../_images/191.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 43 </span><span class="caption-text">Non-negative Matrix Factorization</span><a class="headerlink" href="#fig-nmf" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Non-negative Matrix Factorization (NMF) is a linear algebraic method used for dimensionality reduction and data representation. It has been widely applied in various fields, including image processing, signal processing, and text mining, where it has been utilized for topic modeling. NMF was introduced by Lee and Seung in 1999 as a method to decompose a non-negative data matrix into two non-negative lower-dimensional matrices that, when multiplied, approximate the original data matrix.</p>
<p>Given a non-negative document-term matrix <span class="math notranslate nohighlight">\(X\)</span> of size <span class="math notranslate nohighlight">\(N \times M\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of documents and <span class="math notranslate nohighlight">\(M\)</span> is the number of unique terms in the corpus, NMF aims to find two non-negative matrices <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(H\)</span> such that <span class="math notranslate nohighlight">\(X \approx WH\)</span>, where <span class="math notranslate nohighlight">\(W\)</span> is an <span class="math notranslate nohighlight">\(N \times K\)</span> document-topic matrix and <span class="math notranslate nohighlight">\(H\)</span> is a <span class="math notranslate nohighlight">\(K \times M\)</span> topic-term matrix. Here, <span class="math notranslate nohighlight">\(K\)</span> is the number of topics or latent factors.</p>
<p>The primary objective of NMF is to minimize the reconstruction error between the original matrix <span class="math notranslate nohighlight">\(X\)</span> and its approximation <span class="math notranslate nohighlight">\(WH\)</span>. This is achieved by iteratively updating the elements of <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(H\)</span> using multiplicative update rules or other optimization algorithms, such as gradient descent or alternating least squares.</p>
<p>Consider the following three questions:</p>
<ol class="arabic simple">
<li><p>How do football players stay safe?</p></li>
<li><p>What is the most hated NFL football team of all time?</p></li>
<li><p>Who is the greatest political leader in the world and why?</p></li>
</ol>
<p>To apply Non-negative Matrix Factorization (NMF) for topic modeling on these questions, we first create a term-document matrix that represents the frequency of terms in each question. This matrix will have rows representing unique terms and columns representing the questions.</p>
<p>Next, we choose the number of topics, k=2 in this case, and decompose the term-document matrix into two non-negative matrices: a term-topic matrix (n words by k topics) and a document-topic matrix (k topics by m original documents).</p>
<p>The term-topic matrix represents the relationship between terms and topics, where each topic is characterized by a distribution of words. The document-topic matrix represents the relationship between documents (questions) and topics, where each document is a mixture of topics with varying proportions.</p>
<figure class="align-default" id="fig-nmf-example">
<a class="reference internal image-reference" href="../../../_images/20.png"><img alt="../../../_images/20.png" src="../../../_images/20.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 44 </span><span class="caption-text">Non-negative Matrix Factorization Example</span><a class="headerlink" href="#fig-nmf-example" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>After applying NMF, we can examine the resulting term-topic and document-topic matrices to identify the underlying topics and their association with the original questions. For instance, we might find that Topic 1 is related to football and sports, while Topic 2 is related to politics and leadership. The document-topic matrix would then show the extent to which each question is associated with these topics, allowing us to interpret the hidden topic structure in the data.</p>
<figure class="align-default" id="fig-nmf-example2">
<a class="reference internal image-reference" href="../../../_images/213.png"><img alt="../../../_images/213.png" src="../../../_images/213.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 45 </span><span class="caption-text">Non-negative Matrix Factorization Example Result</span><a class="headerlink" href="#fig-nmf-example2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>NMF has several attractive properties that make it suitable for topic modeling:</p>
<ol class="arabic simple">
<li><p><strong>Non-negativity constraint</strong>: The non-negativity constraint on the factor matrices ensures that the resulting topics and their relationships to documents are interpretable, as they reflect additive combinations of the original features.</p></li>
<li><p><strong>Sparsity</strong>: NMF often results in sparse representations, where each document is associated with only a few topics, and each topic is characterized by a limited set of words. This sparsity promotes interpretability and simplifies the identification of meaningful topics.</p></li>
<li><p><strong>Flexibility</strong>: NMF can be adapted to different problem settings by incorporating additional constraints or using different objective functions, such as the Kullback-Leibler divergence or the Itakura-Saito divergence, to better capture the underlying structure of the data.</p></li>
</ol>
<p>However, NMF also has some limitations:</p>
<ol class="arabic simple">
<li><p><strong>Lack of probabilistic foundation</strong>: Unlike LDA, NMF is not based on a probabilistic generative model, which may limit its applicability in certain scenarios where probabilistic interpretations are desired.</p></li>
<li><p><strong>Non-unique solutions</strong>: NMF does not guarantee unique factorization, as there can be multiple combinations of <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(H\)</span> that approximate the original matrix <span class="math notranslate nohighlight">\(X\)</span>. This can lead to inconsistencies in the extracted topics.</p></li>
</ol>
<p>Despite these limitations, NMF remains a popular and useful technique for topic modeling, as it often provides interpretable and meaningful topic representations in a computationally efficient manner.</p>
</section>
<section id="correlated-topic-model-ctm">
<h2>Correlated Topic Model (CTM)<a class="headerlink" href="#correlated-topic-model-ctm" title="Permalink to this heading">#</a></h2>
<p>The Correlated Topic Model (CTM) is an extension of the Latent Dirichlet Allocation (LDA) model that takes into account the correlations between topics. CTM, like LDA, is a generative probabilistic model used to discover latent topics in a document collection. However, unlike LDA, which assumes that the topics are independent, CTM allows for modeling correlations between topics, making it better suited for capturing more complex topic structures in the documents.</p>
<section id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">#</a></h3>
<p>In LDA, the topic distribution of each document is generated from a Dirichlet distribution with a fixed concentration parameter. This assumption leads to the independence of topics in the LDA model. On the other hand, CTM assumes that the topic proportions for a document are generated from a logistic normal distribution, which allows for modeling the correlations between topics.</p>
<p>The generative process of CTM can be described as follows:</p>
<ol class="arabic simple">
<li><p>For each topic <span class="math notranslate nohighlight">\(k\)</span>, choose a distribution over words <span class="math notranslate nohighlight">\(\beta_k\)</span> from a Dirichlet distribution with parameter <span class="math notranslate nohighlight">\(\eta\)</span>.</p></li>
<li><p>For each document <span class="math notranslate nohighlight">\(d\)</span> in the corpus:</p>
<ul class="simple">
<li><p>a. Choose a topic distribution <span class="math notranslate nohighlight">\(\theta_d\)</span> from a logistic normal distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>.</p></li>
<li><p>b. For each word <span class="math notranslate nohighlight">\(w\)</span> in document <span class="math notranslate nohighlight">\(d\)</span>:</p>
<ul>
<li><p>i. Choose a topic <span class="math notranslate nohighlight">\(z_{d,w}\)</span> from the distribution <span class="math notranslate nohighlight">\(\theta_d\)</span>.</p></li>
<li><p>ii. Choose a word <span class="math notranslate nohighlight">\(w_{d,n}\)</span> from the distribution over words for the chosen topic <span class="math notranslate nohighlight">\(\beta_{z_{d,w}}\)</span>.</p></li>
</ul>
</li>
</ul>
</li>
</ol>
</section>
<section id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this heading">#</a></h3>
<p>Inference in CTM is more challenging than in LDA due to the use of the logistic normal distribution. Approximate inference methods, such as variational inference or Markov Chain Monte Carlo (MCMC) sampling, are often employed to estimate the model parameters.</p>
<p>Variational inference for CTM involves optimizing a lower bound on the log likelihood of the observed data using mean-field variational methods. This approach approximates the true posterior distribution with a simpler distribution that can be optimized iteratively to fit the data.</p>
</section>
<section id="applications">
<h3>Applications<a class="headerlink" href="#applications" title="Permalink to this heading">#</a></h3>
<p>Like LDA, CTM can be used in various text mining and natural language processing tasks, such as:</p>
<ul class="simple">
<li><p>Topic modeling: Discovering the underlying topics in a document collection.</p></li>
<li><p>Document classification: Assigning documents to categories based on their topic distribution.</p></li>
<li><p>Information retrieval: Improving search results by considering topic correlations.</p></li>
<li><p>Recommender systems: Recommending content based on topic similarity.</p></li>
</ul>
<p>CTM is particularly useful when there is a need to model complex relationships between topics, which can lead to more accurate and informative topic representations in the documents.</p>
</section>
</section>
<section id="dynamic-topic-models-dtm">
<h2>Dynamic Topic Models (DTM)<a class="headerlink" href="#dynamic-topic-models-dtm" title="Permalink to this heading">#</a></h2>
<p>Dynamic Topic Models (DTM) are an extension of traditional topic models like Latent Dirichlet Allocation (LDA) that incorporate a temporal aspect. The main idea behind DTM is to model how topics evolve over time in a document collection. This allows for the discovery of not only the latent topics but also how their word distributions and prevalence change across different time periods.</p>
<section id="id1">
<h3>Overview<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>DTM builds on the foundation of LDA by introducing a time component to the generative process. In DTM, the documents are divided into time slices, and a separate LDA model is generated for each time slice. The topics from adjacent time slices are connected through a state-space model, which captures the evolution of topics over time.</p>
<p>The generative process of DTM can be described as follows:</p>
<ol class="arabic simple">
<li><p>For each time slice <span class="math notranslate nohighlight">\(t\)</span>:</p>
<ul class="simple">
<li><p>a. For each topic <span class="math notranslate nohighlight">\(k\)</span>, choose a distribution over words <span class="math notranslate nohighlight">\(\beta_{t,k}\)</span> from a Dirichlet distribution with parameter <span class="math notranslate nohighlight">\(\eta\)</span>.</p></li>
<li><p>b. For each document <span class="math notranslate nohighlight">\(d\)</span> in time slice <span class="math notranslate nohighlight">\(t\)</span>:</p>
<ul>
<li><p>i. Choose a topic distribution <span class="math notranslate nohighlight">\(\theta_{t,d}\)</span> from a Dirichlet distribution with parameter <span class="math notranslate nohighlight">\(α\)</span>.</p></li>
<li><p>ii. For each word <span class="math notranslate nohighlight">\(w\)</span> in document <span class="math notranslate nohighlight">\(d\)</span>:</p>
<ol class="arabic simple">
<li><p>Choose a topic <span class="math notranslate nohighlight">\(z_{t,d,w}\)</span> from the distribution <span class="math notranslate nohighlight">\(\theta_{t,d}\)</span>.</p></li>
<li><p>Choose a word <span class="math notranslate nohighlight">\(w_{t,d,n}\)</span> from the distribution over words for the chosen topic <span class="math notranslate nohighlight">\(\beta_{t,z_{t,d,w}}\)</span>.</p></li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>The main difference between DTM and LDA lies in the choice of <span class="math notranslate nohighlight">\(\beta_{t,k}\)</span>, which models the evolution of topics across time slices. In DTM, the distribution over words for a topic at time <span class="math notranslate nohighlight">\(t\)</span> is influenced by the distribution over words for the same topic at time <span class="math notranslate nohighlight">\(t-1\)</span>.</p>
</section>
<section id="id2">
<h3>Inference<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>Inference in DTM is more challenging than in LDA due to the temporal dependencies between topics across different time slices. Approximate inference methods, such as variational inference or Markov Chain Monte Carlo (MCMC) sampling, are employed to estimate the model parameters. In particular, a specialized variational inference algorithm, called the Kalman variational inference, has been developed for DTM, which takes advantage of the state-space model structure to improve inference efficiency.</p>
</section>
<section id="id3">
<h3>Applications<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>DTM can be applied to various text mining and natural language processing tasks that require the analysis of temporal patterns in documents, such as:</p>
<ul class="simple">
<li><p>Temporal topic modeling: Discovering the latent topics and their evolution over time in a document collection.</p></li>
<li><p>Trend analysis: Identifying emerging or declining trends in a corpus.</p></li>
<li><p>Document classification: Assigning documents to categories based on their topic distribution and time information.</p></li>
<li><p>Information retrieval: Improving search results by considering both topic similarity and temporal relevance.</p></li>
</ul>
<p>DTM is especially useful when analyzing document collections with a strong temporal component, such as news articles, scientific publications, or social media data, where understanding the dynamics of topics is crucial for accurate analysis and interpretation.</p>
</section>
</section>
<section id="topic-modeling-example-using-nmf-and-svd">
<h2>Topic Modeling Example using NMF and SVD<a class="headerlink" href="#topic-modeling-example-using-nmf-and-svd" title="Permalink to this heading">#</a></h2>
<section id="the-problem">
<h3>The problem<a class="headerlink" href="#the-problem" title="Permalink to this heading">#</a></h3>
<p>In this example, we will perform topic modeling on a dataset of newsgroup documents using two matrix factorization techniques: Non-negative Matrix Factorization (NMF) and Singular Value Decomposition (SVD).</p>
<p><strong>Term-document matrix</strong>:</p>
<p>We start by creating a term-document matrix, which represents the frequency of terms in each document. The goal is to decompose this matrix into the product of one tall, thin matrix and one wide, short matrix (possibly with a diagonal matrix in between).</p>
<p>It is important to note that this representation does not consider word order or sentence structure and is an example of a <strong>bag of words</strong> approach.</p>
<figure class="align-default" id="fig-document-term">
<a class="reference internal image-reference" href="../../../_images/document_term.png"><img alt="../../../_images/document_term.png" src="../../../_images/document_term.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 46 </span><span class="caption-text">Term-Document Matrix (<a class="reference external" href="http://player.slideplayer.com/15/4528582/#">source</a>)</span><a class="headerlink" href="#fig-document-term" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="motivation">
<h3>Motivation<a class="headerlink" href="#motivation" title="Permalink to this heading">#</a></h3>
<p>The idea behind matrix factorization is to find an optimal way of representing the original term-document matrix using lower-dimensional matrices that capture the latent topic structure in the data.</p>
<p>We will use a dataset of newsgroup documents belonging to several different categories and find topics (groups of words) for them. Knowing the actual categories helps us evaluate whether the discovered topics make sense.</p>
<p>We will try this with two different matrix factorizations: <strong>Singular Value Decomposition (SVD)</strong> and <strong>Non-negative Matrix Factorization (NMF)</strong>.</p>
<ul class="simple">
<li><p><strong>Data source</strong>: The dataset used in this example is the 20 Newsgroups dataset, which contains 18,000 newsgroup posts across 20 topics. Newsgroups were popular discussion groups on Usenet in the 80s and 90s before the web took off. We will focus on four categories for this example: “alt.atheism”, “talk.religion.misc”, “comp.graphics”, and “sci.space”. To remove noise from the data, we will exclude headers, footers, and quotes from the posts.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># install the necessary packages</span>
<span class="o">%</span><span class="k">pip</span> install scikit-learn
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
Collecting scikit-learn
  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">9.8/9.8 MB</span> <span class=" -Color -Color-Red">54.8 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>a <span class=" -Color -Color-Cyan">0:00:01</span>
?25hRequirement already satisfied: joblib&gt;=1.1.1 in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from scikit-learn) (1.2.0)
Requirement already satisfied: scipy&gt;=1.3.2 in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from scikit-learn) (1.9.3)
Collecting threadpoolctl&gt;=2.0.0
  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)
Requirement already satisfied: numpy&gt;=1.17.3 in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from scikit-learn) (1.23.5)
Installing collected packages: threadpoolctl, scikit-learn
Successfully installed scikit-learn-1.2.2 threadpoolctl-3.1.0

<span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> A new release of pip is available: <span class=" -Color -Color-Red">23.0</span> -&gt; <span class=" -Color -Color-Green">23.0.1</span>
<span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> To update, run: <span class=" -Color -Color-Green">pip install --upgrade pip</span>
Note: you may need to restart the kernel to use updated packages.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">config</span> InlineBackend.figure_format=&#39;retina&#39;
<span class="c1"># import the necessary packages</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">decomposition</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">linalg</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;wordnet&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;omw-1.4&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package stopwords to /home/yjlee/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package wordnet to /home/yjlee/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /home/yjlee/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;alt.atheism&quot;</span><span class="p">,</span> <span class="s2">&quot;talk.religion.misc&quot;</span><span class="p">,</span> <span class="s2">&quot;comp.graphics&quot;</span><span class="p">,</span> <span class="s2">&quot;sci.space&quot;</span><span class="p">]</span>
<span class="n">remove</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;headers&quot;</span><span class="p">,</span> <span class="s2">&quot;footers&quot;</span><span class="p">,</span> <span class="s2">&quot;quotes&quot;</span><span class="p">)</span>
<span class="n">newsgroups_train</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span>
    <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span> <span class="n">remove</span><span class="o">=</span><span class="n">remove</span>
<span class="p">)</span>
<span class="n">newsgroups_test</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span>
    <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span> <span class="n">remove</span><span class="o">=</span><span class="n">remove</span>
<span class="p">)</span>
<span class="n">newsgroups_train</span><span class="o">.</span><span class="n">filenames</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">newsgroups_train</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((2034,), (2034,))
</pre></div>
</div>
</div>
</div>
<p>Let’s take a look at some of the data. Can you guess which category these messages belong to?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Hi,

I&#39;ve noticed that if you only save a model (with all your mapping planes
positioned carefully) to a .3DS file that when you reload it after restarting
3DS, they are given a default position and orientation.  But if you save
to a .PRJ file their positions/orientation are preserved.  Does anyone
know why this information is not stored in the .3DS file?  Nothing is
explicitly said in the manual about saving texture rules in the .PRJ file. 
I&#39;d like to be able to read the texture rule information, does anyone have 
the format for the .PRJ file?

Is the .CEL file format available from somewhere?

Rych


Seems to be, barring evidence to the contrary, that Koresh was simply
another deranged fanatic who thought it neccessary to take a whole bunch of
folks with him, children and all, to satisfy his delusional mania. Jim
Jones, circa 1993.


Nope - fruitcakes like Koresh have been demonstrating such evil corruption
for centuries.
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">target</span></code> attribute is the integer index of the category.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">target_names</span><span class="p">)[</span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">3</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;comp.graphics&#39;, &#39;talk.religion.misc&#39;, &#39;sci.space&#39;], dtype=&#39;&lt;U18&#39;)
</pre></div>
</div>
</div>
</div>
<p>In this example, we will use NMF and SVD to decompose the term-document matrix and extract latent topics from the newsgroup documents. By comparing the discovered topics with the actual categories, we can evaluate the effectiveness of these matrix factorization techniques in uncovering meaningful topic structures in text data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1, 3, 2, 0, 2, 0, 2, 1, 2, 1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_topics</span><span class="p">,</span> <span class="n">num_top_words</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Stopwords</strong></p>
<p>Stopwords are extremely common words that appear to have little value in helping to identify or match documents based on user needs. In text processing and information retrieval, these words are often excluded from the analysis to reduce noise and improve efficiency. Examples of stopwords include “a”, “an”, “the”, “and”, and “in”.</p>
<p>Over time, the trend in information retrieval systems has shifted from using large stop lists (200-300 terms) to very small stop lists (7-12 terms), and eventually to not using stop lists at all. Web search engines typically do not use stop lists.</p>
<p><strong>Stemming and Lemmatization</strong></p>
<p>Stemming and lemmatization are text preprocessing techniques that aim to generate the root form of words. This is particularly useful in information retrieval, as it helps group similar words together and improves search accuracy. Consider the following words:</p>
<ul class="simple">
<li><p>organize, organizes, and organizing</p></li>
<li><p>democracy, democratic, and democratization</p></li>
</ul>
<p>Both stemming and lemmatization aim to reduce these words to their base forms, making it easier to identify their similarities.</p>
<p>Lemmatization uses linguistic rules and knowledge about a language to generate the root form of words. The resulting tokens are actual words that carry meaning in the language. In contrast, stemming is a simpler and more heuristic-based approach that truncates words by chopping off their ends. The resulting tokens may not always be real words. As a result, stemming is faster than lemmatization but may not be as accurate.</p>
<p>In summary, stopwords, stemming, and lemmatization are essential techniques in text processing and information retrieval. They help reduce noise, improve efficiency, and enhance the overall effectiveness of search and topic modeling tasks by simplifying the input text and generating meaningful word representations.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>

<span class="n">STOPWORDS</span> <span class="o">=</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">STOPWORDS</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">STOPWORDS</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>179
[&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;, &#39;ourselves&#39;, &#39;you&#39;, &quot;you&#39;re&quot;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">stem</span>

<span class="n">wnl</span> <span class="o">=</span> <span class="n">stem</span><span class="o">.</span><span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="n">porter</span> <span class="o">=</span> <span class="n">stem</span><span class="o">.</span><span class="n">porter</span><span class="o">.</span><span class="n">PorterStemmer</span><span class="p">()</span>

<span class="n">word_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;feet&quot;</span><span class="p">,</span> <span class="s2">&quot;foot&quot;</span><span class="p">,</span> <span class="s2">&quot;foots&quot;</span><span class="p">,</span> <span class="s2">&quot;footing&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">wnl</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_list</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;foot&#39;, &#39;foot&#39;, &#39;foot&#39;, &#39;footing&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">porter</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_list</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;feet&#39;, &#39;foot&#39;, &#39;foot&#39;, &#39;foot&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="data-processing">
<h3>Data Processing<a class="headerlink" href="#data-processing" title="Permalink to this heading">#</a></h3>
<p>In this step, we will process the newsgroup data by extracting word counts using scikit-learn’s <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code>. This method will create a term-document matrix representing the frequency of terms in each document. In a later lesson, we will learn how to create our own version of <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> to gain a better understanding of what happens under the hood.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="c1"># Initialize the CountVectorizer with English stopwords</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>

<span class="c1"># Fit the vectorizer to the newsgroups_train.data and transform the text data</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span>
    <span class="n">newsgroups_train</span><span class="o">.</span><span class="n">data</span>
<span class="p">)</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span>  <span class="c1"># (documents, vocab)</span>

<span class="c1"># Print the shape of the term-document matrix (documents, vocabulary)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Retrieve the vocabulary from the vectorizer</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>

<span class="c1"># Print the shape of the vocabulary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Display the last 10 terms in the vocabulary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vocab</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2034 (2034, 26576)
(26576,)
[&#39;zurich&#39; &#39;zurvanism&#39; &#39;zus&#39; &#39;zvi&#39; &#39;zwaartepunten&#39; &#39;zwak&#39; &#39;zwakke&#39; &#39;zware&#39;
 &#39;zwarte&#39; &#39;zyxel&#39;]
</pre></div>
</div>
</div>
</div>
<p>In this code snippet, we first import <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> from scikit-learn’s <code class="docutils literal notranslate"><span class="pre">feature_extraction.text</span></code> module. We then initialize the <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> object, specifying that we want to remove English stopwords from the text data. Next, we fit the vectorizer to the training data and transform the text data into a term-document matrix. Finally, we retrieve the vocabulary from the vectorizer and display its shape and the last 10 terms.</p>
<p>By applying <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> to the newsgroup data, we can transform the text data into a structured format suitable for further analysis using NMF and SVD.</p>
</section>
<section id="singular-value-decomposition-svd">
<h3>Singular Value Decomposition (SVD)<a class="headerlink" href="#singular-value-decomposition-svd" title="Permalink to this heading">#</a></h3>
<p>SVD is a matrix factorization technique that can be used to identify latent topics in a term-document matrix. We expect the topics to be <strong>orthogonal</strong> since words that appear frequently in one topic should appear less frequently in others, making them suitable for separating topics.</p>
<p>The SVD algorithm factorizes a matrix into one matrix with <strong>orthogonal columns</strong> and one with <strong>orthogonal rows</strong>, along with a diagonal matrix that contains the <strong>relative importance</strong> of each factor. SVD is an <strong>exact decomposition</strong> as the resulting matrices fully cover the original matrix. It is widely used in data science for tasks such as semantic analysis, collaborative filtering, data compression, and principal component analysis. Latent Semantic Analysis (LSA) also employs SVD.</p>
<figure class="align-default" id="fig-svd-fb">
<a class="reference internal image-reference" href="../../../_images/svd_fb.png"><img alt="../../../_images/svd_fb.png" src="../../../_images/svd_fb.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 47 </span><span class="caption-text">Fast Randomized SVD (<a class="reference external" href="https://research.fb.com/fast-randomized-svd/">source</a>)</span><a class="headerlink" href="#fig-svd-fb" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Latent Semantic Analysis (LSA) uses SVD.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span> 
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">linalg</span>

<span class="c1"># Perform SVD on the term-document matrix</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Print the shapes of the resulting matrices</span>
<span class="nb">print</span><span class="p">(</span><span class="n">U</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Vh</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2034, 2034) (2034,) (2034, 26576)
CPU times: user 9min 13s, sys: 7min 22s, total: 16min 35s
Wall time: 17.8 s
</pre></div>
</div>
</div>
</div>
<p>In this code snippet, we import the linalg module from SciPy and use its svd function to perform SVD on the term-document matrix. We then print the shapes of the resulting matrices U, s, and Vh.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_top_words</span> <span class="o">=</span> <span class="mi">8</span>


<span class="k">def</span> <span class="nf">show_topics</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="c1"># Helper function to retrieve top words for each topic</span>
    <span class="n">top_words</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">t</span><span class="p">)[:</span> <span class="o">-</span><span class="n">num_top_words</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="n">topic_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">top_words</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">a</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">[</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">topic_words</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display the top topics extracted using SVD</span>
<span class="n">show_topics</span><span class="p">(</span><span class="n">Vh</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;critus ditto propagandist surname galacticentric kindergarten surreal imaginative&#39;,
 &#39;jpeg gif file color quality image jfif format&#39;,
 &#39;graphics edu pub mail 128 3d ray ftp&#39;,
 &#39;jesus god matthew people atheists atheism does graphics&#39;,
 &#39;image data processing analysis software available tools display&#39;,
 &#39;god atheists atheism religious believe religion argument true&#39;,
 &#39;space nasa lunar mars probe moon missions probes&#39;,
 &#39;image probe surface lunar mars probes moon orbit&#39;,
 &#39;argument fallacy conclusion example true ad argumentum premises&#39;,
 &#39;space larson image theory universe physical nasa material&#39;]
</pre></div>
</div>
</div>
</div>
<p>In this part, we define a function <code class="docutils literal notranslate"><span class="pre">show_topics</span></code> to display the top words associated with each topic. We then use this function to display the top topics extracted using SVD. The results match the expected clusters, even though this is an <strong>unsupervised algorithm</strong> - meaning we never provided information about the grouping of our documents during the process.</p>
</section>
<section id="id4">
<h3>Non-negative Matrix Factorization (NMF)<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p>In contrast to constraining our factors to be <em>orthogonal</em> as in SVD, we can constrain them to be <em>non-negative</em>. NMF is a factorization of a non-negative data set <span class="math notranslate nohighlight">\(V\)</span> into non-negative matrices <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(H\)</span>: <span class="math notranslate nohighlight">\(V=WH\)</span>. Often, positive factors are <strong>more easily interpretable</strong>, contributing to NMF’s popularity.</p>
<p>NMF is a non-exact factorization that factors a matrix into one skinny positive matrix and one short positive matrix. It is NP-hard and non-unique, with various versions created by adding different constraints.</p>
<figure class="align-default" id="fig-nmf-doc">
<a class="reference internal image-reference" href="../../../_images/nmf_doc.png"><img alt="../../../_images/nmf_doc.png" src="../../../_images/nmf_doc.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 48 </span><span class="caption-text">Non-negative Matrix Factorization (<a class="reference external" href="http://perso.telecom-paristech.fr/~essid/teach/NMF_tutorial_ICME-2014.pdf">source</a>)</span><a class="headerlink" href="#fig-nmf-doc" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="nmf-applications">
<h4>NMF Applications<a class="headerlink" href="#nmf-applications" title="Permalink to this heading">#</a></h4>
<p>NMF has been applied in various fields, such as:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py">Face Decompositions</a></p></li>
<li><p><a class="reference external" href="http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/">Collaborative Filtering, eg movie recommendations</a></p></li>
<li><p><a class="reference external" href="https://pdfs.semanticscholar.org/cc88/0b24791349df39c5d9b8c352911a0417df34.pdf">Audio source separation</a></p></li>
<li><p><a class="reference external" href="http://ieeexplore.ieee.org/document/1532909/">Chemistry</a></p></li>
<li><p><a class="reference external" href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0485-4">Bioinformatics</a></p></li>
<li><p><a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2623306/">Gene Expression</a></p></li>
</ul>
</section>
<section id="nmf-using-scikit-learn">
<h4>NMF using scikit-learn<a class="headerlink" href="#nmf-using-scikit-learn" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">decomposition</span>

<span class="c1"># Perform NMF on the term-document matrix</span>
<span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">vectors</span><span class="o">.</span><span class="n">shape</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># num topics</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">W1</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">vectors</span><span class="p">))</span>
<span class="n">H1</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">components_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 1min 36s, sys: 1min 33s, total: 3min 9s
Wall time: 2.86 s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display the top topics extracted using NMF</span>
<span class="n">show_topics</span><span class="p">(</span><span class="n">H1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;jpeg image gif file color images format quality&#39;,
 &#39;edu graphics pub mail 128 ray ftp send&#39;,
 &#39;space launch satellite nasa commercial satellites year market&#39;,
 &#39;jesus god people matthew atheists does atheism said&#39;,
 &#39;image data available software processing ftp edu analysis&#39;]
</pre></div>
</div>
</div>
</div>
<p>In this code snippet, we import the <code class="docutils literal notranslate"><span class="pre">decomposition</span></code> module from scikit-learn and use its <code class="docutils literal notranslate"><span class="pre">NMF</span></code> class to perform NMF on the term-document matrix. We then display the top topics extracted using NMF.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>For NMF, the matrix needs to be at least as tall as it is wide, or we get an error with <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code>.</p></li>
<li><p>You can use <code class="docutils literal notranslate"><span class="pre">min_df</span></code> in <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> to only consider words that appear in at least k of the split texts.</p></li>
</ul>
</div>
</section>
</section>
<section id="truncated-svd">
<h3>Truncated SVD<a class="headerlink" href="#truncated-svd" title="Permalink to this heading">#</a></h3>
<p>When we calculated NMF, we saved a lot of time by only calculating the subset of columns we were interested in. We can achieve a similar benefit with SVD by using truncated SVD. Truncated SVD focuses on the vectors corresponding to the <strong>largest</strong> singular values, which can save computation time.</p>
<section id="shortcomings-of-classical-algorithms-for-decomposition">
<h4>Shortcomings of classical algorithms for decomposition:<a class="headerlink" href="#shortcomings-of-classical-algorithms-for-decomposition" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Matrices can be “stupendously big.”</p></li>
<li><p>Data are often <strong>missing or inaccurate</strong>. Spending extra computational resources may not be worthwhile when the imprecision of input limits the precision of the output.</p></li>
<li><p><strong>Data transfer</strong> plays a significant role in the time complexity of algorithms. Techniques that require fewer passes over the data might be faster, even if they require more floating point operations (flops).</p></li>
<li><p>It is essential to take advantage of <strong>GPUs</strong>.</p></li>
</ul>
</section>
<section id="advantages-of-randomized-algorithms">
<h4>Advantages of randomized algorithms:<a class="headerlink" href="#advantages-of-randomized-algorithms" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Inherently stable.</p></li>
<li><p>Performance guarantees do not depend on subtle spectral properties.</p></li>
<li><p>The required matrix-vector products can be performed in parallel.</p></li>
</ul>
<p>(source: <a class="reference external" href="https://arxiv.org/abs/0909.4061">Halko</a>)</p>
</section>
<section id="timing-comparison">
<h4>Timing comparison<a class="headerlink" href="#timing-comparison" title="Permalink to this heading">#</a></h4>
<p>In the following code snippets, we compare the computation time for full SVD and randomized SVD:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span> 
<span class="c1"># Full SVD</span>
<span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 10min 29s, sys: 10min 50s, total: 21min 20s
Wall time: 20.7 s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span> 
<span class="c1"># Randomized SVD</span>
<span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">randomized_svd</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 1min 1s, sys: 2min 30s, total: 3min 32s
Wall time: 5.91 s
</pre></div>
</div>
</div>
</div>
<p>These code snippets demonstrate the time difference between full SVD and randomized SVD, showing that the latter can be more efficient in certain cases.</p>
</section>
</section>
</section>
<section id="document-clustering">
<h2>Document Clustering<a class="headerlink" href="#document-clustering" title="Permalink to this heading">#</a></h2>
<p>Document clustering is a technique used in text mining and natural language processing to group documents based on their content or other features. This can help in organizing, summarizing, and extracting meaningful information from large collections of text. Some common applications include topic modeling, document classification, and information retrieval.</p>
<section id="cosine-similarity">
<h3>Cosine Similarity<a class="headerlink" href="#cosine-similarity" title="Permalink to this heading">#</a></h3>
<p>Cosine similarity is a popular similarity measure used in text mining and information retrieval. It measures the cosine of the angle between two non-zero vectors, with a range between -1 and 1. In the context of document clustering, each document is represented as a vector of term frequencies or other weights, and the cosine similarity quantifies the similarity between two documents.</p>
<p>The cosine similarity between two vectors A and B can be calculated as follows:</p>
<div class="math notranslate nohighlight">
\[
\text{cosine\_similarity}(A, B) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^n A_iB_i}{\sqrt{\sum_{i=1}^n A_i^2}\sqrt{\sum_{i=1}^n B_i^2}}
\]</div>
<figure class="align-default" id="fig-cosine-sim">
<a class="reference internal image-reference" href="../../../_images/214.png"><img alt="../../../_images/214.png" src="../../../_images/214.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 49 </span><span class="caption-text">Cosine Similarity Example</span><a class="headerlink" href="#fig-cosine-sim" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-projected-docs">
<a class="reference internal image-reference" href="../../../_images/3-800x852.png"><img alt="../../../_images/3-800x852.png" src="../../../_images/3-800x852.png" style="width: 60%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 50 </span><span class="caption-text">Projecting Documents onto a 3D Space</span><a class="headerlink" href="#fig-projected-docs" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="dimensionality-reduction">
<h3>Dimensionality Reduction<a class="headerlink" href="#dimensionality-reduction" title="Permalink to this heading">#</a></h3>
<p>Before applying clustering algorithms to document collections, it is often beneficial to perform dimensionality reduction. High-dimensional data can be noisy and may result in poor clustering performance due to the curse of dimensionality. Dimensionality reduction techniques, such as PCA, SVD, or t-SNE, can help in reducing the number of dimensions while preserving the essential structure and relationships in the data. This can lead to more efficient and accurate clustering results.</p>
</section>
<section id="clustering-algorithms">
<h3>Clustering Algorithms<a class="headerlink" href="#clustering-algorithms" title="Permalink to this heading">#</a></h3>
<p>There are several clustering algorithms that can be applied to document clustering. Some popular ones include:</p>
<ol class="arabic simple">
<li><p><strong>K-means</strong>: K-means is a widely-used partitioning clustering algorithm that aims to minimize the sum of squared distances between data points and their respective cluster centroids. K-means requires the number of clusters (K) to be specified beforehand and is sensitive to the initial placement of cluster centroids.</p></li>
<li><p><strong>DBSCAN</strong>: Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a density-based clustering algorithm that identifies clusters based on the density of data points in a region. DBSCAN does not require the number of clusters to be specified beforehand and can handle noise and outliers effectively.</p></li>
<li><p><strong>Agglomerative Hierarchical Clustering</strong>: This is a bottom-up approach to hierarchical clustering that starts with each data point as its own cluster and iteratively merges the closest pair of clusters until only one cluster remains. The result is a tree-like structure, called a dendrogram, that can be cut at different levels to obtain the desired number of clusters.</p></li>
<li><p><strong>Latent Dirichlet Allocation (LDA)</strong>: LDA is a generative probabilistic model for collections of discrete data, such as text corpora. It is particularly useful for topic modeling, where the goal is to discover the underlying topics in a document collection. LDA represents documents as mixtures of topics and topics as mixtures of words, and it learns these representations using a Dirichlet prior.</p></li>
</ol>
<p>To perform document clustering, first preprocess the text data (e.g., tokenization, stemming, and removing stop words), convert the documents into a suitable vector representation (e.g., TF-IDF or word embeddings), and then apply the desired clustering algorithm to group similar documents together.</p>
</section>
</section>
<section id="next">
<h2>Next<a class="headerlink" href="#next" title="Permalink to this heading">#</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l1"><a class="reference internal" href="coherence-practice.html">Topic Coherence in Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="tomotopy.html">Topic Modeling Tools - Tomotopy</a></li>
</ul>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/nlp_intro/topic"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../datasets/lab-dart.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Lab: Crawling DART Data</p>
      </div>
    </a>
    <a class="right-next"
       href="coherence.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Topic Coherence Measures</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-methodologies">Topic Modeling Methodologies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-latent-semantic-analysis-plsa">Probabilistic Latent Semantic Analysis (pLSA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-dirichlet-allocation-lda">Latent Dirichlet Allocation (LDA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-negative-matrix-factorization-nmf">Non-negative Matrix Factorization (NMF)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlated-topic-model-ctm">Correlated Topic Model (CTM)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">Applications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-topic-models-dtm">Dynamic Topic Models (DTM)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Applications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-example-using-nmf-and-svd">Topic Modeling Example using NMF and SVD</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem">The problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-processing">Data Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#singular-value-decomposition-svd">Singular Value Decomposition (SVD)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Non-negative Matrix Factorization (NMF)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nmf-applications">NMF Applications</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nmf-using-scikit-learn">NMF using scikit-learn</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#truncated-svd">Truncated SVD</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#shortcomings-of-classical-algorithms-for-decomposition">Shortcomings of classical algorithms for decomposition:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-randomized-algorithms">Advantages of randomized algorithms:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#timing-comparison">Timing comparison</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-clustering">Document Clustering</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cosine-similarity">Cosine Similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction">Dimensionality Reduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-algorithms">Clustering Algorithms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next">Next</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://entelecheia.me" target="_blank">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>