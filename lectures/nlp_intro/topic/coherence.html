

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Topic Coherence Measures &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../../_static/tabs.js"></script>
    <script src="../../../_static/js/hoverxref.js"></script>
    <script src="../../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/chatgpt.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/nlp_intro/topic/coherence';</script>
    <link rel="canonical" href="https://lecture.entelecheia.ai/lectures/nlp_intro/topic/coherence.html" />
    <link rel="shortcut icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Topic Coherence in Practice" href="coherence-practice.html" />
    <link rel="prev" title="Topic Modeling" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content"><p>
  <strong>Announcement:</strong>
  You can install the accompanying AI tutor <a href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd" target="_blank">here</a>.
  <a class="reference external" href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd" target="_blank"><img alt="chrome-web-store-image" src="https://img.shields.io/chrome-web-store/v/lfgfgbomindbccgidgalhhndggddpagd" /></a>
</p>
</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Introduction to NLP</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../intro/index.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research/index.html">Research Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lm/index.html">Language Models</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../datasets/index.html">Datasets</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../datasets/corpus.html">Corpus and Text Data Collection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../datasets/lab-dart.html">Lab: Crawling DART Data</a></li>
</ul>
</li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="index.html">Topic Modeling</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Topic Coherence Measures</a></li>
<li class="toctree-l3"><a class="reference internal" href="coherence-practice.html">Topic Coherence in Practice</a></li>
<li class="toctree-l3"><a class="reference internal" href="tomotopy.html">Tomotopy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../sentiments/index.html">Sentiment Analysis</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tokenization/index.html">Tokenization</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../tokenization/segmentation.html">Word Segmentation and Association</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../embeddings/index.html">Word Embeddings</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_deep/index.html">Deep Learning for NLP</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/llms/index.html">Large Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/zeroshot.html">Zero Shot and Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/decoding.html">Decoding and Search Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/plms.html">Pretrained Language Models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/transformers/index.html">Transformers</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/bert.html">BERT: Bidirectional Encoder Representations from Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/bertviz.html">BERT: Visualizing Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/datasets/index.html">Datasets</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/datasets/mc4.html">mC4 Dataset</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/tokenization/index.html">Tokenization</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/lab-train-tokenizers.html">Lab: Training Tokenizers</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/training/index.html">Training Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-pretraining.html">Lab: Pretraining Language Models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/chatbots/index.html">Conversational AI and Chatbots</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/chatbots/rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/chatbots/detectGPT.html">How to Spot Machine-Written Texts</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_advances/index.html">Advances in AI and NLP</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/thesis.html">Writing a Thesis</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_advances/gpt/index.html">Generative Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/detectGPT.html">DetectGPT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/gpt4.html">GPT-4</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/camelids.html">Meet the Camelids: A Family of LLMs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/sam/index.html">Segment Everything</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../aiart/index.html">AI Art (Generative AI)</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/intro/index.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/brave/index.html">A Brave New World</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/aiart/index.html">Art and Music in Light of AI</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../aiart/text-to-image/index.html">Text-to-Image Models</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/imagen.html">Imagen</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/motion-capture-and-synthesis/index.html">Motion Capture and Motion Synthesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/robot/index.html">Robot Drawing System</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/devops/index.html">DevOps</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/gitops.html">GitOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/devsecops.html">DevSecOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/llmops.html">LLMOps</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/dotfiles/index.html">Dotfiles</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai">Dotfiles Repository</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/dotfiles/dotdrop.html">Dotdrop</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dotdrop/">Dotdrop Setup Script</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/github/">GitHub Setup Scripts</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/ssh-gpg-age/">SSH, GPG, and Age Setup Scripts</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/pass/">Pass and Passage Scripts</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/doppler/">Doppler Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dockerfiles/">Dockerfiles Scripts</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/security/index.html">Security Management</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/auth-enc-sign.html">Authentication, Encryption, and Signing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/age-gpg-ssh.html">SSH, GPG, and AGE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/pass.html">Unix Password Managers</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/github/index.html">Github’s Fork &amp; Pull Workflow</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/template.html">Project Templating Tools</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-python.entelecheia.ai/">Hyperfast Python Template</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-template.entelecheia.ai/">Hyperfast Template</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/containerization/index.html">Containerization</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/docker.html">Docker</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/containerd.html"><code class="docutils literal notranslate"><span class="pre">containerd</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/simple-pipeline/index.html">Simple MLOps Pipeline</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../ds/index.html">Data Science for Economics and Finance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../about/lecture-bot.html">LectureBot for ἐντελέχεια.άι</a></li>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference external" href="https://course.entelecheia.ai">course.entelecheia.ai</a></li>
<li class="toctree-l1"><a class="reference external" href="https://research.entelecheia.ai">research.entelecheia.ai</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about/index.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about/execution.html">Execution statistics</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/lectures/nlp_intro/topic/coherence.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Topic Coherence Measures</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling">Topic Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-topics">Evaluating Topics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-coherence">Topic Coherence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#segmentation">1. Segmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-calculation">2. Probability Calculation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confirmation-measure">3. Confirmation Measure</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#direct-confirmation-measure">Direct Confirmation Measure</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#indirect-confirmation-measure">Indirect Confirmation Measure</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#aggregation">Aggregation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean">Mean</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#median">Median</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric-mean">Geometric Mean</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-everything-together">Putting Everything Together</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-pointwise-mutual-information">Understanding Pointwise Mutual Information</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pmi">PMI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalized-pointwise-mutual-information">Normalized Pointwise Mutual Information</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-cosine-similarity">Understanding Cosine Similarity</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="topic-coherence-measures">
<h1>Topic Coherence Measures<a class="headerlink" href="#topic-coherence-measures" title="Permalink to this heading">#</a></h1>
<a class="bg-primary mb-1 reference internal image-reference" href="../../../_images/entelecheia_topic_coherence.png"><img alt="Topic Coherence" class="bg-primary mb-1 align-center" src="../../../_images/entelecheia_topic_coherence.png" style="width: 70%;" /></a>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>Topic coherence represents the overall interpretability of topics and is used to assess their quality. It is essential in topic modeling, a technique that aims to explain a collection of documents as a mixture of topics. Evaluating topics based on their interpretability is necessary, as mathematically optimal topics are not always human-readable. Topic coherence is a measure of interpretability and is used to evaluate the quality of topics. It tries to represent the degree of semantic similarity between high scoring words in a topic.</p>
</section>
<section id="topic-modeling">
<h2>Topic Modeling<a class="headerlink" href="#topic-modeling" title="Permalink to this heading">#</a></h2>
<p>Topic modeling aims to explain a collection of documents as a mixture of topics, where each topic is a distribution over words, and each document is a distribution over topics. The goal of topic modeling is to find the topics and their distributions over words and documents. The assumption behind topic modeling is that:</p>
<ul class="simple">
<li><p>A text (document) is composed of several topics.</p></li>
<li><p>A topic is composed of several words.</p></li>
</ul>
</section>
<section id="evaluating-topics">
<h2>Evaluating Topics<a class="headerlink" href="#evaluating-topics" title="Permalink to this heading">#</a></h2>
<p>Evaluating topics using topic coherence is essential for determining the quality and interpretability of the topics generated by topic modeling algorithms. Let’s take a look at a few simple examples to better understand this concept.</p>
<p>Suppose a topic modeling algorithm has generated the following topics from a given document collection:</p>
<ul class="simple">
<li><p>Topic 1: <code class="docutils literal notranslate"><span class="pre">['coffee',</span> <span class="pre">'espresso',</span> <span class="pre">'cappuccino',</span> <span class="pre">'latte']</span></code></p></li>
<li><p>Topic 2: <code class="docutils literal notranslate"><span class="pre">['gardening',</span> <span class="pre">'plants',</span> <span class="pre">'flowers',</span> <span class="pre">'soil']</span></code></p></li>
<li><p>Topic 3: <code class="docutils literal notranslate"><span class="pre">['house',</span> <span class="pre">'red',</span> <span class="pre">'building',</span> <span class="pre">'green']</span></code></p></li>
</ul>
<p>In these examples, Topic 1 and Topic 2 seem to be coherent and easily interpretable because the words within each topic are closely related semantically. Topic 1 is related to different types of coffee, while Topic 2 is about gardening and plants.</p>
<p>However, Topic 3 is less coherent and harder to interpret. Although ‘house’ and ‘building’ are related, the words ‘red’ and ‘green’ do not seem to fit well within the topic. The lack of coherence in Topic 3 could indicate that the topic modeling algorithm might need further tuning or that the document collection might contain some noise.</p>
<p>Using topic coherence measures, we can quantify the coherence of each topic and compare them. High coherence scores would indicate that the topics are easily interpretable and semantically related, whereas low coherence scores would signify that the topics are less interpretable and might require further investigation.</p>
</section>
<section id="topic-coherence">
<h2>Topic Coherence<a class="headerlink" href="#topic-coherence" title="Permalink to this heading">#</a></h2>
<p>Topic coherence assesses how well a topic is supported by a text corpus. It uses statistics and probabilities drawn from the text corpus to measure the coherence of a topic, focusing on the word’s context. Topic coherence depends on both the words in a topic and the reference corpus.</p>
<figure class="align-default" id="fig-topic-coherence">
<a class="reference internal image-reference" href="../../../_images/topic_coherence.png"><img alt="../../../_images/topic_coherence.png" src="../../../_images/topic_coherence.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 51 </span><span class="caption-text">Topic Coherence</span><a class="headerlink" href="#fig-topic-coherence" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>A general structure for topic coherence measures has been proposed by Röder, M. et al <span id="id1">[<a class="reference internal" href="../../../about/index.html#id14" title="Michael Röder, Andreas Both, and Alexander Hinneburg. Exploring the space of topic coherence measures. In Proceedings of the eighth ACM international conference on Web search and data mining, 399–408. 2015.">Röder <em>et al.</em>, 2015</a>]</span>. It consists of three components:</p>
<ol class="arabic simple">
<li><p>Segmentation</p></li>
<li><p>Probability Calculation</p></li>
<li><p>Confirmation Measure</p></li>
<li><p>Aggregation</p></li>
</ol>
<figure class="align-default" id="fig-topic-coherence-structure">
<a class="reference internal image-reference" href="../../../_images/topic_coherence_structure.png"><img alt="../../../_images/topic_coherence_structure.png" src="../../../_images/topic_coherence_structure.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 52 </span><span class="caption-text">Topic Coherence Structure</span><a class="headerlink" href="#fig-topic-coherence-structure" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="segmentation">
<h3>1. Segmentation<a class="headerlink" href="#segmentation" title="Permalink to this heading">#</a></h3>
<p>The segmentation module is a crucial step in the coherence measurement process, as it creates pairs of word subsets used to assess the coherence of a topic. Given a topic <span class="math notranslate nohighlight">\(t\)</span> represented by its top-n most important words <span class="math notranslate nohighlight">\(W = \{w_1, w_2, \dots, w_n\}\)</span>, the application of a segmentation <span class="math notranslate nohighlight">\(S\)</span> results in a set of subset pairs derived from <span class="math notranslate nohighlight">\(W\)</span>:</p>
<div class="math notranslate nohighlight">
\[
S = \{(W^{\prime}, W^*) | W^{\prime}, W^* \subseteq W\}
\]</div>
<p>Segmentation can be understood as the method we use to combine words in a topic before evaluating their relationships.</p>
<p>For instance, the S-one-one segmentation requires the creation of pairs of distinct words. If <span class="math notranslate nohighlight">\(W = \{w_1, w_2, \dots, w_n\}\)</span>, then the S-one-one segmentation generates the following pairs:</p>
<div class="math notranslate nohighlight">
\[ S = \{(w_1, w_2), (w_1, w_3), \dots, (w_1, w_n), (w_2, w_3), \dots, (w_2, w_n), \dots, (w*{n-1}, w_n)\} \]</div>
<p>By using this approach, our model computes the final coherence score based on the relationship between each word in the topic and the other words in the topic.</p>
<p>Another example is the S-one-all segmentation, which entails creating pairs of each word with all other words. Applying it to <span class="math notranslate nohighlight">\(W\)</span> yields the following pairs:</p>
<div class="math notranslate nohighlight">
\[ S = \{(\{w_1\}, \{w_2, w_3, \dots, w_n\}), (\{w_2\}, \{w_1, w_3, \dots, w_n\}), \dots, (\{w_n\}, \{w_1, w_2, \dots, w*{n-1}\})\} \]</div>
<p>By using this approach, our model computes the final coherence score based on the relationship between each word in the topic and all the other words in the topic. This way, the segmentation step allows us to choose different strategies to investigate the coherence of a given topic.</p>
</section>
<section id="probability-calculation">
<h3>2. Probability Calculation<a class="headerlink" href="#probability-calculation" title="Permalink to this heading">#</a></h3>
<p>The probability calculation module is a crucial component in coherence metrics, as it computes the probabilities for word subsets generated by the segmentation module. These probabilities are derived from the textual corpus and are essential for understanding the relationships between words in a topic.</p>
<p>For instance, we might be interested in calculating the following probabilities:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(w)\)</span>: the probability of a word <span class="math notranslate nohighlight">\(w\)</span> occurring in the corpus.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(w_1, w_2)\)</span>: the probability of a word pair <span class="math notranslate nohighlight">\((w_1, w_2)\)</span> occurring in the corpus.</p></li>
</ul>
<p>Various techniques can estimate these probabilities in distinct ways. One example is the estimation of <span class="math notranslate nohighlight">\(P_{bd}(w)\)</span>, which is calculated by counting the number of documents containing the word <span class="math notranslate nohighlight">\(w\)</span> and dividing it by the total number of documents in the corpus:</p>
<div class="math notranslate nohighlight">
\[ P\_{bd}(w) = \frac{count(D_w)}{N} \]</div>
<p>Similarly, the probability <span class="math notranslate nohighlight">\(P(w_1, w_2)\)</span> can be estimated by counting the number of documents in which both <span class="math notranslate nohighlight">\(w_1\)</span> and <span class="math notranslate nohighlight">\(w_2\)</span> appear and dividing the result by the total number of documents in the corpus:</p>
<div class="math notranslate nohighlight">
\[ P(w_1, w_2) = \frac{count(D*{w_1, w_2})}{N} \]</div>
<p>Another approach is to use sentence-level probabilities. In this case, the probability <span class="math notranslate nohighlight">\(P_{bs}(w)\)</span> is calculated by counting the number of sentences in which the word <span class="math notranslate nohighlight">\(w\)</span> appears and dividing the result by the total number of sentences in the corpus.</p>
<p>Alternatively, for <span class="math notranslate nohighlight">\(P_{sw}\)</span>, the probability is determined by counting the number of sliding windows containing the word <span class="math notranslate nohighlight">\(w\)</span> and dividing the result by the total number of sliding windows in the corpus.</p>
<p>These probabilities serve as the foundation for computing coherence metrics and are essential for evaluating the quality of topics generated by topic modeling algorithms.</p>
</section>
<section id="confirmation-measure">
<h3>3. Confirmation Measure<a class="headerlink" href="#confirmation-measure" title="Permalink to this heading">#</a></h3>
<p>The confirmation measure module lies at the heart of coherence metrics, as it calculates the confirmation of word subsets. It assesses how well a word subset <span class="math notranslate nohighlight">\(W^*\)</span> supports the words in another subset <span class="math notranslate nohighlight">\(W^{\prime}\)</span> by comparing their probabilities derived from the textual corpus.</p>
<p>The confirmation measure aims to evaluate the relationship between two subsets of words by analyzing how likely they are to appear together in the corpus. If the words in <span class="math notranslate nohighlight">\(W^{\prime}\)</span> frequently co-occur with those in <span class="math notranslate nohighlight">\(W^*\)</span>, the confirmation measure will be high; if not, it will be low.</p>
<p>For instance, suppose we have the following word subsets:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(W^{\prime} = \{w_1, w_2\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(W^* = \{w_3, w_4\}\)</span></p></li>
</ul>
<p>A high confirmation measure indicates that the words <span class="math notranslate nohighlight">\(w_1\)</span> and <span class="math notranslate nohighlight">\(w_2\)</span> are likely to appear with the words <span class="math notranslate nohighlight">\(w_3\)</span> and <span class="math notranslate nohighlight">\(w_4\)</span> in the reference corpus.</p>
<figure class="align-default" id="fig-topic-confirmation-measure">
<a class="reference internal image-reference" href="../../../_images/confirmation_measure.png"><img alt="../../../_images/confirmation_measure.png" src="../../../_images/confirmation_measure.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 53 </span><span class="caption-text">Confirmation Measures</span><a class="headerlink" href="#fig-topic-confirmation-measure" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Confirmation measures can be classified into two types: direct and indirect.</p>
<section id="direct-confirmation-measure">
<h4>Direct Confirmation Measure<a class="headerlink" href="#direct-confirmation-measure" title="Permalink to this heading">#</a></h4>
<p>The direct confirmation measure is a straightforward approach that compares the probabilities of word subsets <span class="math notranslate nohighlight">\(W^{\prime}\)</span> and <span class="math notranslate nohighlight">\(W^*\)</span>.</p>
<div class="math notranslate nohighlight">
\[ m_r(S_i) = \frac{P(W^{\prime}, W^*)}{P(W^{\prime})P(W^*)} \]</div>
<p>Alternatively, using logarithms:</p>
<div class="math notranslate nohighlight">
\[ m_{lr}(S_i) = log \frac{P(W^{\prime}, W^*) + \epsilon}{P(W^{\prime})P(W^\_) + \epsilon} \]</div>
<p>Here, <span class="math notranslate nohighlight">\(\epsilon\)</span> represents a small constant that prevents undefined values for logarithms.</p>
</section>
<section id="indirect-confirmation-measure">
<h4>Indirect Confirmation Measure<a class="headerlink" href="#indirect-confirmation-measure" title="Permalink to this heading">#</a></h4>
<p>The indirect confirmation measure is a more sophisticated method. It computes a direct confirmation measure for each word in the subsets <span class="math notranslate nohighlight">\(W^{\prime}\)</span> and <span class="math notranslate nohighlight">\(W^*\)</span>, producing a vector of confirmation scores for each word in <span class="math notranslate nohighlight">\(W^{\prime}\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \vec{v}_m(W^{\prime}) = \left\{\sum_{w \in W^{\prime}} m(w_i, w_j)\right\}*{j=1,2, …, |W|} \]</div>
<p>where <span class="math notranslate nohighlight">\(|W|\)</span> denotes the number of words in <span class="math notranslate nohighlight">\(W\)</span>.</p>
<p>The process is repeated for the subset <span class="math notranslate nohighlight">\(W^*\)</span>. The indirect confirmation measure is then the similarity between the two vectors.</p>
<div class="math notranslate nohighlight">
\[
\tilde{m}_{cos}(W^{\prime}, W^*) = sim(\vec{v}_m(W^{\prime}), \vec{v}_m(W^*))
\]</div>
<p>In this case, <span class="math notranslate nohighlight">\(sim\)</span> is a similarity function, such as cosine similarity.</p>
<figure class="align-default" id="fig-topic-indirect-confirmation-measure">
<a class="reference internal image-reference" href="../../../_images/indirect_confirmation_measure.png"><img alt="../../../_images/indirect_confirmation_measure.png" src="../../../_images/indirect_confirmation_measure.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 54 </span><span class="caption-text">Indirect Confirmation Measures</span><a class="headerlink" href="#fig-topic-indirect-confirmation-measure" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The indirect confirmation measure captures relationships that the direct confirmation measure might overlook. For example, while the words ‘cats’ and ‘dogs’ may never appear together in the dataset, they could frequently co-occur with the words ‘toys’, ‘pets’, and ‘cute’. The direct confirmation measure would fail to identify the relationship between ‘cats’ and ‘dogs’ since they never appear together. However, the indirect confirmation measure would detect this relationship by observing that ‘cats’ and ‘dogs’ frequently appear with the words ‘toys’, ‘pets’, and ‘cute’.</p>
</section>
</section>
<section id="aggregation">
<h3>Aggregation<a class="headerlink" href="#aggregation" title="Permalink to this heading">#</a></h3>
<p>The aggregation module plays a crucial role in computing the final coherence score for a topic. It aggregates the confirmation scores of the pairs generated in the previous steps, condensing the information into a single value that represents the topic’s coherence.</p>
<p>Various aggregation techniques can be employed, including mean, median, and geometric mean, among others. Each method has its advantages and disadvantages, and the choice of aggregation technique depends on the specific use case and the desired properties of the resulting coherence score.</p>
<section id="mean">
<h4>Mean<a class="headerlink" href="#mean" title="Permalink to this heading">#</a></h4>
<p>The mean (arithmetic average) is a widely-used aggregation technique. It calculates the sum of the confirmation scores and divides it by the total number of scores. The mean is sensitive to extreme values and might not be the best choice if the distribution of scores is skewed.</p>
</section>
<section id="median">
<h4>Median<a class="headerlink" href="#median" title="Permalink to this heading">#</a></h4>
<p>The median represents the middle value in a sorted list of confirmation scores. It is less sensitive to extreme values compared to the mean and provides a more robust estimate of central tendency, especially in cases where the distribution of scores is not symmetrical.</p>
</section>
<section id="geometric-mean">
<h4>Geometric Mean<a class="headerlink" href="#geometric-mean" title="Permalink to this heading">#</a></h4>
<p>The geometric mean is another aggregation technique that calculates the nth root of the product of confirmation scores, where n is the total number of scores. The geometric mean is less sensitive to extreme values than the mean and is particularly useful when dealing with multiplicative factors or when the data has a log-normal distribution.</p>
<figure class="align-default" id="fig-topic-aggregation">
<a class="reference internal image-reference" href="../../../_images/aggregation.png"><img alt="../../../_images/aggregation.png" src="../../../_images/aggregation.png" style="width: 30%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 55 </span><span class="caption-text">Aggregation Techniques</span><a class="headerlink" href="#fig-topic-aggregation" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In summary, the aggregation module is responsible for combining the confirmation scores of word subset pairs into a single coherence score that represents the topic’s interpretability. The choice of aggregation technique depends on the desired properties of the coherence score and the distribution of confirmation scores.</p>
</section>
</section>
<section id="putting-everything-together">
<h3>Putting Everything Together<a class="headerlink" href="#putting-everything-together" title="Permalink to this heading">#</a></h3>
<p>To measure the coherence of a topic, the following steps are taken:</p>
<ol class="arabic simple">
<li><p><strong>Topic Selection</strong>: Identify the topic <span class="math notranslate nohighlight">\(T\)</span> for which you want to measure coherence.</p></li>
<li><p><strong>Reference Corpus</strong>: Choose a reference corpus <span class="math notranslate nohighlight">\(C\)</span> that represents the context of the topic.</p></li>
<li><p><strong>Top-n Words Extraction</strong>: Extract the top-n most important words in the topic <span class="math notranslate nohighlight">\(T\)</span>, resulting in a word subset <span class="math notranslate nohighlight">\(W\)</span>.</p></li>
<li><p><strong>Segmentation</strong>: Segment the word subset <span class="math notranslate nohighlight">\(W\)</span> into pairs of words, generating a set of word subsets <span class="math notranslate nohighlight">\(S\)</span>.</p></li>
<li><p><strong>Probability Calculation</strong>: Utilize the reference corpus <span class="math notranslate nohighlight">\(C\)</span> to calculate the probabilities of the word subsets <span class="math notranslate nohighlight">\(S\)</span>.</p></li>
<li><p><strong>Confirmation Measure</strong>: Compute the confirmation measure for each pair of words in <span class="math notranslate nohighlight">\(S\)</span> using the probabilities obtained from the reference corpus <span class="math notranslate nohighlight">\(C\)</span>.</p></li>
<li><p><strong>Aggregation</strong>: Aggregate all the confirmation scores into a single coherence score for the topic <span class="math notranslate nohighlight">\(T\)</span>.</p></li>
</ol>
<p>If you have multiple topics, repeat the process for each topic and use the average coherence score as a measure of the overall quality of the topic model.</p>
<figure class="align-default" id="fig-topic-multiple-topics">
<a class="reference internal image-reference" href="../../../_images/multiple_topics.png"><img alt="../../../_images/multiple_topics.png" src="../../../_images/multiple_topics.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 56 </span><span class="caption-text">Coherence Score for Multiple Topics</span><a class="headerlink" href="#fig-topic-multiple-topics" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The Gensim library offers a class that implements four widely-used coherence models: <span class="math notranslate nohighlight">\(u_{mass}\)</span>, <span class="math notranslate nohighlight">\(c_v\)</span>, <span class="math notranslate nohighlight">\(c_{uci}\)</span>, and <span class="math notranslate nohighlight">\(c_{npmi}\)</span>.</p>
<figure class="align-default" id="fig-topic-gensim-coherence-models">
<a class="reference internal image-reference" href="../../../_images/gensim_coherence_models.png"><img alt="../../../_images/gensim_coherence_models.png" src="../../../_images/gensim_coherence_models.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 57 </span><span class="caption-text">Gensim Coherence Models</span><a class="headerlink" href="#fig-topic-gensim-coherence-models" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The <span class="math notranslate nohighlight">\(C_{NPMI}\)</span> coherence model uses the following steps:</p>
<ul class="simple">
<li><p>Segmentation: S-one-one (one word in each subset)</p></li>
<li><p>Probability estimation: probabilities are calculated using a sliding window of size 10.</p></li>
<li><p>Confirmation measure: Normalized Pointwise Mutual Information (NPMI) is employed.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
NPMI(W^{\prime}, W^*) = \frac{\log \frac{P(W^{\prime}, W^*) + \epsilon}{P(W^{\prime})P(W^*) + \epsilon}}{-\log (P(W^{\prime}, W^*) + \epsilon)}
\]</div>
<p>The <span class="math notranslate nohighlight">\(C_V\)</span> coherence model involves these steps:</p>
<ul class="simple">
<li><p>Segmentation: S-one-set, where the confirmation measure is calculated for pairs of words in the same subset.</p></li>
<li><p>Probability estimation: probabilities are calculated using a sliding window of size 110.</p></li>
<li><p>Confirmation measure: indirect confirmation measure with cosine similarity as the similarity function.</p></li>
<li><p>Aggregation: the mean of the confirmation scores is used for aggregation.</p></li>
</ul>
<p>In summary, measuring topic coherence involves a series of steps, from selecting a topic and reference corpus to calculating probabilities, confirmation measures, and aggregating scores. These steps help assess the interpretability and quality of a topic model, providing valuable insights for users.</p>
</section>
</section>
<section id="understanding-pointwise-mutual-information">
<h2>Understanding Pointwise Mutual Information<a class="headerlink" href="#understanding-pointwise-mutual-information" title="Permalink to this heading">#</a></h2>
<p>Determining whether two (or more) words are related or form a concept can be achieved by assessing their co-occurrence patterns:</p>
<ul class="simple">
<li><p>If words appear together more often than expected by chance, it suggests they are related.</p></li>
<li><p>However, caution is needed, as some words may co-occur frequently just because they are common. For instance, in the case of “New York,” the word “New” frequently appears in news articles, so it may co-occur with other words more often than expected by chance.</p></li>
<li><p>To discern whether the co-occurrence of “New” and “York” is due to chance or an actual relationship, a measure like Pointwise Mutual Information (PMI) can be employed.</p></li>
</ul>
<p>PMI is used to determine the association between two words, quantifying the likelihood of their co-occurrence in text given their individual appearances.</p>
<section id="pmi">
<h3>PMI<a class="headerlink" href="#pmi" title="Permalink to this heading">#</a></h3>
<p>PMI is defined as:</p>
<div class="math notranslate nohighlight">
\[ PMI(w_i, w_j) = log \frac{P(w_i, w_j)}{P(w_i)P(w_j)} \]</div>
<p>Here, <span class="math notranslate nohighlight">\(P(w_i, w_j)\)</span> denotes the probability of words <span class="math notranslate nohighlight">\(w_i\)</span> and <span class="math notranslate nohighlight">\(w_j\)</span> appearing together in the corpus, while <span class="math notranslate nohighlight">\(P(w_i)\)</span> and <span class="math notranslate nohighlight">\(P(w_j)\)</span> represent the probabilities of <span class="math notranslate nohighlight">\(w_i\)</span> and <span class="math notranslate nohighlight">\(w_j\)</span> occurring individually.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(w_i\)</span> and <span class="math notranslate nohighlight">\(w_j\)</span> are independent, <span class="math notranslate nohighlight">\(P(w_i, w_j) = P(w_i)P(w_j)\)</span>, and the PMI equals zero.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(w_i\)</span> and <span class="math notranslate nohighlight">\(w_j\)</span> are dependent, <span class="math notranslate nohighlight">\(P(w_i, w_j) &gt; P(w_i)P(w_j)\)</span>, and the PMI is positive.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(w_i\)</span> and <span class="math notranslate nohighlight">\(w_j\)</span> are negatively dependent, <span class="math notranslate nohighlight">\(P(w_i, w_j) &lt; P(w_i)P(w_j)\)</span>, and the PMI is negative.</p></li>
</ul>
</section>
<section id="normalized-pointwise-mutual-information">
<h3>Normalized Pointwise Mutual Information<a class="headerlink" href="#normalized-pointwise-mutual-information" title="Permalink to this heading">#</a></h3>
<p>While PMI effectively measures the association between two words, it has some limitations. For example, PMI is not normalized, making it difficult to compare associations between words with different frequencies.</p>
<p>Normalized Pointwise Mutual Information (NPMI) addresses this issue by normalizing PMI. It is defined as:</p>
<div class="math notranslate nohighlight">
\[ NPMI(w_i, w_j) = \frac{PMI(w_i, w_j)}{-log(P(w_i, w_j))} \]</div>
<p>In summary, understanding the association between words using PMI or NPMI helps determine whether they form a concept or are related. By considering the co-occurrence patterns of words and their individual frequencies, these measures provide valuable insights into the relationships between words in a corpus.</p>
</section>
</section>
<section id="understanding-cosine-similarity">
<h2>Understanding Cosine Similarity<a class="headerlink" href="#understanding-cosine-similarity" title="Permalink to this heading">#</a></h2>
<p>Cosine similarity is a measure used to determine the similarity between two vectors. It assesses the cosine of the angle between the vectors, which provides a measure of similarity that is independent of their magnitude. The formula for cosine similarity is:</p>
<div class="math notranslate nohighlight">
\[ cos(\theta) = \frac{A \cdot B}{||A|| \cdot ||B||} \]</div>
<p>Here, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> represent the two vectors being compared, and <span class="math notranslate nohighlight">\(\theta\)</span> is the angle between them.</p>
<p>Cosine similarity produces a value between -1 and 1:</p>
<ul class="simple">
<li><p>If the two vectors are identical, the cosine similarity is 1.</p></li>
<li><p>If the two vectors are orthogonal (i.e., not related), the cosine similarity is 0.</p></li>
<li><p>If the two vectors are antipodal (i.e., opposite directions), the cosine similarity is -1.</p></li>
<li><p>If the two vectors are similar (i.e., pointing in the same direction), the cosine similarity is positive.</p></li>
<li><p>If the two vectors are dissimilar (i.e., pointing in different directions), the cosine similarity is negative.</p></li>
</ul>
<p>In the context of natural language processing and information retrieval, cosine similarity is often used to measure the similarity between documents or between a query and a document. In this scenario, each document or query is represented as a high-dimensional vector, where each dimension corresponds to a specific term in the vocabulary, and the value of each dimension represents the weight (e.g., term frequency) of that term in the document.</p>
<p>By computing the cosine similarity between these vectors, it is possible to identify documents or queries with similar semantic content, even if they do not share the exact same words. This is because the cosine similarity focuses on the direction of the vectors, which is influenced by the overall distribution of terms, rather than the specific words themselves.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/nlp_intro/topic"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Topic Modeling</p>
      </div>
    </a>
    <a class="right-next"
       href="coherence-practice.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Topic Coherence in Practice</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling">Topic Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-topics">Evaluating Topics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-coherence">Topic Coherence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#segmentation">1. Segmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-calculation">2. Probability Calculation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confirmation-measure">3. Confirmation Measure</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#direct-confirmation-measure">Direct Confirmation Measure</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#indirect-confirmation-measure">Indirect Confirmation Measure</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#aggregation">Aggregation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean">Mean</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#median">Median</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric-mean">Geometric Mean</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-everything-together">Putting Everything Together</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-pointwise-mutual-information">Understanding Pointwise Mutual Information</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pmi">PMI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalized-pointwise-mutual-information">Normalized Pointwise Mutual Information</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-cosine-similarity">Understanding Cosine Similarity</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://entelecheia.me" target="_blank">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>