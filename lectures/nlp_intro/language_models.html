

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Language Models &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/js/hoverxref.js"></script>
    <script src="../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/chatgpt.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/nlp_intro/language_models';</script>
    <link rel="canonical" href="https://lecture.entelecheia.ai/lectures/nlp_intro/language_models.html" />
    <link rel="shortcut icon" href="../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Topic Modeling" href="topic.html" />
    <link rel="prev" title="Research Applications" href="research.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Introduction to NLP</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="research.html">Research Applications</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="topic.html">Topic Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="topic_models.html">Topic Models </a></li>
<li class="toctree-l2"><a class="reference internal" href="topic_coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="sentiments.html">Sentiment Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="word_segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="vectorization.html">Vector Semantics and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="word_embeddings.html">Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab2-corpus-eda.html">Lab 2: EDA on Corpora</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_deep/index.html">Deep Learning for NLP</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/llms.html">Large Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/zeroshot.html">Zero Shot, Prompt, and Search Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/detectGPT.html">How to Spot Machine-Written Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/transformers.html">Transformers </a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/plms.html">Pretrained Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/lab3-train-tokenizers.html">Lab 3: Training Tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_deep/lab4-pretraining-lms.html">Lab 4: Pretraining Language Models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_advances/index.html">Advances in AI and NLP</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/detectGPT.html">DetectGPT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_advances/gpt4.html">GPT-4</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../aiart/index.html">AI Art (Generative AI)</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../aiart/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/project-themes.html">Project Themes - A Brave New World</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/robot_drawings.html">Robot Drawing Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/imagen.html">Imagen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/textual-inversion.html">Textual Inversion (Dreambooth)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/whisper.html">Automatic Speech Recognition (Whisper)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/text2music.html">Text to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/image2music.html">Image to Music</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/devops.html">DevOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/gitops.html">GitOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/dotfiles.html">Dotfiles</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ds/index.html">Data Science for Economics and Finance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/index.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/entelecheia/lecture/blob/main/book/lectures/nlp_intro/language_models.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>



<a href="https://github.com/entelecheia/lecture" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/nlp_intro/language_models.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Language Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-concept">General Concept</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-language-models">Why do we need language models?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n-gram-language-models">N-gram Language Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n-grams-and-probability-estimation">N-Grams and Probability Estimation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-probabilities-from-counts">Estimating Probabilities from Counts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bigram-model">Bigram Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-joint-probabilities-of-word-sequences">Estimating Joint Probabilities of Word Sequences</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n-gram-models-and-markov-assumption">N-gram Models and Markov Assumption</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stock-prices-and-markov-assumption">Stock Prices and Markov Assumption</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-bigram-or-n-gram-probabilities-using-maximum-likelihood-estimation-mle">Estimating Bigram or N-gram Probabilities using Maximum Likelihood Estimation (MLE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bigram-probability-calculation">Bigram Probability Calculation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-issues-in-n-gram-models">Practical Issues in N-gram Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-mle">Examples of MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-generate-n-grams-using-nltk">How to generate n-grams using NLTK</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="language-models">
<h1>Language Models<a class="headerlink" href="#language-models" title="Permalink to this heading">#</a></h1>
<section id="general-concept">
<h2>General Concept<a class="headerlink" href="#general-concept" title="Permalink to this heading">#</a></h2>
<p>Language models are computational models that assign probabilities to sequences of words or predict the next word in a sequence. They play a crucial role in various natural language processing tasks, such as:</p>
<ul class="simple">
<li><p>Speech recognition</p></li>
<li><p>Machine translation</p></li>
<li><p>Spelling and grammar correction</p></li>
<li><p>Text generation</p></li>
</ul>
<p>Language models capture the structure and patterns within a language, allowing them to estimate how likely a given word or phrase is to appear in a specific context. There are different types of language models, such as:</p>
<ul class="simple">
<li><p>N-gram models, which rely on sequences of words</p></li>
<li><p>Neural language models, which utilize deep learning techniques to understand and generate text</p></li>
</ul>
<p>Understanding and developing effective language models is essential for improving the performance of natural language processing systems.</p>
</section>
<section id="why-do-we-need-language-models">
<h2>Why do we need language models?<a class="headerlink" href="#why-do-we-need-language-models" title="Permalink to this heading">#</a></h2>
<p>Language models are essential for various reasons in natural language processing tasks:</p>
<ol class="arabic simple">
<li><p><strong>Disambiguation</strong>: Language models help in resolving ambiguities in speech recognition and text processing, as they can assign probabilities to different interpretations based on the context, selecting the most likely one.</p></li>
<li><p><strong>Machine Translation</strong>: In translating text from one language to another, language models can help choose the most fluent and accurate translations by estimating the likelihood of word sequences in the target language.</p></li>
<li><p><strong>Text Generation</strong>: Language models can generate coherent and contextually relevant text, which is useful for tasks like summarization, question-answering, and dialogue systems.</p></li>
<li><p><strong>Spelling and Grammar Correction</strong>: Language models can identify and correct errors in written text by comparing the probabilities of different word sequences and suggesting more likely alternatives.</p></li>
<li><p><strong>Assistive Technologies</strong>: Language models are crucial for augmentative and alternative communication (AAC) systems, as they can predict and suggest likely words or phrases for users with speech or language impairments, making communication more efficient.</p></li>
</ol>
<p>Overall, language models play a critical role in improving the performance and accuracy of natural language processing systems by capturing the structure, patterns, and nuances of a language.</p>
</section>
<section id="n-gram-language-models">
<h2>N-gram Language Models<a class="headerlink" href="#n-gram-language-models" title="Permalink to this heading">#</a></h2>
<p>N-gram language models are a simple yet powerful approach to language modeling. They predict the next word in a sequence based on the previous n-1 words, where n is the order of the model. The main types of n-grams are:</p>
<ul class="simple">
<li><p><strong>Unigram</strong>: Considers only a single word, ignoring context (n=1).</p></li>
<li><p><strong>Bigram</strong>: Considers a sequence of two words (n=2).</p></li>
<li><p><strong>Trigram</strong>: Considers a sequence of three words (n=3).</p></li>
<li><p><strong>Higher-order n-grams</strong>: Considers longer sequences of words (n&gt;3).</p></li>
</ul>
<p>In n-gram models, the probability of a word depends only on the (n-1) previous words. This simplification is known as the Markov assumption. To estimate the probabilities, n-gram models rely on counting the occurrences of n-grams in a large corpus and normalizing the counts.</p>
<p>Advantages of n-gram models:</p>
<ul class="simple">
<li><p>Relatively simple and easy to implement.</p></li>
<li><p>Efficient in terms of computation and memory usage.</p></li>
</ul>
<p>Limitations of n-gram models:</p>
<ul class="simple">
<li><p>Unable to capture long-range dependencies between words.</p></li>
<li><p>Sensitive to data sparsity issues, as some n-grams may not appear in the training corpus.</p></li>
</ul>
<p>Despite their limitations, n-gram models serve as a foundational tool for understanding language modeling concepts and are still used in various NLP applications.</p>
</section>
<section id="n-grams-and-probability-estimation">
<h2>N-Grams and Probability Estimation<a class="headerlink" href="#n-grams-and-probability-estimation" title="Permalink to this heading">#</a></h2>
<p>N-grams can be used to estimate the probability of a word given a history (P(w|h)). For instance, if the history h is “I like to eat”, we might want to estimate the probability of the next word being “pizza” (P(pizza|I like to eat)).</p>
<section id="estimating-probabilities-from-counts">
<h3>Estimating Probabilities from Counts<a class="headerlink" href="#estimating-probabilities-from-counts" title="Permalink to this heading">#</a></h3>
<p>We can estimate this probability by counting the occurrences of the history followed by the target word in a large corpus and dividing by the total count of the history:</p>
<div class="math notranslate nohighlight">
\[
P(\text{pizza}|\text{I like to eat}) = \frac{C(\text{I like to eat pizza})}{C(\text{I like to eat})}
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{먹습니다}|\text{저는 김치를}) = \frac{C(\text{저는 김치를 먹습니다})}{C(\text{저는 김치를})}
\]</div>
<p>However, even with a large corpus, it’s often not sufficient to provide good estimates due to the creative nature of language and the possibility of unseen word sequences.</p>
</section>
<section id="bigram-model">
<h3>Bigram Model<a class="headerlink" href="#bigram-model" title="Permalink to this heading">#</a></h3>
<p>To tackle this issue, we can use the bigram model, which approximates the probability of a word given its entire history by considering only the preceding word:</p>
<div class="math notranslate nohighlight">
\[
P(\text{pizza}|\text{I like to eat}) \approx P(\text{pizza}|\text{eat})
\]</div>
<div class="math notranslate nohighlight">
\[
P(\text{먹습니다}|\text{저는 김치를}) \approx P(\text{먹습니다}|\text{김치를})
\]</div>
<p>This simplification allows us to estimate probabilities more reliably, but it may not capture longer context dependencies. Nevertheless, n-grams serve as a foundational tool for understanding language modeling concepts and can be useful in various NLP applications.</p>
</section>
</section>
<section id="estimating-joint-probabilities-of-word-sequences">
<h2>Estimating Joint Probabilities of Word Sequences<a class="headerlink" href="#estimating-joint-probabilities-of-word-sequences" title="Permalink to this heading">#</a></h2>
<p>To estimate the joint probability of an entire sequence of words, such as “The cat sat on the mat”, we can decompose this probability using the chain rule of probability:</p>
<div class="math notranslate nohighlight">
\[
P(w_1:n) = P(w_1)P(w_2|w_1)P(w_3|w_1:2)...P(w_n|w_1:n−1) = \prod_{k=1}^n P(w_k|w_1:k−1)
\]</div>
<p>This decomposition shows the link between computing the joint probability of a sequence and computing the conditional probability of a word given previous words. However, it doesn’t really seem to help us! Computing the exact probability of a word given a long sequence of preceding words (e.g., P(wn|w1:n−1)) is challenging because language is creative, and any particular context might have never occurred before.</p>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this heading">#</a></h3>
<p>For the sequence “The cat sat on the mat”, we can decompose the joint probability as follows:</p>
<div class="math notranslate nohighlight">
\[
P(\text{The, cat, sat, on, the, mat}) = P(\text{The})P(\text{cat}|\text{The})P(\text{sat}|\text{The, cat})...P(\text{mat}|\text{The, cat, sat, on, the})
\]</div>
<p>Estimating each conditional probability using counts from a large corpus is not feasible since many long sequences might never have occurred before.</p>
</section>
</section>
<section id="n-gram-models-and-markov-assumption">
<h2>N-gram Models and Markov Assumption<a class="headerlink" href="#n-gram-models-and-markov-assumption" title="Permalink to this heading">#</a></h2>
<p>N-gram models are used to predict the probability of a word given a fixed number of preceding words. The assumption that the probability of a word depends only on a limited number of previous words is called the Markov assumption.</p>
<p>For a bigram model (N=2), the approximation is:</p>
<div class="math notranslate nohighlight">
\[
P(w_n|w_1:n−1) ≈ P(w_n|w_{n−1})
\]</div>
<p>For an n-gram model with size N, the approximation is:</p>
<div class="math notranslate nohighlight">
\[
P(w_n|w_1:n−1) ≈ P(w_n|w_{n−N+1:n−1})
\]</div>
<p>Given the n-gram assumption for the probability of an individual word, we can compute the probability of a complete word sequence as:</p>
<div class="math notranslate nohighlight">
\[
P(w_1:n) ≈ \prod_{k=1}^n P(w_k|w_{k−N+1:k−1})
\]</div>
<section id="id1">
<h3>Example<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>For the trigram model (N=3), the approximation for the sequence “The cat sat on the mat” is:</p>
<div class="math notranslate nohighlight">
\[
P(\text{The, cat, sat, on, the, mat}) ≈ P(\text{The})P(\text{cat}|\text{The})P(\text{sat}|\text{The, cat})P(\text{on}|\text{cat, sat})P(\text{the}|\text{sat, on})P(\text{mat}|\text{on, the})
\]</div>
</section>
<section id="stock-prices-and-markov-assumption">
<h3>Stock Prices and Markov Assumption<a class="headerlink" href="#stock-prices-and-markov-assumption" title="Permalink to this heading">#</a></h3>
<p>The Markov assumption can be applied to model stock prices as well. In finance, the Markov assumption is often used to represent the idea that future stock prices depend only on the current price and a limited number of past prices, rather than the entire price history.</p>
<p>The relationship between stock prices and the Markov assumption can be understood as follows:</p>
<ol class="arabic simple">
<li><p><strong>Memoryless Property</strong>: The Markov assumption implies that the stock price at a certain time is only influenced by a fixed number of previous time steps. This means that the future price movement doesn’t depend on the entire history but only on the recent past. This property is also known as the memoryless property of Markov models.</p></li>
<li><p><strong>Simplifying Complex Systems</strong>: Stock prices are affected by a vast number of factors, including market trends, company performance, global events, and investor sentiment. By applying the Markov assumption, we can simplify the modeling of stock prices by focusing only on the most recent price changes, which are assumed to capture relevant information.</p></li>
<li><p><strong>Prediction and Analysis</strong>: Using the Markov assumption in financial models allows us to predict and analyze stock price movements. For example, we can create Markov models to estimate the probabilities of stock price changes, which can be useful for trading strategies, risk management, and portfolio optimization.</p></li>
</ol>
<p>It’s important to note that the Markov assumption is a simplification and may not always accurately represent the complexities of the stock market. However, it serves as a useful tool in modeling and analyzing stock prices.</p>
</section>
</section>
<section id="estimating-bigram-or-n-gram-probabilities-using-maximum-likelihood-estimation-mle">
<h2>Estimating Bigram or N-gram Probabilities using Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#estimating-bigram-or-n-gram-probabilities-using-maximum-likelihood-estimation-mle" title="Permalink to this heading">#</a></h2>
<p>To estimate the probabilities for bigrams or n-grams, we use maximum likelihood estimation (MLE) by counting the occurrences in a corpus and normalizing the counts.</p>
<p>MLE, or Maximum Likelihood Estimation, is a statistical method used for estimating the parameters of a probability distribution or a model. It works by finding the parameter values that maximize the likelihood of the observed data under the given model. In other words, MLE selects the parameters that make the observed data most probable.</p>
<p>For example, if we have a dataset of coin tosses (heads and tails), and we want to estimate the probability of getting heads, we can use MLE to find the parameter value that makes the observed sequence of coin tosses most likely. This is usually done by calculating the ratio of the number of heads to the total number of tosses.</p>
<section id="bigram-probability-calculation">
<h3>Bigram Probability Calculation<a class="headerlink" href="#bigram-probability-calculation" title="Permalink to this heading">#</a></h3>
<p>Compute the bigram probability of a word <span class="math notranslate nohighlight">\(w_n\)</span> given the previous word <span class="math notranslate nohighlight">\(w_{n-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[ P(w_n|w_{n−1}) = C(w_{n−1}w_n) / C(w_{n−1}) \]</div>
<p>where <span class="math notranslate nohighlight">\(C(w_{n−1} w_n)\)</span> is the count of the bigram <span class="math notranslate nohighlight">\(w_{n−1}w_n\)</span> and <span class="math notranslate nohighlight">\(C(w_{n−1})\)</span> is the count of the unigram <span class="math notranslate nohighlight">\(w_{n−1}\)</span>.</p>
</section>
<section id="id2">
<h3>Example<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>Consider a mini-corpus with three sentences:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span> <span class="pre">I</span> <span class="pre">am</span> <span class="pre">Sam</span> <span class="pre">&lt;/s&gt;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span> <span class="pre">Sam</span> <span class="pre">I</span> <span class="pre">am</span> <span class="pre">&lt;/s&gt;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span> <span class="pre">I</span> <span class="pre">do</span> <span class="pre">not</span> <span class="pre">like</span> <span class="pre">green</span> <span class="pre">eggs</span> <span class="pre">and</span> <span class="pre">ham</span> <span class="pre">&lt;/s&gt;</span></code></p></li>
</ol>
<p>Here are the bigram probabilities for some pairs in this corpus:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Bigram</p></th>
<th class="head"><p>Probability</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>P(I|<code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span></code>)</p></td>
<td><p>2/3</p></td>
</tr>
<tr class="row-odd"><td><p>P(Sam|<code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span></code>)</p></td>
<td><p>1/3</p></td>
</tr>
<tr class="row-even"><td><p>P(am|I)</p></td>
<td><p>2/3</p></td>
</tr>
<tr class="row-odd"><td><p>P(<code class="docutils literal notranslate"><span class="pre">&lt;/s&gt;</span></code>|Sam)</p></td>
<td><p>1/2</p></td>
</tr>
<tr class="row-even"><td><p>P(Sam|am)</p></td>
<td><p>1/2</p></td>
</tr>
<tr class="row-odd"><td><p>P(do|I)</p></td>
<td><p>1/3</p></td>
</tr>
</tbody>
</table>
<p>For general MLE n-gram parameter estimation:</p>
<div class="math notranslate nohighlight">
\[ P(w_n|w_{n−N+1:n−1}) = C(w_{n−N+1:n−1} w_n) / C(w_{n−N+1:n−1}) \]</div>
<p>Bigram probabilities capture various linguistic phenomena, such as syntax, task-specific patterns, and cultural preferences.</p>
</section>
</section>
<section id="practical-issues-in-n-gram-models">
<h2>Practical Issues in N-gram Models<a class="headerlink" href="#practical-issues-in-n-gram-models" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Higher-order n-grams</strong>: In practice, trigram (conditioning on the previous two words), 4-gram, or even 5-gram models are more common when there is sufficient training data. For larger n-grams, extra contexts are needed to the left and right of the sentence end (e.g., P(I|<code class="docutils literal notranslate"><span class="pre">&lt;s&gt;&lt;s&gt;</span></code>) for trigrams).</p></li>
<li><p><strong>Log probabilities</strong>: Since multiplying probabilities can lead to numerical underflow, it’s better to represent and compute language model probabilities in log format. Adding log probabilities is equivalent to multiplying probabilities in linear space. Convert back to probabilities when needed by taking the exponential of the log probability:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[   p_1 × p_2 × p_3 × p_4 = \exp(\log{p_1} + \log{p_2} + \log{p_3} + \log{p_4}) \]</div>
</section>
<section id="examples-of-mle">
<h2>Examples of MLE<a class="headerlink" href="#examples-of-mle" title="Permalink to this heading">#</a></h2>
<p>Here’s a practical example of MLE using Python. We’ll estimate the parameter p of a Bernoulli distribution (coin toss) based on some observed data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Observed data (1 for heads, 0 for tails)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Calculate MLE for p (probability of heads)</span>
<span class="n">mle_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MLE for p (probability of heads):&quot;</span><span class="p">,</span> <span class="n">mle_p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MLE for p (probability of heads): 0.65
</pre></div>
</div>
</div>
</div>
<p>In this example, we have a sequence of 20 coin tosses, where 1 represents heads, and 0 represents tails. To estimate the probability of getting heads (p) using MLE, we simply calculate the mean of the observed data, which is the ratio of the number of heads to the total number of tosses.</p>
<p>Here’s another example, this time estimating the mean (mu) and standard deviation (sigma) of a Gaussian distribution given some observed data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Observed data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">4.1</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">3.9</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">])</span>

<span class="c1"># Calculate MLE for mu (mean) and sigma (standard deviation)</span>
<span class="n">mle_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">mle_sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MLE for mu (mean):&quot;</span><span class="p">,</span> <span class="n">mle_mu</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MLE for sigma (standard deviation):&quot;</span><span class="p">,</span> <span class="n">mle_sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MLE for mu (mean): 3.4899999999999998
MLE for sigma (standard deviation): 0.6441273166075167
</pre></div>
</div>
</div>
</div>
</section>
<section id="how-to-generate-n-grams-using-nltk">
<h2>How to generate n-grams using NLTK<a class="headerlink" href="#how-to-generate-n-grams-using-nltk" title="Permalink to this heading">#</a></h2>
<p>Here’s an example of how to generate n-grams using the Natural Language Toolkit (NLTK) library in Python.</p>
<p>First, make sure to install NLTK:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">pip</span> install nltk
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
Collecting nltk
  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">1.5/1.5 MB</span> <span class=" -Color -Color-Red">27.5 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>a <span class=" -Color -Color-Cyan">0:00:01</span>
?25hRequirement already satisfied: click in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk) (8.1.3)
Requirement already satisfied: tqdm in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk) (4.65.0)
Collecting joblib
  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">298.0/298.0 kB</span> <span class=" -Color -Color-Red">100.6 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25hRequirement already satisfied: regex&gt;=2021.8.3 in /home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages (from nltk) (2022.10.31)
Installing collected packages: joblib, nltk
Successfully installed joblib-1.2.0 nltk-3.8.1

<span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> A new release of pip is available: <span class=" -Color -Color-Red">23.0</span> -&gt; <span class=" -Color -Color-Green">23.0.1</span>
<span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> To update, run: <span class=" -Color -Color-Green">pip install --upgrade pip</span>
Note: you may need to restart the kernel to use updated packages.
</pre></div>
</div>
</div>
</div>
<p>Now, let’s generate bigrams, trigrams, and 4-grams using NLTK:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.util</span> <span class="kn">import</span> <span class="n">ngrams</span>

<span class="c1"># Download the punkt tokenizer</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>

<span class="c1"># Sample text</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;I love natural language processing and machine learning.&quot;</span>

<span class="c1"># Tokenize the text</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># Generate bigrams</span>
<span class="n">bigrams</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ngrams</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bigrams:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bigrams</span><span class="p">)</span>

<span class="c1"># Generate trigrams</span>
<span class="n">trigrams</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ngrams</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Trigrams:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">trigrams</span><span class="p">)</span>

<span class="c1"># Generate 4-grams (quadgrams)</span>
<span class="n">quadgrams</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ngrams</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">4-grams:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">quadgrams</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package punkt to /home/yjlee/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Bigrams:
[(&#39;I&#39;, &#39;love&#39;), (&#39;love&#39;, &#39;natural&#39;), (&#39;natural&#39;, &#39;language&#39;), (&#39;language&#39;, &#39;processing&#39;), (&#39;processing&#39;, &#39;and&#39;), (&#39;and&#39;, &#39;machine&#39;), (&#39;machine&#39;, &#39;learning&#39;), (&#39;learning&#39;, &#39;.&#39;)]

Trigrams:
[(&#39;I&#39;, &#39;love&#39;, &#39;natural&#39;), (&#39;love&#39;, &#39;natural&#39;, &#39;language&#39;), (&#39;natural&#39;, &#39;language&#39;, &#39;processing&#39;), (&#39;language&#39;, &#39;processing&#39;, &#39;and&#39;), (&#39;processing&#39;, &#39;and&#39;, &#39;machine&#39;), (&#39;and&#39;, &#39;machine&#39;, &#39;learning&#39;), (&#39;machine&#39;, &#39;learning&#39;, &#39;.&#39;)]

4-grams:
[(&#39;I&#39;, &#39;love&#39;, &#39;natural&#39;, &#39;language&#39;), (&#39;love&#39;, &#39;natural&#39;, &#39;language&#39;, &#39;processing&#39;), (&#39;natural&#39;, &#39;language&#39;, &#39;processing&#39;, &#39;and&#39;), (&#39;language&#39;, &#39;processing&#39;, &#39;and&#39;, &#39;machine&#39;), (&#39;processing&#39;, &#39;and&#39;, &#39;machine&#39;, &#39;learning&#39;), (&#39;and&#39;, &#39;machine&#39;, &#39;learning&#39;, &#39;.&#39;)]
</pre></div>
</div>
</div>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-gram Language Models</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/nlp_intro"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="research.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Research Applications</p>
      </div>
    </a>
    <a class="right-next"
       href="topic.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Topic Modeling</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-concept">General Concept</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-language-models">Why do we need language models?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n-gram-language-models">N-gram Language Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n-grams-and-probability-estimation">N-Grams and Probability Estimation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-probabilities-from-counts">Estimating Probabilities from Counts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bigram-model">Bigram Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-joint-probabilities-of-word-sequences">Estimating Joint Probabilities of Word Sequences</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n-gram-models-and-markov-assumption">N-gram Models and Markov Assumption</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stock-prices-and-markov-assumption">Stock Prices and Markov Assumption</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-bigram-or-n-gram-probabilities-using-maximum-likelihood-estimation-mle">Estimating Bigram or N-gram Probabilities using Maximum Likelihood Estimation (MLE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bigram-probability-calculation">Bigram Probability Calculation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-issues-in-n-gram-models">Practical Issues in N-gram Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-mle">Examples of MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-generate-n-grams-using-nltk">How to generate n-grams using NLTK</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://entelecheia.me">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>