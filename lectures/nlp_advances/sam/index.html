

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Segment Everything &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../../_static/tabs.js"></script>
    <script src="../../../_static/js/hoverxref.js"></script>
    <script src="../../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/chatgpt.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/nlp_advances/sam/index';</script>
    <link rel="canonical" href="https://lecture.entelecheia.ai/lectures/nlp_advances/sam/index.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="AI Art (Generative AI)" href="../../aiart/index.html" />
    <link rel="prev" title="Meet the Camelids: A Family of LLMs" href="../camelids.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content"><p>
  <strong>Announcement:</strong>
  You can install the accompanying AI tutor <a href="https://chrome.google.com/webstore/detail/lecturebot-for-%E1%BC%90%CE%BD%CF%84%CE%B5%CE%BB%CE%AD%CF%87%CE%B5%CE%B9%CE%B1/lfgfgbomindbccgidgalhhndggddpagd">here</a>.
</p>
</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_intro/index.html">Introduction to NLP</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/research.html">Research Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/language_models.html">Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/topic_modeling.html">Topic Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/topic_coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/sentiments.html">Sentiment Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/word_segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/vectorization.html">Vector Semantics and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/word_embeddings.html">Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/lab2-corpus-eda.html">Lab 2: EDA on Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/lab4-tomotopy.html">Lab 4: Topic Modeling Tools- Tomotopy</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_deep/index.html">Deep Learning for NLP</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/llms.html">Large Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/zeroshot.html">Zero Shot and Prompt Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/decoding.html">Decoding and Search Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/transformers.html">Transformers</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/bert/index.html">BERT: Bidirectional Encoder Representations from Transformers</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/bert/bertviz.html">BERT: Visualizing Attention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/plms.html">Pretrained Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/detectGPT.html">How to Spot Machine-Written Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/lab3-train-tokenizers.html">Lab 3: Training Tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_deep/lab4-pretraining-lms.html">Lab 4: Pretraining Language Models</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Advances in AI and NLP</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../thesis.html">Writing a Thesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../detectGPT.html">DetectGPT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpt4.html">GPT-4</a></li>
<li class="toctree-l2"><a class="reference internal" href="../camelids.html">Meet the Camelids: A Family of LLMs</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Segment Everything</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../aiart/index.html">AI Art (Generative AI)</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/aiart.html">Art and Music in Light of AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/imagen.html">Imagen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/textual-inversion.html">Textual Inversion (Dreambooth)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/whisper.html">Automatic Speech Recognition (Whisper)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/text2music.html">Text to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/image2music.html">Image to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/robot_drawings.html">Robot Drawing Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/project-themes.html">Project Themes - A Brave New World</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/devops.html">DevOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/gitops.html">GitOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/devsecops.html">DevSecOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/llmops.html">LLMOps - Large Language Model Operations</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/dotfiles/index.html">Dotfiles</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/dotfiles/age-gpg-ssh.html">SSH, GPG, and AGE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/dotfiles/pass.html">Unix Password Managers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/dotfiles/dotdrop.html">Dotdrop: A Powerful Tool for Managing Dotfiles</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/auth-enc-sign.html">Authentication, Encryption, and Signing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/github.html">Github’s Fork &amp; Pull Workflow</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/containerization/index.html">Containerization - Docker and containerd</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/containerd.html"><code class="docutils literal notranslate"><span class="pre">containerd</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/simple-pipeline/index.html">Simple MLOps Pipeline</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../ds/index.html">Data Science for Economics and Finance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../about/lecture-bot.html">LectureBot for ἐντελέχεια.άι</a></li>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about/index.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/lectures/nlp_advances/sam/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Segment Everything</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sam-a-generalized-approach-to-segmentation">SAM: A generalized approach to segmentation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-sam-works-promptable-segmentation">How SAM works: Promptable segmentation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-encoder">Image encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-encoder">Prompt encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lightweight-mask-decoder">Lightweight mask decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resolving-ambiguity">Resolving ambiguity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#losses">Losses</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segment-anything-data-engine">Segment Anything Data Engine</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segmenting-1-billion-masks">Segmenting 1 billion masks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="segment-everything">
<h1>Segment Everything<a class="headerlink" href="#segment-everything" title="Permalink to this heading">#</a></h1>
<a class="bg-primary mb-1 reference internal image-reference" href="../../../_images/segment-everything.jpeg"><img alt="Segment Everything" class="bg-primary mb-1 align-center" src="../../../_images/segment-everything.jpeg" style="width: 70%;" /></a>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>The Segment Anything project aims to make image segmentation more accessible and user-friendly. Image segmentation is the process of identifying which pixels in an image belong to a specific object. This task is essential in computer vision and has numerous applications, such as scientific image analysis and photo editing.</p>
<p>The project introduces a new task, dataset, and model for image segmentation. It consists of the <code class="docutils literal notranslate"><span class="pre">Segment</span> <span class="pre">Anything</span> <span class="pre">Model</span> <span class="pre">(SAM)</span></code> and the <code class="docutils literal notranslate"><span class="pre">Segment</span> <span class="pre">Anything</span> <span class="pre">1-Billion</span> <span class="pre">mask</span> <span class="pre">dataset</span> <span class="pre">(SA-1B)</span></code>, which is the largest segmentation dataset ever created. The dataset is available for research purposes, while the model is released under an open license (Apache 2.0).</p>
<p>The goal of Segment Anything is to <code class="docutils literal notranslate"><span class="pre">eliminate</span></code> the need for <code class="docutils literal notranslate"><span class="pre">specialized</span> <span class="pre">expertise</span></code>, <code class="docutils literal notranslate"><span class="pre">expensive</span> <span class="pre">computing</span> <span class="pre">resources</span></code>, and <code class="docutils literal notranslate"><span class="pre">custom</span> <span class="pre">data</span> <span class="pre">annotation</span></code> in image segmentation. The project aims to develop a foundation model for image segmentation, similar to how prompts are used in natural language processing models. This is achieved by training the model on diverse data and making it adaptable to specific tasks.</p>
<p>SAM is a versatile model that can generate masks for any object in any <code class="docutils literal notranslate"><span class="pre">image</span></code> or <code class="docutils literal notranslate"><span class="pre">video</span></code>, even if it hasn’t encountered them during training. It can be applied to various image “domains,” such as <code class="docutils literal notranslate"><span class="pre">underwater</span> <span class="pre">photography</span></code> or <code class="docutils literal notranslate"><span class="pre">cell</span> <span class="pre">microscopy</span></code>, without additional training (<code class="docutils literal notranslate"><span class="pre">zero-shot</span> <span class="pre">transfer</span></code>).</p>
<p>Potential applications for SAM include integration into larger AI systems for multimodal understanding, AR/VR applications, creative content creation, and scientific studies on Earth or in space. The project’s creators are excited about the wide range of potential use cases, including those not yet imagined.</p>
</section>
<section id="sam-a-generalized-approach-to-segmentation">
<h2>SAM: A generalized approach to segmentation<a class="headerlink" href="#sam-a-generalized-approach-to-segmentation" title="Permalink to this heading">#</a></h2>
<p>SAM is a groundbreaking image segmentation model that combines the benefits of both <code class="docutils literal notranslate"><span class="pre">interactive</span></code> and <code class="docutils literal notranslate"><span class="pre">automatic</span></code> segmentation approaches. It is a highly versatile model that can perform a wide range of segmentation tasks simply by using the right prompt, such as <code class="docutils literal notranslate"><span class="pre">clicks</span></code>, <code class="docutils literal notranslate"><span class="pre">boxes</span></code>, or <code class="docutils literal notranslate"><span class="pre">text</span></code>. This flexibility is a first in the field of image segmentation.</p>
<p>The model’s capabilities are powered by its training on a diverse and high-quality dataset of over 1 billion masks, enabling it to generalize to new objects and images that it hasn’t encountered during training. This means practitioners won’t need to gather their own segmentation data or fine-tune a model for their specific use case.</p>
<p>SAM’s features include:</p>
<ol class="arabic simple">
<li><p>Allowing users to segment objects with just a click, interactively clicking points to include or exclude from the object, or prompting with a bounding box.</p></li>
<li><p>Producing multiple valid masks when there’s ambiguity about the object being segmented, which is essential for real-world applications.</p></li>
<li><p>Automatically finding and masking all objects in an image.</p></li>
<li><p>Generating a segmentation mask for any prompt in real-time after precomputing the image embedding, allowing for swift interaction with the model.</p></li>
</ol>
</section>
<section id="how-sam-works-promptable-segmentation">
<h2>How SAM works: Promptable segmentation<a class="headerlink" href="#how-sam-works-promptable-segmentation" title="Permalink to this heading">#</a></h2>
<p>SAM is a state-of-the-art image segmentation model inspired by <code class="docutils literal notranslate"><span class="pre">prompting</span></code> techniques used in natural language processing and computer vision. The model is designed to generate valid segmentation masks for any prompt, which can include points, boxes, masks, text, or any other information indicating what to segment in an image.</p>
<p>To create a balance between quality and runtime, SAM uses a simple design that allows it to run in <code class="docutils literal notranslate"><span class="pre">real-time</span></code> on a <code class="docutils literal notranslate"><span class="pre">CPU</span></code> in a web browser. This feature enables annotators to interact with the model efficiently while annotating images.</p>
<p>SAM’s architecture consists of an <code class="docutils literal notranslate"><span class="pre">image</span> <span class="pre">encoder</span></code>, a <code class="docutils literal notranslate"><span class="pre">prompt</span> <span class="pre">encoder</span></code>, and a <code class="docutils literal notranslate"><span class="pre">decoder</span></code>. The image encoder produces a one-time embedding for the image, while the lightweight prompt encoder converts any given prompt into an embedding vector in real-time. These two pieces of information are combined in a lightweight decoder that predicts segmentation masks.</p>
<p>Once the image embedding is computed, SAM can produce a segmentation mask within just 50 milliseconds when given any prompt in a web browser. This makes the model highly efficient and suitable for various segmentation tasks in real-time.</p>
<figure class="align-default" id="fig-sam">
<a class="reference internal image-reference" href="../../../_images/sam.png"><img alt="../../../_images/sam.png" src="../../../_images/sam.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 94 </span><span class="caption-text">SAM: Universal segmentation model</span><a class="headerlink" href="#fig-sam" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="image-encoder">
<h3>Image encoder<a class="headerlink" href="#image-encoder" title="Permalink to this heading">#</a></h3>
<p>The image encoder is a key component of SAM that processes images and generates image embeddings. It is designed to be scalable and benefit from powerful pretraining methods. In this case, the encoder uses a pre-trained <code class="docutils literal notranslate"><span class="pre">Vision</span> <span class="pre">Transformer</span> <span class="pre">(ViT)</span></code> with minimal adaptations to handle high-resolution inputs.</p>
<p>The image encoder can be any network that generates a <code class="docutils literal notranslate"><span class="pre">CxHxW</span></code> image embedding. The specific encoder used here is based on the ViT-H/16 architecture, with 14x14 windowed attention and four equally-spaced global attention blocks. This setup outputs a 16x downscaled embedding of the input image.</p>
<p>The goal is to process each prompt in real-time, so the image encoder’s computations are performed only once per image, not per prompt. This allows for a higher number of FLOPs (floating-point operations per second) in the image encoder.</p>
<p>Images are processed at a 1024x1024 resolution by rescaling and padding the shorter side. The resulting image embedding is 64x64. To reduce the channel dimension, a 1x1 convolution is used to get 256 channels, followed by a 3x3 convolution, also with 256 channels. Each convolution is followed by layer normalization.</p>
</section>
<section id="prompt-encoder">
<h3>Prompt encoder<a class="headerlink" href="#prompt-encoder" title="Permalink to this heading">#</a></h3>
<p>The prompt encoder in SAM processes different types of prompts, which can be either sparse (points, boxes, text) or dense (masks). These prompts help the model understand what to segment in an image.</p>
<p>For sparse prompts, points and boxes are represented using positional encodings combined with learned embeddings for each prompt type. Free-form text is encoded using an off-the-shelf text encoder from the CLIP model.</p>
<p>For dense prompts, such as masks, convolutions are used to create embeddings. These embeddings are then combined element-wise with the image embedding.</p>
<p>In summary, the prompt encoder processes various types of prompts by generating suitable embeddings that are combined with the image embedding to guide the model in generating segmentation masks.</p>
</section>
<section id="lightweight-mask-decoder">
<h3>Lightweight mask decoder<a class="headerlink" href="#lightweight-mask-decoder" title="Permalink to this heading">#</a></h3>
<p>The mask decoder in SAM is responsible for converting the image and prompt embeddings into a final segmentation mask. It’s inspired by Transformer segmentation models and consists of a modified Transformer decoder block followed by a dynamic mask prediction head.</p>
<p>The decoder takes the image and prompt embeddings and performs several steps to update them: self-attention on the tokens, cross-attention from tokens to the image embedding, a point-wise MLP to update each token, and cross-attention from the image embedding to tokens. These steps are repeated in multiple decoder layers.</p>
<p>Positional encodings are added to the image embedding and prompt tokens whenever they participate in an attention layer, allowing the decoder to capture geometric information.</p>
<p>After the decoder has finished processing, the updated image embedding is upsampled using transposed convolutional layers. Then, the tokens attend once more to the image embedding, and an MLP processes the output token embedding. Finally, a mask is predicted by performing a point-wise product between the upscaled image embedding and the MLP’s output.</p>
<p>The design of the mask decoder aims to efficiently combine image and prompt embeddings to produce accurate segmentation masks while maintaining computational efficiency.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A Multilayer Perceptron (MLP) is a type of artificial neural network composed of multiple layers of interconnected nodes or neurons. It’s designed to process and learn patterns from data, such as images, text, or numerical values. Here’s a simple explanation of how it works:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Input</span> <span class="pre">layer</span></code>: The first layer of the MLP receives the input data, where each node represents a single data point (e.g., a pixel in an image or a word in a text).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Hidden</span> <span class="pre">layers</span></code>: These are the layers between the input and output layers, and they perform transformations on the data. Each node in a hidden layer is connected to every node in the previous layer, and each connection has a weight, which is a numerical value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Activation</span> <span class="pre">function</span></code>: Every node in the hidden layers has an activation function, such as the sigmoid or ReLU (Rectified Linear Unit) function. The activation function helps to introduce non-linearity into the network, enabling it to learn complex patterns.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Output</span> <span class="pre">layer</span></code>: The final layer of the MLP generates the output or prediction based on the processed input data. The number of nodes in the output layer depends on the type of problem being solved (e.g., classification, regression).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Training</span></code>: The MLP learns from the input data by adjusting the weights of the connections using a technique called backpropagation. The goal is to minimize the error between the predictions and the actual outputs.</p></li>
</ul>
</div>
</section>
<section id="resolving-ambiguity">
<h3>Resolving ambiguity<a class="headerlink" href="#resolving-ambiguity" title="Permalink to this heading">#</a></h3>
<p>SAM is designed to handle ambiguous prompts, which may correspond to multiple valid segmentation masks. To address this, the model predicts multiple masks simultaneously (typically three) instead of just one. These three masks often represent different layers, such as whole, part, and subpart, to describe nested masks.</p>
<p>During training, the loss is calculated between the ground truth and each of the predicted masks, but only the lowest loss is used for backpropagation. This technique is commonly used in models with multiple outputs. Additionally, a small head is added to estimate the Intersection over Union (IoU) between each predicted mask and the object it covers, allowing the masks to be ranked.</p>
<p>When multiple prompts are provided, ambiguity is less likely, and the three output masks often become similar. To minimize computation and ensure a single unambiguous mask receives a regular gradient signal, the model predicts a single mask when more than one prompt is given. This is achieved by adding a fourth output token for an additional mask prediction, which is never returned for a single prompt and is the only mask returned for multiple prompts.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Backpropagation is a widely-used algorithm in training neural networks. It helps adjust the weights of the network so that the model can make better predictions. Here’s a simple explanation of the process:</p>
<ol class="arabic simple">
<li><p>The neural network makes a prediction based on input data and its current weights.</p></li>
<li><p>The prediction is compared to the actual output (also called the target or ground truth) to calculate the error or difference.</p></li>
<li><p>The error is then used to update the weights of the network, starting from the last layer and moving backwards to the first layer. This process is called backpropagation because the error information flows from the output layer back towards the input layer.</p></li>
<li><p>The weights are adjusted in such a way that the error becomes smaller, improving the network’s performance.</p></li>
<li><p>This process is repeated multiple times using different input-output pairs from the training dataset, gradually refining the weights of the network until it can make accurate predictions.</p></li>
</ol>
</div>
</section>
<section id="losses">
<h3>Losses<a class="headerlink" href="#losses" title="Permalink to this heading">#</a></h3>
<p>The mask prediction in SAM is supervised using a combination of two loss functions: <code class="docutils literal notranslate"><span class="pre">focal</span> <span class="pre">loss</span></code> and <code class="docutils literal notranslate"><span class="pre">dice</span> <span class="pre">loss</span></code>. These losses are combined in a 20:1 ratio (20 parts focal loss to 1 part dice loss), as inspired by previous research. Focal loss helps address the class imbalance problem, while dice loss measures the overlap between the predicted and ground truth masks.</p>
<p>Unlike some earlier studies, it was found that using auxiliary deep supervision after each decoder layer didn’t help in this case. Additionally, the Intersection over Union (IoU) prediction head is trained using mean square error loss, comparing the predicted IoU and the actual IoU between the predicted and ground truth masks. This loss is then added to the mask loss with a constant scaling factor of 1.0.</p>
</section>
<section id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this heading">#</a></h3>
<p>The training algorithm for SAM (segmentation as multi-task learning) simulates an interactive segmentation setup. Here’s a simplified explanation of the process:</p>
<ol class="arabic simple">
<li><p>Initially, a random foreground point or bounding box is selected for the target mask. Points are sampled from the ground truth mask, and bounding boxes are created with some added noise.</p></li>
<li><p>After predicting a mask from the first prompt, more points are selected from the error region between the previous mask prediction and the ground truth mask.</p></li>
<li><p>The model uses the unthresholded mask logits from the previous iteration as an additional prompt to improve its prediction.</p></li>
<li><p>Up to 8 iteratively sampled points are used, with diminishing returns observed beyond that. Two extra iterations are added to encourage the model to refine its own mask predictions.</p></li>
<li><p>The lightweight mask decoder allows for a relatively large number of iterations since it requires only a small computational overhead.</p></li>
</ol>
<p>For training, they use the AdamW optimizer, a linear learning rate warmup, and a step-wise learning rate decay schedule. The model is trained for 90k iterations using a batch size of 256 images. Weight decay, drop path, and layer-wise learning rate decay are applied for regularization. No data augmentation is used. SAM is initialized from an MAE pre-trained ViT-H model and trained across 256 GPUs.</p>
<p>When training with data from the first and second data engine stages only, they apply large-scale jitter for data augmentation. Different training settings are used for ViT-B and ViT-L models, including variations in learning rate, layer-wise decay, weight decay, and drop path rates.</p>
</section>
</section>
<section id="segment-anything-data-engine">
<h2>Segment Anything Data Engine<a class="headerlink" href="#segment-anything-data-engine" title="Permalink to this heading">#</a></h2>
<p>To generate the released SA-1B, the data engine’s fully automatic stage was used. This stage is designed to generate masks for images that are not annotated, and it uses a special version of SAM to do so. Here’s a simplified explanation of the process:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Cropping</span></code>: Masks were generated from a grid of points on the full image and additional zoomed-in crops. High-resolution images were used for this purpose. Masks touching the inner boundaries of the crops were removed, and Non-Maximum Suppression (NMS) was applied to rank masks.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Filtering</span></code>: To improve mask quality, three filtering techniques were used:</p>
<ul>
<li><p>a. Confident masks were kept by using a predicted IoU score threshold of 88.0.</p></li>
<li><p>b. Stable masks were retained by comparing two binary masks from the same soft mask, keeping the prediction only if the IoU between them was equal to or greater than 95.0.</p></li>
<li><p>c. Uninteresting masks covering 95% or more of an image were removed.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Postprocessing</span></code>: To fix common errors in masks, two techniques were applied:</p>
<ul>
<li><p>a. Small, spurious components were removed by eliminating connected components with an area less than 100 pixels.</p></li>
<li><p>b. Small, spurious holes were filled by identifying components of inverted masks with an area less than 100 pixels.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Automatic</span> <span class="pre">mask</span> <span class="pre">generation</span> <span class="pre">model</span></code>: A special version of SAM was used for generating masks. This version had some differences from the default model, including longer training with data augmentation, using only point and mask prompts in simulated interactive training, and having three layers in the mask decoder instead of two.</p></li>
</ul>
</section>
<section id="segmenting-1-billion-masks">
<h2>Segmenting 1 billion masks<a class="headerlink" href="#segmenting-1-billion-masks" title="Permalink to this heading">#</a></h2>
<p>To train the SAM model, a massive and diverse dataset called <code class="docutils literal notranslate"><span class="pre">SA-1B</span></code> was created, as no such dataset existed before. The data was collected using SAM itself, through an iterative process where annotators interactively annotated images, and the model was updated accordingly.</p>
<p>The annotation process with SAM is much faster than previous methods, taking only about <code class="docutils literal notranslate"><span class="pre">14</span> <span class="pre">seconds</span></code> to annotate a mask, making it <code class="docutils literal notranslate"><span class="pre">6.5x</span> <span class="pre">faster</span></code> than COCO’s manual annotation and <code class="docutils literal notranslate"><span class="pre">2x</span> <span class="pre">faster</span></code> than the previous largest data annotation effort.</p>
<p>However, interactive annotation alone wouldn’t be sufficient to create the 1 billion mask dataset. So, a data engine was built with three “gears.” The first gear involved <code class="docutils literal notranslate"><span class="pre">model-assisted</span> <span class="pre">annotation</span></code>, the second combined <code class="docutils literal notranslate"><span class="pre">fully</span> <span class="pre">automatic</span> <span class="pre">annotation</span></code> with assisted annotation to increase mask diversity, and the third gear involved <code class="docutils literal notranslate"><span class="pre">fully</span> <span class="pre">automatic</span> <span class="pre">mask</span> <span class="pre">creation</span></code> to scale the dataset.</p>
<p>The final SA-1B dataset consists of over <code class="docutils literal notranslate"><span class="pre">1.1</span> <span class="pre">billion</span></code> segmentation masks collected from around 11 million licensed and privacy-preserving images. It has <code class="docutils literal notranslate"><span class="pre">400x</span></code> more masks than any existing dataset, and the masks are of high quality and diversity, even comparable to smaller, fully manually annotated datasets.</p>
<p>SA-1B includes images from <code class="docutils literal notranslate"><span class="pre">multiple</span> <span class="pre">countries</span></code>, covering diverse geographic regions and income levels. Although certain regions are underrepresented, the dataset has better overall representation compared to previous segmentation datasets. SAM’s performance across different perceived <code class="docutils literal notranslate"><span class="pre">gender</span> <span class="pre">presentations</span></code>, <code class="docutils literal notranslate"><span class="pre">skin</span> <span class="pre">tones</span></code>, and <code class="docutils literal notranslate"><span class="pre">age</span> <span class="pre">ranges</span></code> is similar, making the model more equitable for real-world use cases.</p>
<p>SA-1B not only supports SAM but can also help other researchers train foundation models for image segmentation. It may also serve as the basis for new datasets with additional annotations, such as text descriptions for each mask.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<figure class="align-default" id="fig-sam-demo">
<a class="reference internal image-reference" href="https://scontent-ssn1-1.xx.fbcdn.net/v/t39.2365-6/339964818_688443716371831_1075393517353493400_n.gif?_nc_cat=105&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=M_hK3WKmxT0AX8SztlP&amp;_nc_ht=scontent-ssn1-1.xx&amp;oh=00_AfAb7Q6pnOu9Zigs7YPny01FgCt5n3LFlqcjRsOyr0C27w&amp;oe=643ACD5C"><img alt="https://scontent-ssn1-1.xx.fbcdn.net/v/t39.2365-6/339964818_688443716371831_1075393517353493400_n.gif?_nc_cat=105&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=M_hK3WKmxT0AX8SztlP&amp;_nc_ht=scontent-ssn1-1.xx&amp;oh=00_AfAb7Q6pnOu9Zigs7YPny01FgCt5n3LFlqcjRsOyr0C27w&amp;oe=643ACD5C" src="https://scontent-ssn1-1.xx.fbcdn.net/v/t39.2365-6/339964818_688443716371831_1075393517353493400_n.gif?_nc_cat=105&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=M_hK3WKmxT0AX8SztlP&amp;_nc_ht=scontent-ssn1-1.xx&amp;oh=00_AfAb7Q6pnOu9Zigs7YPny01FgCt5n3LFlqcjRsOyr0C27w&amp;oe=643ACD5C" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 95 </span><span class="caption-text">In the future, SAM could be used to identify everyday items via AR glasses that could prompt users with reminders and instructions.</span><a class="headerlink" href="#fig-sam-demo" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-sam-demo2">
<a class="reference internal image-reference" href="https://scontent-ssn1-1.xx.fbcdn.net/v/t39.2365-6/10000000_194265759985577_710186242221904122_n.gif?_nc_cat=104&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=Vg1Qs6bwnZAAX_pMiVd&amp;_nc_ht=scontent-ssn1-1.xx&amp;oh=00_AfDUDnHOlajymA_TYVsWw6qvNaiJMdMK0GvllZBvfqxLZQ&amp;oe=64391C6A"><img alt="https://scontent-ssn1-1.xx.fbcdn.net/v/t39.2365-6/10000000_194265759985577_710186242221904122_n.gif?_nc_cat=104&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=Vg1Qs6bwnZAAX_pMiVd&amp;_nc_ht=scontent-ssn1-1.xx&amp;oh=00_AfDUDnHOlajymA_TYVsWw6qvNaiJMdMK0GvllZBvfqxLZQ&amp;oe=64391C6A" src="https://scontent-ssn1-1.xx.fbcdn.net/v/t39.2365-6/10000000_194265759985577_710186242221904122_n.gif?_nc_cat=104&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=Vg1Qs6bwnZAAX_pMiVd&amp;_nc_ht=scontent-ssn1-1.xx&amp;oh=00_AfDUDnHOlajymA_TYVsWw6qvNaiJMdMK0GvllZBvfqxLZQ&amp;oe=64391C6A" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 96 </span><span class="caption-text">SAM has the potential to impact a wide range of domains — perhaps one day helping farmers in the agricultural sector or assisting biologists in their research.</span><a class="headerlink" href="#fig-sam-demo2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>By sharing their research and the extensive SA-1B dataset, the creators of SAM aim to accelerate advancements in segmentation and broader image and video understanding. As a promptable segmentation model, SAM can be part of larger systems and be utilized in various ways, even for tasks not initially considered during the model’s design.</p>
<p>The concept of composable system design, combined with prompt engineering, allows for more versatile applications than systems designed for fixed tasks. SAM has the potential to become a crucial component in a wide range of domains, such as AR/VR, content creation, scientific research, and more comprehensive AI systems.</p>
<p>Looking forward, a closer integration of pixel-level image understanding and higher-level semantic comprehension of visual content could lead to even more powerful AI systems in the future.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://segment-anything.com">Segment Everything</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2304.02643.pdf">Segment Everython Paper</a></p></li>
<li><p><a class="reference external" href="https://github.com/facebookresearch/segment-anything">Segment Everython GitHub</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/nlp_advances/sam"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../camelids.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Meet the Camelids: A Family of LLMs</p>
      </div>
    </a>
    <a class="right-next"
       href="../../aiart/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">AI Art (Generative AI)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sam-a-generalized-approach-to-segmentation">SAM: A generalized approach to segmentation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-sam-works-promptable-segmentation">How SAM works: Promptable segmentation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-encoder">Image encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-encoder">Prompt encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lightweight-mask-decoder">Lightweight mask decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resolving-ambiguity">Resolving ambiguity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#losses">Losses</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segment-anything-data-engine">Segment Anything Data Engine</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segmenting-1-billion-masks">Segmenting 1 billion masks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://entelecheia.me">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>