
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>PEFT for LLMs &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/slide.css?v=06ccce15" />
    <link rel="stylesheet" type="text/css" href="../../../_static/dataframe.css?v=574a5d82" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../../_static/tabs.js?v=3ee01567"></script>
    <script src="../../../_static/js/hoverxref.js"></script>
    <script src="../../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/llms/peft/peft-llms';</script>
    <link rel="canonical" href="https://lecture.entelecheia.ai/lectures/llms/peft/peft-llms.html" />
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="PEFT in HuggingFace Libraries" href="peft-hf.html" />
    <link rel="prev" title="Parameter-Efficient Fine-Tuning (PEFT)" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Courses</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_intro/index.html">Introduction to NLP</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/intro/index.html">Introduction</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/apps/index.html">NLP Applications</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/apps/research1.html">Research Part I</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/apps/research2.html">Research Part II</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/lm/index.html">Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/lm/ngram.html">N-gram Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/lm/usage.html">Usage of Language Models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/datasets/index.html">Datasets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/datasets/corpus.html">Text Data Collection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/datasets/lab-dart.html">Lab: Crawling DART Data</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/topic/index.html">Topic Modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/methods.html">Topic Modeling Methodologies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/lab-methods.html">Lab: Topic Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/lab-coherence.html">Lab: Topic Coherence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/tomotopy.html">Lab: Tomotopy</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/sentiments/index.html">Sentiment Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lexicon.html">Lexicon-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/ml.html">Machine Learning-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lab-lexicon.html">Lab: Lexicon-based Sentiment Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lab-ml.html">Lab: ML-based Sentiment Classification</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/tokenization/index.html">Tokenization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/tokenization.html">Understanding the Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/pos.html">Part-of-Speech Tagging and Parsing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/ngrams.html">N-grams for Tokenization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/korean.html">Tokenization in Korean</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/lab-tokenization.html">Lab: Tokenization and Pre-processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/lab-korean.html">Lab: Korean Text Processing</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/vectorization/index.html">Vector Representation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/semantics.html">Vector Semantics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/bow.html">Bags of Words Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/tf-idf.html">TF-IDF Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/similarity.html">Word Similarity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/lab-similarity.html">Lab: Word Similarity</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/embeddings/index.html">Word Embeddings</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/nlm.html">Neural Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/w2v.html">Word2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/glove.html">GloVe</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/fasttext.html">FastText</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_deep/index.html">Deep Learning for NLP</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/llms/index.html">Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/zeroshot.html">Zero Shot and Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/decoding.html">Decoding and Search Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/plms.html">Pretrained Language Models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/transformers/index.html">Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/bert.html">BERT: Bidirectional Encoder Representations from Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/bertviz.html">BERT: Visualizing Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/datasets/index.html">Datasets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/datasets/mc4.html">mC4 Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/datasets/lab-eda.html">Lab: Exploratory Data Analysis (EDA)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/tokenization/index.html">Tokenization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/subword.html">Subword Tokenization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/pipeline.html">Tokenization Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/bpe.html">BPE Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/wordpiece.html">WordPiece Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/unigram.html">Unigram Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/lab-train-tokenizers.html">Lab: Training Tokenizers</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/training/index.html">Training Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-pretrain-mlm.html">Lab: Pretraining LMs - MLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-pretrain-clm.html">Lab: Pretraining LMs - CLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-finetune-mlm.html">Lab: Finetuining a MLM</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/chatbots/index.html">Conversational AI and Chatbots</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/chatbots/rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/chatbots/detectGPT.html">How to Spot Machine-Written Texts</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_advances/index.html">Advances in AI and NLP</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/thesis.html">Writing a Thesis</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_advances/gpt/index.html">Generative Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/detectGPT.html">DetectGPT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/gpt4.html">GPT-4</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/camelids.html">Meet the Camelids: A Family of LLMs</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/sam/index.html">Segment Anything</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../aiart/index.html">AI Art (Generative AI)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/intro/index.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/brave/index.html">A Brave New World</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/aiart/index.html">Art and Music in Light of AI</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../aiart/text-to-image/index.html">Text-to-Image Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/imagen.html">Imagen</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/motion-capture-and-synthesis/index.html">Motion Capture and Motion Synthesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/robot/index.html">Robot Drawing System</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mlops/index.html">Machine Learning Systems Design</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/devops/index.html">DevOps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/gitops.html">GitOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/devsecops.html">DevSecOps</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/dotfiles/index.html">Dotfiles</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai">Dotfiles Project</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dotdrop/">Dotdrop Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/github/">GitHub Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/ssh-gpg-age/">SSH, GPG, and Age Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/sops/">SOPS Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/pass/">Pass and Passage Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/doppler/">Doppler Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dockerfiles/">Dockerfiles Usage</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/security/index.html">Security Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/auth-enc-sign.html">Authentication, Encryption, and Signing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/age-gpg-ssh.html">SSH, GPG, and AGE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/pass.html">Unix Password Managers</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/github/index.html">GitHub Workflow</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/fork-pull.html">Github’s Fork &amp; Pull Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/template.html">Project Templating Tools</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-python.entelecheia.ai/">Hyperfast Python Template</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-template.entelecheia.ai/">Hyperfast Template</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/containerization/index.html">Containerization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/docker.html">Docker</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/containerd.html"><code class="docutils literal notranslate"><span class="pre">containerd</span></code></a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/simple-pipeline/index.html">Simple MLOps Pipeline</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/simple-pipeline/server.html">Server Setup &amp; Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/simple-pipeline/vpn.html">VPN Connectivity</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/llmops/index.html">LLMOps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/llmops/bentoml.html">Introduction to BentoML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/llmops/bentochain.html">Deploy a Voice-Based Chatbot with BentoML, LangChain, and Gradio</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsecon/index.html">Data Science for Economics and Finance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/intro/index.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/intro/introduction.html">Data Science in Economics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/intro/challenges.html">Technical Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/intro/methods.html">Data Analytics Methods</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/cb/index.html">Central Banks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/cb/altdata.html">Alternative Data Sources for Central Banks</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/fomc/index.html">Textual Analysis of FOMC contents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/01_numerical_data.html">Preparing Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/02_textual_data.html">Preparing Textual Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/03_EDA_numericals1.html">EDA on Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/03_EDA_numericals2.html">EDA on Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/04_training_datasets.html">Create Training Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/05_features.html">Visualizing Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/06_AutoML.html">Checking Baseline with AutoML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/07_predict_sentiments.html">Predicting Sentiments of FOMC Corpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/08_EDA_sentiments1.html">EDA on Sentiments: Correlation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/08_EDA_sentiments2.html">EDA on Sentiment Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/09_visualize_features.html">Visualize Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/10_monetary_shocks.html">Monetary Policy Shocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/11_AutoML_with_tones.html">Predicting the next decisions with tones</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/esg-ratings/index.html">ESG Ratings</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/prepare_datasets.html">Preparing training datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/improve_datasets.html">Improving classification datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/train_classifiers.html">Training Classifiers for ESG Ratings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/build_news_corpus.html">Building <code class="docutils literal notranslate"><span class="pre">econ_news_kr</span></code> corpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/predict_esg_classes.html">Predicting ESG Categories and Polarities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/cross_validate_datasets.html">Cross validating datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/prepare_datasets_for_labeling.html">Preparing active learning data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/all_in_one_pipeline.html">Putting them together in a pipeline</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../softeng/index.html">Software Engineering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/intro/index.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/introduction.html">Software Engineering?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/processes.html">Software Processes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/sdlc.html">Software Development Life Cycle (SDLC)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/requirements.html">Requirements Engineering (RE)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/proposal/index.html">Project Proposal</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/proposal/steps.html">Steps in Software Engineering Projects</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/proposal/guidelines.html">Software Engineering Proposal Guideline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/proposal/template.html">Project Proposal Template</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/vcs/index.html">Version Control Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/00_introduction.html">0. Introduction to version control</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/01_solo_work_with_git.html">1. Solo work with git</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/02_fixing_mistakes.html">2. Fixing mistakes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/03_publishing.html">3. Publishing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/04_collaboration.html">4. Collaboration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/05_fork_and_pull.html">5. Fork and Pull</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/06_git_theory.html">6. Git Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/07_branches.html">7. Branches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/08_advanced_git_concepts.html">8. Advanced git concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/09_github_pages.html">9. Publishing from GitHub</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/10_rebasing.html">10. Rebasing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/11_debugging_with_git_bisect.html">11. Debugging With git bisect</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/12_multiple_remotes.html">12. Working with multiple remotes</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/spm/index.html">Software Process Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/spm/agile.html">Agile Software Development</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/devops/index.html">DevOps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/devops/gitops.html">GitOps</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Large Language Models</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../intro/index.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../intro/llms.html">Large Language Models?</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../stack/index.html">LLM Stacks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../stack/infra.html">Generative AI Infrastructure Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../stack/architecture.html">LLM Application Architectures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../stack/app.html">LLM App Ecosystem</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../agents/index.html">AI Agents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../agents/autogen.html">AutoGen</a></li>
<li class="toctree-l3"><a class="reference internal" href="../agents/autoscraper.html">AutoGen AutoScraper Agent</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../finetune/index.html">LLM Fine-tuning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetune/autotrain.html">Fine-Tuning LLMs with Hugging Face AutoTrain</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/index.html">Retrieval Augmented Generation (RAG)</a></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="index.html">Parameter-Efficient Fine-Tuning (PEFT)</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">PEFT for LLMs</a></li>
<li class="toctree-l3"><a class="reference internal" href="peft-hf.html">PEFT in HuggingFace Libraries</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../q-learning/index.html">Q-Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../q-learning/qstar.html">Q-Star (Q*)</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../about/lecture-bot.html">LectureBot for ἐντελέχεια.άι</a></li>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference external" href="https://course.entelecheia.ai">course.entelecheia.ai</a></li>
<li class="toctree-l1"><a class="reference external" href="https://research.entelecheia.ai">research.entelecheia.ai</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about/index.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/lectures/llms/peft/peft-llms.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>PEFT for LLMs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-fine-tuning-vs-peft">Standard Fine-Tuning vs. PEFT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-security-and-privacy-concerns">Data Security and Privacy Concerns</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-of-data-quality">Importance of Data Quality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparative-analysis">Comparative Analysis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#who-uses-peft">Who Uses PEFT?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#current-adoption-of-peft">Current Adoption of PEFT</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face">Hugging Face 🤗</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#googles-vertex-ai">Google’s Vertex AI</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#openai">OpenAI</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-transformer-review">Quick Transformer Review</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts-of-transformer-architecture">Core Concepts of Transformer Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-mechanism">Self-Attention Mechanism</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-block-structure">Transformer Block Structure</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-in-peft">Importance in PEFT</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#peft-techniques-overview">PEFT Techniques Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additive-methods">Additive Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#soft-prompts">Soft-Prompts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reparameterization-based-methods">Reparameterization-Based Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selective-methods">Selective Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-methods">Comparison of Methods</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="peft-for-llms">
<h1>PEFT for LLMs<a class="headerlink" href="#peft-for-llms" title="Link to this heading">#</a></h1>
<p>Large Language Models (LLMs) have revolutionized the field of natural language processing. Their vast sizes, ranging from millions to billions of parameters, present considerable challenges in computational resources and training costs. For instance, loading a 70 billion parameter model necessitates an immense 280 GB of GPU memory. The extensive resources and high expenses required for training these models underscore the need for more efficient methodologies. Parameter-Efficient Fine-Tuning (PEFT) emerges as a solution, offering techniques to adapt segments of these colossal models to specific tasks, thereby reducing computational burdens.</p>
<p>In this context, the necessity for more efficient approaches to leverage these models is clear, especially for those with limited datasets and computational resources. This is where Parameter-Efficient Fine-Tuning (PEFT) comes into play. PEFT offers a suite of techniques that enable the adaptation of small sections of these massive models to specific tasks, thereby reducing the overall computational burden.</p>
<p>This lecture aims to provide a comprehensive understanding of each PEFT technique currently implemented in Hugging Face, delineating the differences among them. This guide fills the gaps identified in these materials and serves as a conceptual foundation for all PEFT methods present in Hugging Face. The goal is to equip readers with a fundamental understanding of PEFT, facilitating their engagement with research literature on additional PEFT techniques.</p>
<section id="standard-fine-tuning-vs-peft">
<h2>Standard Fine-Tuning vs. PEFT<a class="headerlink" href="#standard-fine-tuning-vs-peft" title="Link to this heading">#</a></h2>
<p>Fine-tuning Large Language Models (LLMs) is a complex process with implications for model performance, data security, and resource utilization. The choice between standard fine-tuning and Parameter-Efficient Fine-Tuning (PEFT) depends on various factors, including data availability, computational resources, and specific use-case requirements.</p>
<section id="data-security-and-privacy-concerns">
<h3>Data Security and Privacy Concerns<a class="headerlink" href="#data-security-and-privacy-concerns" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Data Retention in LLMs</strong>: Research indicates that LLMs can retain a fraction of their training data. The risk of data retention increases with data duplication, leading to potential privacy and security issues, especially when models are externally accessible.</p></li>
<li><p><strong>Prompt Injection Attacks</strong>: One significant risk associated with fine-tuned LLMs is prompt injection attacks, where malicious inputs can trigger the model to reveal sensitive training data.</p></li>
<li><p><strong>In-Context Learning (ICL) as an Alternative</strong>: For scenarios where data security is paramount, ICL, which involves dynamic observation selection, can be a safer alternative. It allows the model to adapt to new tasks without explicit fine-tuning, reducing the risk of data leakage.</p></li>
</ul>
</section>
<section id="importance-of-data-quality">
<h3>Importance of Data Quality<a class="headerlink" href="#importance-of-data-quality" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Impact on Fine-Tuning Success</strong>: The success of both standard and PEFT largely depends on the quality of data labels. Poorly labeled data can lead to ineffective fine-tuning, regardless of the method used.</p></li>
<li><p><strong>Data Labeling Commitment</strong>: High-quality data labeling is crucial, particularly for standard fine-tuning, which involves a more extensive modification of the model.</p></li>
</ul>
</section>
<section id="comparative-analysis">
<h3>Comparative Analysis<a class="headerlink" href="#comparative-analysis" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p><strong>Parameter-efficient Fine-tuning</strong></p></th>
<th class="head"><p><strong>Standard Fine-tuning</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Goal</strong></p></td>
<td><p>Tailor a pre-trained model to specific tasks efficiently with limited data and computational resources.</p></td>
<td><p>Enhance model performance on specific tasks using comprehensive training with ample data and resources.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Training Data</strong></p></td>
<td><p>Optimal for scenarios with limited datasets, typically requiring fewer examples.</p></td>
<td><p>Requires large datasets with numerous examples for effective training.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Training Time</strong></p></td>
<td><p>Generally offers a quicker training process due to fewer parameters being tuned.</p></td>
<td><p>Involves a longer training duration due to the extensive tuning of all model parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Computational Resources</strong></p></td>
<td><p>More resource-efficient, utilizing fewer computational resources.</p></td>
<td><p>Demands significant computational resources for processing large models and datasets.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Model Parameters</strong></p></td>
<td><p>Involves modifying a small, specific subset of the model’s parameters.</p></td>
<td><p>Entails re-training the entire model, adjusting all parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Overfitting</strong></p></td>
<td><p>Reduced risk of overfitting due to limited modifications to the model.</p></td>
<td><p>Higher potential for overfitting as the model undergoes extensive modifications.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Training Performance</strong></p></td>
<td><p>Achieves good performance, although it may not match the levels attained by full fine-tuning.</p></td>
<td><p>Typically yields better performance, optimizing the model’s capabilities to the fullest.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Use Cases</strong></p></td>
<td><p>Suitable for applications in low-resource settings or when extensive training data isn’t available.</p></td>
<td><p>Ideal for well-resourced environments with access to large amounts of training data.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Selecting between standard fine-tuning and PEFT involves balancing various factors. Standard fine-tuning is more resource-intensive but can lead to higher model performance, making it suitable for scenarios with sufficient computational resources and data. In contrast, PEFT is ideal for situations where resources are limited or when working with smaller datasets. It offers a practical way to leverage the capabilities of large models without the extensive resource requirements of standard fine-tuning. Additionally, considerations of data privacy and security are crucial in deciding the most appropriate approach for fine-tuning LLMs.</p>
</section>
</section>
<section id="who-uses-peft">
<h2>Who Uses PEFT?<a class="headerlink" href="#who-uses-peft" title="Link to this heading">#</a></h2>
<p>Parameter-Efficient Fine-Tuning (PEFT) has garnered significant attention and adoption across various sectors of the artificial intelligence and machine learning industry, especially among entities that specialize in language model services. The appeal of PEFT lies in its ability to adapt large-scale pre-trained language models (PLMs) to specific tasks with minimal resource overhead.</p>
<section id="current-adoption-of-peft">
<h3>Current Adoption of PEFT<a class="headerlink" href="#current-adoption-of-peft" title="Link to this heading">#</a></h3>
<section id="hugging-face">
<h4>Hugging Face 🤗<a class="headerlink" href="#hugging-face" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Wide Range of PEFT Techniques</strong>: Hugging Face, a prominent player in the field of AI and NLP, offers an extensive suite of PEFT techniques through its platform. This includes various innovative methods such as LoRA, Prefix Tuning, and P-Tuning, among others.</p></li>
<li><p><strong>Open-Source Accessibility</strong>: Hugging Face’s commitment to open-source technology has made PEFT techniques more accessible to the wider AI community, allowing researchers and developers to experiment with and implement these methods in their projects.</p></li>
<li><p><strong>Community Contributions</strong>: The active community around Hugging Face contributes to the continuous development and refinement of PEFT methods, ensuring they stay at the forefront of AI research and application.</p></li>
</ul>
</section>
<section id="googles-vertex-ai">
<h4>Google’s Vertex AI<a class="headerlink" href="#googles-vertex-ai" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Reference to PEFT Research</strong>: Google’s Vertex AI, known for providing advanced machine learning services, references the survey by Liali et al. on PEFT in its tuning guide.</p></li>
<li><p><strong>Opaque Implementation Details</strong>: While Google acknowledges the significance of PEFT, the specifics of how Vertex AI implements these techniques are not fully disclosed. This opacity leaves room for speculation about the extent and manner of PEFT’s integration into Google’s AI offerings.</p></li>
</ul>
</section>
<section id="openai">
<h4>OpenAI<a class="headerlink" href="#openai" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Current Non-Implementation</strong>: As per a now-removed blog post, OpenAI, renowned for its GPT models, has not yet incorporated PEFT methods into its suite of tools and services.</p></li>
<li><p><strong>Potential Future Adoption</strong>: Given OpenAI’s reputation for pioneering AI research and development, it is anticipated that PEFT will eventually be part of its toolset. The adoption of PEFT by OpenAI could be driven by the increasing demand for efficient fine-tuning methods and the need to make large models like GPT more accessible and adaptable.</p></li>
</ul>
</section>
</section>
</section>
<section id="quick-transformer-review">
<h2>Quick Transformer Review<a class="headerlink" href="#quick-transformer-review" title="Link to this heading">#</a></h2>
<p>The Transformer architecture, introduced by Vaswani et al. in their seminal paper “Attention Is All You Need,” has become a cornerstone in modern natural language processing (NLP) and machine learning. This architecture is fundamental to understanding the nuances of Parameter-Efficient Fine-Tuning (PEFT) techniques.</p>
<section id="core-concepts-of-transformer-architecture">
<h3>Core Concepts of Transformer Architecture<a class="headerlink" href="#core-concepts-of-transformer-architecture" title="Link to this heading">#</a></h3>
<section id="self-attention-mechanism">
<h4>Self-Attention Mechanism<a class="headerlink" href="#self-attention-mechanism" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Principle</strong>: At the heart of the Transformer is the self-attention mechanism, which enables the model to weigh the importance of different words in a sentence for a given task. Unlike previous architectures that processed words in sequence, the Transformer treats input data in parallel, allowing it to capture complex word relationships more effectively.</p></li>
<li><p><strong>Functionality</strong>: Self-attention works by creating three vectors for each input token (word): the query vector, key vector, and value vector. These vectors are derived through learned linear transformations.</p></li>
<li><p><strong>Attention Calculation</strong>: The attention scores are computed by taking the dot product of the query vector with key vectors of all tokens and applying a softmax function. These scores determine how much focus to put on other parts of the input sentence as each word is processed.</p></li>
<li><p><strong>Scaled Dot-Product Attention</strong>: The scores are then multiplied with the value vectors and summed up, resulting in the output for each token. This process is scaled by the square root of the dimension of key vectors to prevent gradients from vanishing or exploding.</p></li>
</ul>
</section>
<section id="transformer-block-structure">
<h4>Transformer Block Structure<a class="headerlink" href="#transformer-block-structure" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Layers</strong>: A standard transformer block consists of two main layers: the multi-head self-attention layer and the position-wise feed-forward network.</p>
<ul>
<li><p><strong>Multi-Head Attention</strong>: This layer extends the self-attention mechanism by having multiple ‘heads’, each performing attention calculations independently. The outputs of these heads are then concatenated and linearly transformed into the desired dimension. This multi-head approach allows the model to attend to information from different representation subspaces at different positions.</p></li>
<li><p><strong>Feed-Forward Network</strong>: Following the attention layer, the output passes through a position-wise feed-forward network, typically comprising two linear transformations with an activation function in between.</p></li>
</ul>
</li>
<li><p><strong>Normalization and Residual Connections</strong>: Each of these layers is followed by layer normalization and is equipped with residual connections. This means the output of each layer is the sum of its input and its processed result, enhancing training stability and allowing deeper networks.</p></li>
<li><p><strong>Positional Encoding</strong>: Since the Transformer doesn’t inherently capture the sequential nature of data, positional encodings are added to the input embeddings to give the model information about the position of each token in the sequence.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Project the input to create key (k), query (q), and value (v) matrices</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_k</span>  <span class="c1"># Key matrix is obtained by multiplying the input with weight W_k</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_q</span>  <span class="c1"># Query matrix is obtained by multiplying the input with weight W_q</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_v</span>  <span class="c1"># Value matrix is obtained by multiplying the input with weight W_v</span>

    <span class="c1"># Calculate the attention scores and apply them to the value matrix</span>
    <span class="c1"># Softmax is applied to the dot product of q and the transpose of k for normalization</span>
    <span class="c1"># The resulting attention scores are then multiplied with the value matrix v</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>

<span class="k">def</span> <span class="nf">transformer_block</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Store the original input for the residual connection</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

    <span class="c1"># Apply self-attention to the input</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Add the residual (original input) to the output of the self-attention layer</span>
    <span class="c1"># and apply layer normalization</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span>

    <span class="c1"># Store the output of the first sub-layer for the next residual connection</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

    <span class="c1"># Apply the Feed-Forward Network (FFN) to the output of the first sub-layer</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">FFN</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Add the residual (output of the first sub-layer) to the output of the FFN</span>
    <span class="c1"># and apply layer normalization</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span>

    <span class="c1"># Return the output of the transformer block</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>In this code snippet, the <code class="docutils literal notranslate"><span class="pre">self_attention</span></code> function defines the self-attention mechanism used in transformer models, projecting input <code class="docutils literal notranslate"><span class="pre">x</span></code> into key, query, and value matrices and applying the attention mechanism. The <code class="docutils literal notranslate"><span class="pre">transformer_block</span></code> function represents a single block of a transformer model, which includes a self-attention layer followed by a feed-forward network (FFN), both supplemented with residual connections and layer normalization for stability and performance improvement.</p>
</section>
</section>
<section id="importance-in-peft">
<h3>Importance in PEFT<a class="headerlink" href="#importance-in-peft" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>PEFT Adaptations</strong>: Understanding the Transformer’s architecture is crucial for comprehending how PEFT techniques modify it. PEFT methods typically intervene at the level of the self-attention mechanism and the feed-forward networks, adjusting or augmenting the model’s parameters efficiently for task-specific fine-tuning.</p></li>
<li><p><strong>Targeting Specific Components</strong>: Depending on the PEFT technique used, modifications may be made to specific components like the attention heads or layers of the feed-forward network, enabling the fine-tuning process to be both efficient and effective with fewer trainable parameters.</p></li>
</ul>
</section>
</section>
<section id="peft-techniques-overview">
<h2>PEFT Techniques Overview<a class="headerlink" href="#peft-techniques-overview" title="Link to this heading">#</a></h2>
<section id="additive-methods">
<h3>Additive Methods<a class="headerlink" href="#additive-methods" title="Link to this heading">#</a></h3>
<p><strong>Adapters</strong>: Introduced by Houlsby et al., this technique involves adding small fully connected networks after Transformer sub-layers. The pseudo code for an adapted transformer block illustrates these additions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transformer_block_adapter</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Store the original input for the residual connection</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

    <span class="c1"># Apply self-attention to the input</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Apply the first Feed-Forward Network (FFN), which acts as an adapter in this context</span>
    <span class="c1"># Adapters are small networks added after the transformer sub-layers</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">FFN</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Adapter layer</span>

    <span class="c1"># Add the residual (original input) to the output of the adapter layer</span>
    <span class="c1"># and apply layer normalization</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span>

    <span class="c1"># Store the output of the first sub-layer (including the adapter) for the next residual connection</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

    <span class="c1"># Apply the second Feed-Forward Network (FFN) for further transformation</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">FFN</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Apply another adapter after the second FFN</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">FFN</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Second adapter layer</span>

    <span class="c1"># Add the residual (output of the first adapter layer) to the output of the second adapter layer</span>
    <span class="c1"># and apply layer normalization</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span>

    <span class="c1"># Return the output of the transformer block with adapters</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>In this code snippet, the <code class="docutils literal notranslate"><span class="pre">transformer_block_adapter</span></code> function represents a transformer block modified to include adapters, as introduced by Houlsby et al. Adapters are additional small feed-forward networks inserted after each sub-layer within the transformer block (after self-attention and the original FFN). These adapters provide a means to fine-tune the pre-trained transformer models efficiently by only training the parameters within these adapter layers, rather than the entire model. This approach helps in adapting the model to specific tasks while keeping the computational overhead low.</p>
<p><strong>(IA)³</strong>: This method, proposed by Liu et al., augments the transformer block with additional column vectors (<span class="math notranslate nohighlight">\(l_k\)</span>, <span class="math notranslate nohighlight">\(l_v\)</span>) that modify the key and value matrices in the attention mechanism. This is done without strictly adding fully connected layers, distinguishing it from traditional adapter methods.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">self_attention_ia3</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Project the input to create key (k), query (q), and value (v) matrices</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_k</span>  <span class="c1"># Key matrix is obtained by multiplying the input with weight W_k</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_q</span>  <span class="c1"># Query matrix is obtained by multiplying the input with weight W_q</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_v</span>  <span class="c1"># Value matrix is obtained by multiplying the input with weight W_v</span>

    <span class="c1"># IA³ augmentation: Modify the key and value matrices using additional column vectors l_k and l_v</span>
    <span class="c1"># These additional vectors allow for specific adjustments to the attention mechanism</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">l_k</span> <span class="o">@</span> <span class="n">k</span>  <span class="c1"># Modify key matrix with l_k vector</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">l_v</span> <span class="o">@</span> <span class="n">v</span>  <span class="c1"># Modify value matrix with l_v vector</span>

    <span class="c1"># Calculate the attention scores and apply them to the value matrix</span>
    <span class="c1"># Softmax is applied to the dot product of q and the transpose of k for normalization</span>
    <span class="c1"># The resulting attention scores are then multiplied with the value matrix v</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>

<span class="k">def</span> <span class="nf">transformer_block_ia3</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Store the original input for the residual connection</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

    <span class="c1"># Apply the IA³-augmented self-attention to the input</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self_attention_ia3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Add the residual (original input) to the output of the self-attention layer</span>
    <span class="c1"># and apply layer normalization</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span>

    <span class="c1"># Store the output of the first sub-layer for the next residual connection</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

    <span class="c1"># Apply the first part of the Feed-Forward Network (FFN)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_1</span>  <span class="c1"># Normal transformer feed-forward operation</span>

    <span class="c1"># IA³ augmentation for the Feed-Forward Network</span>
    <span class="c1"># Apply an element-wise multiplication with the l_ff vector after applying the GELU activation function</span>
    <span class="c1"># This step introduces adaptability specific to the IA³ approach</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">l_ff</span> <span class="o">*</span> <span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Modify FFN output with l_ff vector</span>

    <span class="c1"># Complete the FFN operation</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_2</span>

    <span class="c1"># Add the residual (output of the first sub-layer) to the output of the FFN</span>
    <span class="c1"># and apply layer normalization</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span>

    <span class="c1"># Return the output of the transformer block with IA³ modifications</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>In this code snippet, the <code class="docutils literal notranslate"><span class="pre">self_attention_ia3</span></code> function and the <code class="docutils literal notranslate"><span class="pre">transformer_block_ia3</span></code> function represent the IA³ method as proposed by Liu et al. The IA³ method augments the standard Transformer architecture by introducing additional column vectors to modify the key and value matrices in the self-attention mechanism, as well as applying modifications to the feed-forward network. This approach allows for targeted adjustments to the model’s attention and feed-forward mechanisms, enhancing its adaptability for specific tasks without adding fully connected layers typical of traditional adapter methods.</p>
</section>
<section id="soft-prompts">
<h3>Soft-Prompts<a class="headerlink" href="#soft-prompts" title="Link to this heading">#</a></h3>
<p><strong>Prompt-Tuning</strong>: Developed by Lester et al., this technique involves creating a set of parameters for prompt tokens and integrating them at the beginning of the network. It allows for optimization of a continuous representation of the prompt text.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prompt_tuning</span><span class="p">(</span><span class="n">seq_tokens</span><span class="p">,</span> <span class="n">prompt_tokens</span><span class="p">):</span>
    <span class="c1"># Create embeddings for the input sequence tokens</span>
    <span class="c1"># seq_embedding is a function that transforms sequence tokens into their corresponding embeddings</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">seq_embedding</span><span class="p">(</span><span class="n">seq_tokens</span><span class="p">)</span>

    <span class="c1"># Create soft prompt embeddings</span>
    <span class="c1"># prompt_embedding is a function that creates embeddings for a set of trainable prompt tokens</span>
    <span class="c1"># These prompt tokens are not part of the original input but are learned parameters that are optimized during training</span>
    <span class="n">soft_prompt</span> <span class="o">=</span> <span class="n">prompt_embedding</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">)</span>

    <span class="c1"># Concatenate the soft prompt embeddings with the input sequence embeddings</span>
    <span class="c1"># This operation combines the prompt information with the actual input, allowing the model to consider the prompt context</span>
    <span class="c1"># &#39;dim=seq&#39; specifies the dimension along which the concatenation occurs, typically the sequence length dimension</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">concat</span><span class="p">([</span><span class="n">soft_prompt</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="n">seq</span><span class="p">)</span>

    <span class="c1"># Pass the concatenated input through the model</span>
    <span class="c1"># The model processes the combined prompt and sequence input, utilizing the prompt context for better adaptation to the task</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
</pre></div>
</div>
<p>In this code snippet, <code class="docutils literal notranslate"><span class="pre">prompt_tuning</span></code> function represents the Prompt-Tuning technique as developed by Lester et al. This approach involves integrating a set of learned prompt tokens at the beginning of the input sequence. These prompt tokens, represented as “soft prompts,” are not fixed but are trainable parameters that are optimized during the training process. By concatenating these soft prompt embeddings with the input sequence embeddings, the model is provided with additional context or guidance, enhancing its ability to adapt to specific tasks or datasets. This method allows for the fine-tuning of large language models in a parameter-efficient manner, as only the prompt embeddings are trained, leaving the rest of the model’s parameters frozen.</p>
<p><strong>Prefix Tuning</strong>: Similar to prompt tuning but differs in that the representation is fed to all layers of the transformer. It also involves learning additional parameters for the soft prompt in the form of a fully connected network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transformer_block_prefix_tuning</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">soft_prompt</span><span class="p">):</span>
    <span class="c1"># Apply a Feed-Forward Network (FFN) to the soft prompt</span>
    <span class="c1"># The FFN is used to transform the soft prompt embeddings, allowing for more complex and adaptable representations</span>
    <span class="c1"># This FFN is part of the learnable parameters and is specific to the prefix tuning method</span>
    <span class="n">soft_prompt</span> <span class="o">=</span> <span class="n">FFN</span><span class="p">(</span><span class="n">soft_prompt</span><span class="p">)</span>

    <span class="c1"># Concatenate the transformed soft prompt embeddings with the input sequence embeddings</span>
    <span class="c1"># Unlike prompt tuning where the prompt is only added at the beginning, in prefix tuning</span>
    <span class="c1"># the transformed soft prompt is designed to be integrated with every layer of the transformer</span>
    <span class="c1"># &#39;dim=seq&#39; specifies the dimension along which the concatenation occurs, typically the sequence length dimension</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">concat</span><span class="p">([</span><span class="n">soft_prompt</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="n">seq</span><span class="p">)</span>

    <span class="c1"># Pass the concatenated input (including the soft prompt) through the model</span>
    <span class="c1"># The transformer model processes the input sequence along with the embedded prompts,</span>
    <span class="c1"># allowing each layer of the transformer to utilize the prompt information</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
</pre></div>
</div>
<p>In this code snippet, the <code class="docutils literal notranslate"><span class="pre">transformer_block_prefix_tuning</span></code> function represents the Prefix Tuning technique. Prefix Tuning is similar to Prompt Tuning in that it involves adding soft prompts to the input. However, it differs significantly in its approach to integrating these prompts. In Prefix Tuning, the soft prompts are first transformed by a Feed-Forward Network (FFN), and this transformed representation is then concatenated with the input sequence embeddings. This concatenated input is fed to every layer of the transformer model, as opposed to just the beginning of the model in standard Prompt Tuning. This method allows for a more extensive and integrated use of the prompt information throughout the model, potentially leading to more nuanced and effective adaptations to specific tasks or datasets.</p>
<p><strong>P-Tuning</strong>: Proposed by Liu et al., P-Tuning encodes the prompt using an LSTM, aiming to address the discrete nature of word embeddings and their independent associations in other prompting methods.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">p_tuning</span><span class="p">(</span><span class="n">seq_tokens</span><span class="p">,</span> <span class="n">prompt_tokens</span><span class="p">):</span>
    <span class="c1"># Create embeddings for the prompt tokens</span>
    <span class="c1"># prompt_embedding is a function that transforms prompt tokens into their corresponding embeddings</span>
    <span class="c1"># These prompt tokens are learnable parameters that are optimized during training</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">prompt_embedding</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">)</span>

    <span class="c1"># Process the prompt embeddings with a bidirectional LSTM</span>
    <span class="c1"># The LSTM (Long Short-Term Memory) network is used to capture sequential information in the prompts</span>
    <span class="c1"># and can model dependencies in both forward and reverse directions (bidirectional)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">LSMT</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Apply a Feed-Forward Network (FFN) to the output of the LSTM</span>
    <span class="c1"># This step further transforms the prompt embeddings, allowing for richer representations</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">FFN</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

    <span class="c1"># Create embeddings for the input sequence tokens</span>
    <span class="c1"># seq_embedding is a function that transforms sequence tokens into their corresponding embeddings</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">seq_embedding</span><span class="p">(</span><span class="n">seq_tokens</span><span class="p">)</span>

    <span class="c1"># Concatenate the transformed prompt embeddings (h) with the input sequence embeddings (x)</span>
    <span class="c1"># This operation combines the prompt information with the actual input sequence</span>
    <span class="c1"># &#39;dim=seq&#39; specifies the dimension along which the concatenation occurs, typically the sequence length dimension</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">concat</span><span class="p">([</span><span class="n">h</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="n">seq</span><span class="p">)</span>

    <span class="c1"># Pass the concatenated input (including the prompt embeddings) through the model</span>
    <span class="c1"># The transformer model processes the input sequence along with the embedded prompts</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
</pre></div>
</div>
<p>In this code snippet, the <code class="docutils literal notranslate"><span class="pre">p_tuning</span></code> function represents the P-Tuning technique as proposed by Liu et al. P-Tuning aims to address the limitations of other prompting methods by using an LSTM network to encode the prompts. This approach helps to capture the sequential nature and dependencies within the prompt embeddings more effectively. The prompt embeddings are first transformed by the LSTM and then further processed by a Feed-Forward Network (FFN) to enrich their representation. These transformed prompt embeddings are then concatenated with the input sequence embeddings, and the combined input is fed into the model. This method allows for a more nuanced integration of the prompt information into the model, potentially leading to more effective adaptations for specific tasks.</p>
<p><strong>LLaMA-Adapter</strong>: As per Zhang et al., this technique applies a variant of prefix learning to the Llama model. It introduces adaptation prompts and zero-initialized attention for efficient fine-tuning.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transformer_block_llama_adapter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">soft_prompt</span><span class="p">,</span> <span class="n">gating_factor</span><span class="p">):</span>
    <span class="c1"># Store the original input for the residual connection</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

    <span class="c1"># Create an adaptation prompt by concatenating the soft prompt with the input</span>
    <span class="c1"># This adaptation prompt is a modification specific to the LLaMA-Adapter approach</span>
    <span class="n">adaption_prompt</span> <span class="o">=</span> <span class="n">concat</span><span class="p">([</span><span class="n">soft_prompt</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="n">seq</span><span class="p">)</span>

    <span class="c1"># Apply self-attention to the adaptation prompt</span>
    <span class="c1"># The self-attention mechanism processes the combined input of soft prompts and sequence tokens</span>
    <span class="n">adaption_prompt</span> <span class="o">=</span> <span class="n">self_attention</span><span class="p">(</span><span class="n">adaption_prompt</span><span class="p">)</span>

    <span class="c1"># Apply gating to the adaptation prompt using a zero-initialized attention mechanism</span>
    <span class="c1"># This gating factor controls the influence of the adaptation prompt on the transformer block</span>
    <span class="c1"># The zero-init attention helps in starting the training from a state where the adaptation prompt has minimal impact,</span>
    <span class="c1"># gradually learning its influence during training</span>
    <span class="n">adaption_prompt</span> <span class="o">=</span> <span class="n">adaption_prompt</span> <span class="o">*</span> <span class="n">gating_factor</span>

    <span class="c1"># Apply self-attention to the original input sequence</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Combine the output of the self-attention with the adapted prompt</span>
    <span class="c1"># The element-wise multiplication integrates the adaptation prompt into the main data flow</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">adaption_prompt</span> <span class="o">*</span> <span class="n">x</span>

    <span class="c1"># Add the residual (original input) to the output of the combined self-attention and adaptation prompt</span>
    <span class="c1"># and apply layer normalization</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span>

    <span class="c1"># Store the output of the first sub-layer for the next residual connection</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

    <span class="c1"># Apply the Feed-Forward Network (FFN) to the output of the first sub-layer</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">FFN</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Add the residual (output of the first sub-layer) to the output of the FFN</span>
    <span class="c1"># and apply layer normalization</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span>

    <span class="c1"># Return the output of the transformer block with the LLaMA-Adapter modifications</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>In this code snippet, the <code class="docutils literal notranslate"><span class="pre">transformer_block_llama_adapter</span></code> function represents the implementation of the LLaMA-Adapter technique. This method applies a variant of prefix learning to the transformer model, incorporating adaptation prompts and a zero-initialized attention mechanism. The adaptation prompt, created by combining soft prompts with the input sequence, is processed through self-attention and then modulated by a gating factor. This approach allows for efficient fine-tuning by introducing an adaptable mechanism that initially has minimal impact but learns to influence the transformer block’s processing over the course of training. The LLaMA-Adapter thus provides a novel way to fine-tune transformer models, enhancing their adaptability to specific tasks while maintaining the underlying model structure.</p>
</section>
<section id="reparameterization-based-methods">
<h3>Reparameterization-Based Methods<a class="headerlink" href="#reparameterization-based-methods" title="Link to this heading">#</a></h3>
<p><strong>LoRa</strong>: A popular technique by Hu et al., LoRa reparameterizes a weight matrix by learning a separate matrix representing updates from optimization. It uses two smaller matrices to represent these updates, reducing the number of parameters to be learned.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lora_linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="c1"># Scale factor based on the rank r</span>
    <span class="c1"># In LoRa, a low-rank approximation is used to reduce the number of learnable parameters</span>
    <span class="c1"># r is the rank, which is a hyperparameter determining the size of the low-rank matrices W_a and W_b</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">r</span>

    <span class="c1"># Standard linear transformation using the original weight matrix W</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W</span>

    <span class="c1"># LoRa modification: Apply low-rank linear transformation</span>
    <span class="c1"># W_a and W_b are smaller matrices representing updates from optimization</span>
    <span class="c1"># The product of W_a and W_b approximates the updates to the original weight matrix W</span>
    <span class="c1"># This approach reduces the number of parameters to learn, focusing on the most impactful parts of W</span>
    <span class="n">h</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_a</span> <span class="o">@</span> <span class="n">W_b</span>

    <span class="c1"># Scale the result of the transformation</span>
    <span class="c1"># The scaling helps in balancing the influence of the low-rank approximation</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">h</span>

<span class="k">def</span> <span class="nf">self_attention_lora</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Apply the LoRa-modified linear transformation to the key and value matrices</span>
    <span class="c1"># LoRa reparameterizes the original weight matrices (W_k and W_v) of the key and value vectors</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">lora_linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_k</span><span class="p">)</span>  <span class="c1"># LoRa applied to key matrix</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_q</span>              <span class="c1"># Standard linear transformation for query matrix</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">lora_linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_v</span><span class="p">)</span>  <span class="c1"># LoRa applied to value matrix</span>

    <span class="c1"># Calculate the attention scores and apply them to the value matrix</span>
    <span class="c1"># The attention mechanism remains the same as in the standard self-attention</span>
    <span class="c1"># The key and value matrices are modified by LoRa, allowing for efficient learning of the most impactful parameters</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>
</pre></div>
</div>
<p>In this code snippet, the <code class="docutils literal notranslate"><span class="pre">lora_linear</span></code> function implements the Low-Rank Adaptation (LoRa) technique for linear layers, and the <code class="docutils literal notranslate"><span class="pre">self_attention_lora</span></code> function integrates this technique into the self-attention mechanism of a transformer. LoRa focuses on reparameterizing the weight matrices of the key and value vectors using low-rank matrices. By learning updates in the form of smaller matrices (W_a and W_b), LoRa efficiently captures the most significant changes to the weights while reducing the overall number of parameters that need to be learned. This approach maintains the core functionality of the self-attention mechanism while allowing for more efficient and focused training, particularly beneficial for adapting large pre-trained models to new tasks or datasets.</p>
</section>
<section id="selective-methods">
<h3>Selective Methods<a class="headerlink" href="#selective-methods" title="Link to this heading">#</a></h3>
<p><strong>AdaLoRa</strong>: This hybrid approach, developed by Zhang et al., combines ideas from reparameterization and selective methods. It uses an approximation of Singular Value Decomposition (SVD) to represent weight matrix updates and incorporates a pruning technique to eliminate less important singular vectors.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">adalora_linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">curr_sv</span><span class="p">):</span>
    <span class="c1"># Scale factor based on the rank r</span>
    <span class="c1"># In AdaLoRa, a low-rank approximation with a scaling factor is used</span>
    <span class="c1"># r is the rank and alpha is a scaling hyperparameter</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">r</span>

    <span class="c1"># Standard linear transformation using the original weight matrix W</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W</span>

    <span class="c1"># AdaLoRa modification: Apply low-rank linear transformation using SVD components</span>
    <span class="c1"># p, lambda, and q are matrices derived from the singular value decomposition (SVD) of W</span>
    <span class="c1"># curr_sv represents the current singular vectors being optimized</span>
    <span class="c1"># This approach selectively updates the weight matrix W focusing on its most significant singular vectors</span>
    <span class="n">h</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">p</span><span class="p">[</span><span class="n">curr_sv</span><span class="p">]</span> <span class="o">@</span> <span class="n">lamda</span><span class="p">[</span><span class="n">curr_sv</span><span class="p">]</span> <span class="o">@</span> <span class="n">q</span><span class="p">[</span><span class="n">curr_sv</span><span class="p">]</span>

    <span class="c1"># Scale the result of the transformation</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">h</span>

<span class="k">def</span> <span class="nf">self_attention_adalora</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AdaLoRa-specific self-attention mechanism.</span>
<span class="sd">    This function shows how AdaLoRa is integrated into the self-attention block.</span>
<span class="sd">    It does not include the pruning techniques used in AdaLoRa.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Apply the AdaLoRa-modified linear transformation to the key and value matrices</span>
    <span class="c1"># AdaLoRa reparameterizes the original weight matrices (W_k and W_v) of the key and value vectors</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">adalora_linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_k</span><span class="p">)</span>  <span class="c1"># AdaLoRa applied to key matrix</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_q</span>                 <span class="c1"># Standard linear transformation for query matrix</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">adalora_linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_v</span><span class="p">)</span>  <span class="c1"># AdaLoRa applied to value matrix</span>

    <span class="c1"># Calculate the attention scores and apply them to the value matrix</span>
    <span class="c1"># The attention mechanism remains the same as in standard self-attention</span>
    <span class="c1"># The key and value matrices are modified by AdaLoRa, focusing on efficient learning of significant parameters</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>
</pre></div>
</div>
<p>In this code snippet, the <code class="docutils literal notranslate"><span class="pre">adalora_linear</span></code> function implements the AdaLoRa technique for linear layers, and the <code class="docutils literal notranslate"><span class="pre">self_attention_adalora</span></code> function integrates AdaLoRa into the self-attention mechanism of a transformer. AdaLoRa combines ideas from reparameterization with selective methods, using an approximation of Singular Value Decomposition (SVD) to represent weight matrix updates. This method focuses on optimizing the most significant components of the weight matrices, identified through SVD, and incorporates a scaling factor for effective learning. By selectively updating these key components, AdaLoRa provides an efficient way to fine-tune large pre-trained models, targeting the adjustments that have the most substantial impact on model performance. This approach is particularly beneficial for resource-efficient training and adapting models to new tasks while maintaining their underlying structure.</p>
</section>
<section id="comparison-of-methods">
<h3>Comparison of Methods<a class="headerlink" href="#comparison-of-methods" title="Link to this heading">#</a></h3>
<p>The table below provides a comparison of these methods, highlighting the number of trainable parameters, method type, and a brief summary.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Trainable Parameters</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Summary</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Adapters</p></td>
<td><p>Low</p></td>
<td><p>Additive</p></td>
<td><p>Adds small networks after sub-layers.</p></td>
</tr>
<tr class="row-odd"><td><p>(IA)³</p></td>
<td><p>Low</p></td>
<td><p>Additive</p></td>
<td><p>Augments attention mechanism with additional vectors.</p></td>
</tr>
<tr class="row-even"><td><p>Prompt-Tuning</p></td>
<td><p>Very Low</p></td>
<td><p>Soft-Prompts</p></td>
<td><p>Optimizes a continuous representation of the prompt text.</p></td>
</tr>
<tr class="row-odd"><td><p>Prefix Tuning</p></td>
<td><p>Low</p></td>
<td><p>Soft-Prompts</p></td>
<td><p>Feeds prompt representation to all transformer layers.</p></td>
</tr>
<tr class="row-even"><td><p>P-Tuning</p></td>
<td><p>Low</p></td>
<td><p>Soft-Prompts</p></td>
<td><p>Encodes prompts using an LSTM.</p></td>
</tr>
<tr class="row-odd"><td><p>LLaMA-Adapter</p></td>
<td><p>Low</p></td>
<td><p>Soft-Prompts</p></td>
<td><p>Applies efficient prefix learning to Llama model.</p></td>
</tr>
<tr class="row-even"><td><p>LoRa</p></td>
<td><p>Very Low</p></td>
<td><p>Reparameterization</p></td>
<td><p>Reparameterizes weight matrix with two smaller matrices.</p></td>
</tr>
<tr class="row-odd"><td><p>AdaLoRa</p></td>
<td><p>Low</p></td>
<td><p>Selective/Hybrid</p></td>
<td><p>Uses SVD approximation for weight matrix updates.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Parameter-Efficient Fine-Tuning (PEFT) stands as a transformative approach in the realm of working with Large Language Models (LLMs), addressing the critical challenges of computational demand and resource constraints. By focusing on adapting smaller segments of these expansive models, PEFT offers a path to harness the power of LLMs in a more accessible and sustainable manner. This approach not only broadens the scope of who can utilize these models but also expands the potential applications in various fields.</p>
<p>The integration of PEFT techniques, particularly those developed and made accessible by platforms like Hugging Face, marks a significant advancement in the field of natural language processing and artificial intelligence. These techniques allow for the efficient customization of LLMs for specific tasks, providing a balance between performance and resource utilization. The diverse range of PEFT methods, including LoRA, Prefix Tuning, and AdaLoRA, each offers unique advantages, catering to different requirements and scenarios.</p>
<p>The potential of PEFT extends beyond just language models; its principles are being applied to other areas such as diffusion models, indicating a broad spectrum of impact. Whether it’s fine-tuning for specific tasks, adapting models for resource-limited settings, or pushing the boundaries of what’s possible with consumer-grade hardware, PEFT is at the forefront of this evolution.</p>
<p>In conclusion, PEFT represents a significant step forward in the democratization of AI and ML technologies. It opens up new avenues for innovation, allowing a wider range of researchers, developers, and organizations to leverage the capabilities of large-scale models in a more efficient and effective manner. As the field continues to evolve, the role of PEFT in shaping the future of machine learning and AI will undoubtedly be pivotal, driving forward the possibilities of what can be achieved with these powerful tools.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/llms/peft"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Parameter-Efficient Fine-Tuning (PEFT)</p>
      </div>
    </a>
    <a class="right-next"
       href="peft-hf.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">PEFT in HuggingFace Libraries</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-fine-tuning-vs-peft">Standard Fine-Tuning vs. PEFT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-security-and-privacy-concerns">Data Security and Privacy Concerns</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-of-data-quality">Importance of Data Quality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparative-analysis">Comparative Analysis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#who-uses-peft">Who Uses PEFT?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#current-adoption-of-peft">Current Adoption of PEFT</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face">Hugging Face 🤗</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#googles-vertex-ai">Google’s Vertex AI</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#openai">OpenAI</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-transformer-review">Quick Transformer Review</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts-of-transformer-architecture">Core Concepts of Transformer Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-mechanism">Self-Attention Mechanism</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-block-structure">Transformer Block Structure</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-in-peft">Importance in PEFT</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#peft-techniques-overview">PEFT Techniques Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additive-methods">Additive Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#soft-prompts">Soft-Prompts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reparameterization-based-methods">Reparameterization-Based Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selective-methods">Selective Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-methods">Comparison of Methods</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://entelecheia.me" target="_blank">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>