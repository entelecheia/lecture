
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>PEFT in HuggingFace Libraries &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/dataframe.css?v=574a5d82" />
    <link rel="stylesheet" type="text/css" href="../../../_static/slide.css?v=06ccce15" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../../_static/tabs.js?v=3ee01567"></script>
    <script src="../../../_static/js/hoverxref.js"></script>
    <script src="../../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/llms/peft/peft-hf';</script>
    <link rel="canonical" href="https://lecture.jeju.ai/lectures/llms/peft/peft-hf.html" />
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Q-Learning" href="../q-learning/index.html" />
    <link rel="prev" title="PEFT for LLMs" href="peft-llms.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Courses</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_intro/index.html">Introduction to NLP</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/intro/index.html">Introduction</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/apps/index.html">NLP Applications</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/apps/research1.html">Research Part I</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/apps/research2.html">Research Part II</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/lm/index.html">Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/lm/ngram.html">N-gram Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/lm/usage.html">Usage of Language Models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/datasets/index.html">Datasets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/datasets/corpus.html">Text Data Collection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/datasets/lab-dart.html">Lab: Crawling DART Data</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/topic/index.html">Topic Modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/methods.html">Topic Modeling Methodologies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/lab-methods.html">Lab: Topic Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/lab-coherence.html">Lab: Topic Coherence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/tomotopy.html">Lab: Tomotopy</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/sentiments/index.html">Sentiment Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lexicon.html">Lexicon-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/ml.html">Machine Learning-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lab-lexicon.html">Lab: Lexicon-based Sentiment Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lab-ml.html">Lab: ML-based Sentiment Classification</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/tokenization/index.html">Tokenization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/tokenization.html">Understanding the Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/pos.html">Part-of-Speech Tagging and Parsing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/ngrams.html">N-grams for Tokenization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/korean.html">Tokenization in Korean</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/lab-tokenization.html">Lab: Tokenization and Pre-processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/lab-korean.html">Lab: Korean Text Processing</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/vectorization/index.html">Vector Representation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/semantics.html">Vector Semantics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/bow.html">Bags of Words Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/tf-idf.html">TF-IDF Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/similarity.html">Word Similarity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/lab-similarity.html">Lab: Word Similarity</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/embeddings/index.html">Word Embeddings</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/nlm.html">Neural Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/w2v.html">Word2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/glove.html">GloVe</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/fasttext.html">FastText</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_deep/index.html">Deep Learning for NLP</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/llms/index.html">Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/zeroshot.html">Zero Shot and Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/decoding.html">Decoding and Search Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/plms.html">Pretrained Language Models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/transformers/index.html">Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/bert.html">BERT: Bidirectional Encoder Representations from Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/bertviz.html">BERT: Visualizing Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/datasets/index.html">Datasets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/datasets/mc4.html">mC4 Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/datasets/lab-eda.html">Lab: Exploratory Data Analysis (EDA)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/tokenization/index.html">Tokenization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/subword.html">Subword Tokenization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/pipeline.html">Tokenization Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/bpe.html">BPE Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/wordpiece.html">WordPiece Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/unigram.html">Unigram Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/lab-train-tokenizers.html">Lab: Training Tokenizers</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/training/index.html">Training Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-pretrain-mlm.html">Lab: Pretraining LMs - MLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-pretrain-clm.html">Lab: Pretraining LMs - CLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-finetune-mlm.html">Lab: Finetuining a MLM</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/chatbots/index.html">Conversational AI and Chatbots</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/chatbots/rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/chatbots/detectGPT.html">How to Spot Machine-Written Texts</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_advances/index.html">Advances in AI and NLP</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/thesis.html">Writing a Thesis</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_advances/gpt/index.html">Generative Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/detectGPT.html">DetectGPT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/gpt4.html">GPT-4</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/camelids.html">Meet the Camelids: A Family of LLMs</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/sam/index.html">Segment Anything</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../aiart/index.html">AI Art (Generative AI)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/intro/index.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/brave/index.html">A Brave New World</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/aiart/index.html">Art and Music in Light of AI</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../aiart/text-to-image/index.html">Text-to-Image Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/imagen.html">Imagen</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/motion-capture-and-synthesis/index.html">Motion Capture and Motion Synthesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/robot/index.html">Robot Drawing System</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mlops/index.html">Machine Learning Systems Design</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/devops/index.html">DevOps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/gitops.html">GitOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/devsecops.html">DevSecOps</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/dotfiles/index.html">Dotfiles</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai">Dotfiles Project</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dotdrop/">Dotdrop Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/github/">GitHub Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/ssh-gpg-age/">SSH, GPG, and Age Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/sops/">SOPS Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/pass/">Pass and Passage Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/doppler/">Doppler Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dockerfiles/">Dockerfiles Usage</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/security/index.html">Security Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/auth-enc-sign.html">Authentication, Encryption, and Signing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/age-gpg-ssh.html">SSH, GPG, and AGE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/pass.html">Unix Password Managers</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/github/index.html">GitHub Workflow</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/fork-pull.html">Github’s Fork &amp; Pull Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/template.html">Project Templating Tools</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-python.entelecheia.ai/">Hyperfast Python Template</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-template.entelecheia.ai/">Hyperfast Template</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/containerization/index.html">Containerization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/docker.html">Docker</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/containerd.html"><code class="docutils literal notranslate"><span class="pre">containerd</span></code></a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/simple-pipeline/index.html">Simple MLOps Pipeline</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/simple-pipeline/server.html">Server Setup &amp; Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/simple-pipeline/vpn.html">VPN Connectivity</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/llmops/index.html">LLMOps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/llmops/bentoml.html">Introduction to BentoML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/llmops/bentochain.html">Deploy a Voice-Based Chatbot with BentoML, LangChain, and Gradio</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsecon/index.html">Data Science for Economics and Finance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/intro/index.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/intro/introduction.html">Data Science in Economics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/intro/challenges.html">Technical Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/intro/methods.html">Data Analytics Methods</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/cb/index.html">Central Banks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/cb/altdata.html">Alternative Data Sources for Central Banks</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/fomc/index.html">Textual Analysis of FOMC contents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/01_numerical_data.html">Preparing Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/02_textual_data.html">Preparing Textual Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/03_EDA_numericals1.html">EDA on Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/03_EDA_numericals2.html">EDA on Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/04_training_datasets.html">Create Training Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/05_features.html">Visualizing Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/06_AutoML.html">Checking Baseline with AutoML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/07_predict_sentiments.html">Predicting Sentiments of FOMC Corpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/08_EDA_sentiments1.html">EDA on Sentiments: Correlation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/08_EDA_sentiments2.html">EDA on Sentiment Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/09_visualize_features.html">Visualize Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/10_monetary_shocks.html">Monetary Policy Shocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/11_AutoML_with_tones.html">Predicting the next decisions with tones</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/esg-ratings/index.html">ESG Ratings</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/prepare_datasets.html">Preparing training datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/improve_datasets.html">Improving classification datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/train_classifiers.html">Training Classifiers for ESG Ratings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/build_news_corpus.html">Building <code class="docutils literal notranslate"><span class="pre">econ_news_kr</span></code> corpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/predict_esg_classes.html">Predicting ESG Categories and Polarities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/cross_validate_datasets.html">Cross validating datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/prepare_datasets_for_labeling.html">Preparing active learning data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/all_in_one_pipeline.html">Putting them together in a pipeline</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../softeng/index.html">Software Engineering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/intro/index.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/introduction.html">Software Engineering?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/processes.html">Software Processes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/sdlc.html">Software Development Life Cycle (SDLC)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/requirements.html">Requirements Engineering (RE)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/proposal/index.html">Project Proposal</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/proposal/steps.html">Steps in Software Engineering Projects</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/proposal/guidelines.html">Software Engineering Proposal Guideline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/proposal/template.html">Project Proposal Template</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/vcs/index.html">Version Control Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/00_introduction.html">0. Introduction to version control</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/01_solo_work_with_git.html">1. Solo work with git</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/02_fixing_mistakes.html">2. Fixing mistakes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/03_publishing.html">3. Publishing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/04_collaboration.html">4. Collaboration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/05_fork_and_pull.html">5. Fork and Pull</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/06_git_theory.html">6. Git Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/07_branches.html">7. Branches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/08_advanced_git_concepts.html">8. Advanced git concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/09_github_pages.html">9. Publishing from GitHub</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/10_rebasing.html">10. Rebasing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/11_debugging_with_git_bisect.html">11. Debugging With git bisect</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/12_multiple_remotes.html">12. Working with multiple remotes</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/spm/index.html">Software Process Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/spm/agile.html">Agile Software Development</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/devops/index.html">DevOps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/devops/gitops.html">GitOps</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Large Language Models</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../intro/index.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../intro/llms.html">Large Language Models?</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../stack/index.html">LLM Stacks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../stack/infra.html">Generative AI Infrastructure Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../stack/architecture.html">LLM Application Architectures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../stack/app.html">LLM App Ecosystem</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../agents/index.html">AI Agents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../agents/autogen.html">AutoGen</a></li>
<li class="toctree-l3"><a class="reference internal" href="../agents/autoscraper.html">AutoGen AutoScraper Agent</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../finetune/index.html">LLM Fine-tuning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetune/autotrain.html">Fine-Tuning LLMs with Hugging Face AutoTrain</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/index.html">Retrieval Augmented Generation (RAG)</a></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="index.html">Parameter-Efficient Fine-Tuning (PEFT)</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="peft-llms.html">PEFT for LLMs</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">PEFT in HuggingFace Libraries</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../q-learning/index.html">Q-Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../q-learning/qstar.html">Q-Star (Q*)</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../about/lecture-bot.html">LectureBot for ἐντελέχεια.άι</a></li>
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference external" href="https://course.entelecheia.ai">course.entelecheia.ai</a></li>
<li class="toctree-l1"><a class="reference external" href="https://research.jeju.ai">research.jeju.ai</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about/index.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/lectures/llms/peft/peft-hf.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>PEFT in HuggingFace Libraries</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-peft-methods">Supported PEFT Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started-with-peft">Getting Started with PEFT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-cases">Use Cases</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#get-comparable-performance-to-full-finetuning-by-adapting-llms-to-downstream-tasks-using-consumer-hardware">Get comparable performance to full finetuning by adapting LLMs to downstream tasks using consumer hardware</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-tuning-of-diffusion-models">Parameter Efficient Tuning of Diffusion Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-tuning-of-llms-for-rlhf-components-such-as-ranker-and-policy">Parameter Efficient Tuning of LLMs for RLHF components such as Ranker and Policy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#int8-training-of-large-models-in-colab-using-peft-lora-and-bits-and-bytes">INT8 training of large models in Colab using PEFT LoRA and bits_and_bytes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#save-compute-and-storage-even-for-medium-and-small-models">Save compute and storage even for medium and small models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#peft-accelerate">PEFT + 🤗 Accelerate</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-peft-model-training-using-accelerates-deepspeed-integration">Example of PEFT model training using 🤗 Accelerate’s DeepSpeed integration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-peft-model-inference-using-accelerates-big-model-inferencing-capabilities">Example of PEFT model inference using 🤗 Accelerate’s Big Model Inferencing capabilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="peft-in-huggingface-libraries">
<h1><a class="reference external" href="https://github.com/huggingface/peft">PEFT in HuggingFace Libraries</a><a class="headerlink" href="#peft-in-huggingface-libraries" title="Link to this heading">#</a></h1>
<p>PEFT methods are seamlessly integrated with 🤗 Accelerate, supporting large-scale models through DeepSpeed and Big Model Inference.</p>
<section id="supported-peft-methods">
<h2>Supported PEFT Methods<a class="headerlink" href="#supported-peft-methods" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>LoRA</strong>: Low-Rank Adaptation for efficient model adaptation. <a class="reference external" href="https://arxiv.org/abs/2106.09685">Paper</a></p></li>
<li><p><strong>Prefix Tuning</strong>: Optimizes continuous prompts for generation tasks. <a class="reference external" href="https://aclanthology.org/2021.acl-long.353/">Paper1</a>, <a class="reference external" href="https://arxiv.org/pdf/2110.07602.pdf">Paper2</a></p></li>
<li><p><strong>P-Tuning</strong>: Utilizes prompts for understanding model behavior. <a class="reference external" href="https://arxiv.org/abs/2103.10385">Paper</a></p></li>
<li><p><strong>Prompt Tuning</strong>: Harnesses the power of scale for parameter-efficient prompt tuning. <a class="reference external" href="https://arxiv.org/abs/2104.08691">Paper</a></p></li>
<li><p><strong>AdaLoRA</strong>: Adaptive Budget Allocation for fine-tuning. <a class="reference external" href="https://arxiv.org/abs/2303.10512">Paper</a></p></li>
<li><p><strong><span class="math notranslate nohighlight">\((IA)^3\)</span></strong>: Enhances fine-tuning effectiveness and efficiency. <a class="reference external" href="https://arxiv.org/abs/2205.05638">Paper</a></p></li>
<li><p><strong>MultiTask Prompt Tuning</strong>: Enables transfer learning across multiple tasks. <a class="reference external" href="https://arxiv.org/abs/2303.02861">Paper</a></p></li>
<li><p><strong>LoHa</strong>: Combines Low-Rank and Hadamard Product for efficient learning. <a class="reference external" href="https://arxiv.org/abs/2108.06098">Paper</a></p></li>
<li><p><strong>LoKr</strong>: Incorporates Kronecker Adapter for tuning. <a class="reference external" href="https://arxiv.org/abs/2212.10650">Paper</a></p></li>
</ol>
</section>
<section id="getting-started-with-peft">
<h2>Getting Started with PEFT<a class="headerlink" href="#getting-started-with-peft" title="Link to this heading">#</a></h2>
<p>Here’s a Python snippet demonstrating how to use LoRA with a sequence-to-sequence model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span>
<span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">get_peft_config</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">TaskType</span>
<span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;bigscience/mt0-large&quot;</span>
<span class="n">tokenizer_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;bigscience/mt0-large&quot;</span>

<span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_2_SEQ_LM</span><span class="p">,</span> <span class="n">inference_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>
<span class="c1"># output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282</span>
</pre></div>
</div>
</section>
<section id="use-cases">
<h2>Use Cases<a class="headerlink" href="#use-cases" title="Link to this heading">#</a></h2>
<section id="get-comparable-performance-to-full-finetuning-by-adapting-llms-to-downstream-tasks-using-consumer-hardware">
<h3>Get comparable performance to full finetuning by adapting LLMs to downstream tasks using consumer hardware<a class="headerlink" href="#get-comparable-performance-to-full-finetuning-by-adapting-llms-to-downstream-tasks-using-consumer-hardware" title="Link to this heading">#</a></h3>
<p>GPU memory required for adapting LLMs on the few-shot dataset <a class="reference external" href="https://huggingface.co/datasets/ought/raft/viewer/twitter_complaints"><code class="docutils literal notranslate"><span class="pre">ought/raft/twitter_complaints</span></code></a>. Here, settings considered
are full finetuning, PEFT-LoRA using plain PyTorch and PEFT-LoRA using DeepSpeed with CPU Offloading.</p>
<p>Hardware: Single A100 80GB GPU with CPU RAM above 64GB</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Full Finetuning</p></th>
<th class="head"><p>PEFT-LoRA PyTorch</p></th>
<th class="head"><p>PEFT-LoRA DeepSpeed with CPU Offloading</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>bigscience/T0_3B (3B params)</p></td>
<td><p>47.14GB GPU / 2.96GB CPU</p></td>
<td><p>14.4GB GPU / 2.96GB CPU</p></td>
<td><p>9.8GB GPU / 17.8GB CPU</p></td>
</tr>
<tr class="row-odd"><td><p>bigscience/mt0-xxl (12B params)</p></td>
<td><p>OOM GPU</p></td>
<td><p>56GB GPU / 3GB CPU</p></td>
<td><p>22GB GPU / 52GB CPU</p></td>
</tr>
<tr class="row-even"><td><p>bigscience/bloomz-7b1 (7B params)</p></td>
<td><p>OOM GPU</p></td>
<td><p>32GB GPU / 3.8GB CPU</p></td>
<td><p>18.1GB GPU / 35GB CPU</p></td>
</tr>
</tbody>
</table>
</div>
<p>Performance of PEFT-LoRA tuned <a class="reference external" href="https://huggingface.co/bigscience/T0_3B"><code class="docutils literal notranslate"><span class="pre">bigscience/T0_3B</span></code></a> on <a class="reference external" href="https://huggingface.co/datasets/ought/raft/viewer/twitter_complaints"><code class="docutils literal notranslate"><span class="pre">ought/raft/twitter_complaints</span></code></a> leaderboard.
A point to note is that we didn’t try to squeeze performance by playing around with input instruction templates, LoRA hyperparams and other training related hyperparams. Also, we didn’t use the larger 13B <a class="reference external" href="https://huggingface.co/bigscience/mt0-xxl">mt0-xxl</a> model.
So, we are already seeing comparable performance to SoTA with parameter efficient tuning. Also, the final additional checkpoint size is just <code class="docutils literal notranslate"><span class="pre">19MB</span></code> in comparison to <code class="docutils literal notranslate"><span class="pre">11GB</span></code> size of the backbone <a class="reference external" href="https://huggingface.co/bigscience/T0_3B"><code class="docutils literal notranslate"><span class="pre">bigscience/T0_3B</span></code></a> model, but one still has to load the original full size model.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Submission Name</p></th>
<th class="head"><p>Accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Human baseline (crowdsourced)</p></td>
<td><p>0.897</p></td>
</tr>
<tr class="row-odd"><td><p>Flan-T5</p></td>
<td><p>0.892</p></td>
</tr>
<tr class="row-even"><td><p>lora-t0-3b</p></td>
<td><p>0.863</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Therefore, we can see that performance comparable to SoTA is achievable by PEFT methods with consumer hardware such as 16GB and 24GB GPUs.</strong></p>
<p>An insightful blogpost explaining the advantages of using PEFT for fine-tuning FlanT5-XXL: <a class="reference external" href="https://www.philschmid.de/fine-tune-flan-t5-peft">https://www.philschmid.de/fine-tune-flan-t5-peft</a></p>
</section>
</section>
<section id="parameter-efficient-tuning-of-diffusion-models">
<h2>Parameter Efficient Tuning of Diffusion Models<a class="headerlink" href="#parameter-efficient-tuning-of-diffusion-models" title="Link to this heading">#</a></h2>
<p>GPU memory required by different settings during training is given below. The final checkpoint size is <code class="docutils literal notranslate"><span class="pre">8.8</span> <span class="pre">MB</span></code>.</p>
<p>Hardware: Single A100 80GB GPU with CPU RAM above 64GB</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Full Finetuning</p></th>
<th class="head"><p>PEFT-LoRA</p></th>
<th class="head"><p>PEFT-LoRA with Gradient Checkpointing</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CompVis/stable-diffusion-v1-4</p></td>
<td><p>27.5GB GPU / 3.97GB CPU</p></td>
<td><p>15.5GB GPU / 3.84GB CPU</p></td>
<td><p>8.12GB GPU / 3.77GB CPU</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Training</strong>
An example of using LoRA for parameter efficient dreambooth training is given in <a class="reference internal" href="#examples/lora_dreambooth/train_dreambooth.py"><span class="xref myst"><code class="docutils literal notranslate"><span class="pre">examples/lora_dreambooth/train_dreambooth.py</span></code></span></a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MODEL_NAME</span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;CompVis/stable-diffusion-v1-4&quot;</span><span class="w"> </span><span class="c1">#&quot;stabilityai/stable-diffusion-2-1&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INSTANCE_DIR</span><span class="o">=</span><span class="s2">&quot;path-to-instance-images&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CLASS_DIR</span><span class="o">=</span><span class="s2">&quot;path-to-class-images&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OUTPUT_DIR</span><span class="o">=</span><span class="s2">&quot;path-to-save-model&quot;</span>

accelerate<span class="w"> </span>launch<span class="w"> </span>train_dreambooth.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pretrained_model_name_or_path<span class="o">=</span><span class="nv">$MODEL_NAME</span><span class="w">  </span><span class="se">\</span>
<span class="w">  </span>--instance_data_dir<span class="o">=</span><span class="nv">$INSTANCE_DIR</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--class_data_dir<span class="o">=</span><span class="nv">$CLASS_DIR</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--output_dir<span class="o">=</span><span class="nv">$OUTPUT_DIR</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--train_text_encoder<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--with_prior_preservation<span class="w"> </span>--prior_loss_weight<span class="o">=</span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--instance_prompt<span class="o">=</span><span class="s2">&quot;a photo of sks dog&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--class_prompt<span class="o">=</span><span class="s2">&quot;a photo of dog&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--resolution<span class="o">=</span><span class="m">512</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--train_batch_size<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--lr_scheduler<span class="o">=</span><span class="s2">&quot;constant&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--lr_warmup_steps<span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--num_class_images<span class="o">=</span><span class="m">200</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--use_lora<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--lora_r<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--lora_alpha<span class="w"> </span><span class="m">27</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--lora_text_encoder_r<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--lora_text_encoder_alpha<span class="w"> </span><span class="m">17</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--learning_rate<span class="o">=</span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--gradient_accumulation_steps<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--gradient_checkpointing<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--max_train_steps<span class="o">=</span><span class="m">800</span>
</pre></div>
</div>
<p>Try out the 🤗 Gradio Space which should run seamlessly on a T4 instance:
<a class="reference external" href="https://huggingface.co/spaces/smangrul/peft-lora-sd-dreambooth">smangrul/peft-lora-sd-dreambooth</a>.</p>
<p><img alt="peft lora dreambooth gradio space" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/peft_lora_dreambooth_gradio_space.png" /></p>
<p><strong>NEW</strong> ✨ Multi Adapter support and combining multiple LoRA adapters in a weighted combination
<img alt="peft lora dreambooth weighted adapter" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/weighted_adapter_dreambooth_lora.png" /></p>
<p><strong>NEW</strong> ✨ Dreambooth training for Stable Diffusion using LoHa and LoKr adapters <a class="reference internal" href="#examples/stable_diffusion/train_dreambooth.py"><span class="xref myst"><code class="docutils literal notranslate"><span class="pre">examples/stable_diffusion/train_dreambooth.py</span></code></span></a></p>
</section>
<section id="parameter-efficient-tuning-of-llms-for-rlhf-components-such-as-ranker-and-policy">
<h2>Parameter Efficient Tuning of LLMs for RLHF components such as Ranker and Policy<a class="headerlink" href="#parameter-efficient-tuning-of-llms-for-rlhf-components-such-as-ranker-and-policy" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Here is an example in <a class="reference external" href="https://github.com/lvwerra/trl">trl</a> library using PEFT+INT8 for tuning policy model: <a class="reference external" href="https://github.com/lvwerra/trl/blob/main/examples/sentiment/scripts/gpt2-sentiment_peft.py">gpt2-sentiment_peft.py</a> and corresponding <a class="reference external" href="https://huggingface.co/blog/trl-peft">Blog</a></p></li>
<li><p>Example using PEFT for Instruction finetuning, reward model and policy : <a class="reference external" href="https://github.com/lvwerra/trl/tree/main/examples/research_projects/stack_llama/scripts">stack_llama</a> and corresponding <a class="reference external" href="https://huggingface.co/blog/stackllama">Blog</a></p></li>
</ul>
</section>
<section id="int8-training-of-large-models-in-colab-using-peft-lora-and-bits-and-bytes">
<h2>INT8 training of large models in Colab using PEFT LoRA and bits_and_bytes<a class="headerlink" href="#int8-training-of-large-models-in-colab-using-peft-lora-and-bits-and-bytes" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Here is now a demo on how to fine tune <a class="reference external" href="https://huggingface.co/facebook/opt-6.7b">OPT-6.7b</a> (14GB in fp16) in a Google Colab: <a class="reference external" href="https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p></li>
<li><p>Here is now a demo on how to fine tune <a class="reference external" href="https://huggingface.co/openai/whisper-large-v2">whisper-large</a> (1.5B params) (14GB in fp16) in a Google Colab: <a class="reference external" href="https://colab.research.google.com/drive/1DOkD_5OUjFa0r5Ik3SgywJLJtEo2qLxO?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> and <a class="reference external" href="https://colab.research.google.com/drive/1vhF8yueFqha3Y3CpTHN6q9EVcII9EYzs?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p></li>
</ul>
<section id="save-compute-and-storage-even-for-medium-and-small-models">
<h3>Save compute and storage even for medium and small models<a class="headerlink" href="#save-compute-and-storage-even-for-medium-and-small-models" title="Link to this heading">#</a></h3>
<p>Save storage by avoiding full finetuning of models on each of the downstream tasks/datasets,
With PEFT methods, users only need to store tiny checkpoints in the order of <code class="docutils literal notranslate"><span class="pre">MBs</span></code> all the while retaining
performance comparable to full finetuning.</p>
<p>An example of using LoRA for the task of adapting <code class="docutils literal notranslate"><span class="pre">LayoutLMForTokenClassification</span></code> on <code class="docutils literal notranslate"><span class="pre">FUNSD</span></code> dataset is given in <code class="docutils literal notranslate"><span class="pre">~examples/token_classification/PEFT_LoRA_LayoutLMForTokenClassification_on_FUNSD.py</span></code>. We can observe that with only <code class="docutils literal notranslate"><span class="pre">0.62</span> <span class="pre">%</span></code> of parameters being trainable, we achieve performance (F1 0.777) comparable to full finetuning (F1 0.786) (without any hyperparam tuning runs for extracting more performance), and the checkpoint of this is only <code class="docutils literal notranslate"><span class="pre">2.8MB</span></code>. Now, if there are <code class="docutils literal notranslate"><span class="pre">N</span></code> such datasets, just have these PEFT models one for each dataset and save a lot of storage without having to worry about the problem of catastrophic forgetting or overfitting of backbone/base model.</p>
<p>Another example is fine-tuning <a class="reference external" href="https://huggingface.co/roberta-large"><code class="docutils literal notranslate"><span class="pre">roberta-large</span></code></a> on <a class="reference external" href="https://huggingface.co/datasets/glue/viewer/mrpc"><code class="docutils literal notranslate"><span class="pre">MRPC</span></code> GLUE</a> dataset using different PEFT methods. The notebooks are given in <code class="docutils literal notranslate"><span class="pre">~examples/sequence_classification</span></code>.</p>
</section>
</section>
<section id="peft-accelerate">
<h2>PEFT + 🤗 Accelerate<a class="headerlink" href="#peft-accelerate" title="Link to this heading">#</a></h2>
<p>PEFT models work with 🤗 Accelerate out of the box. Use 🤗 Accelerate for Distributed training on various hardware such as GPUs, Apple Silicon devices, etc during training.
Use 🤗 Accelerate for inferencing on consumer hardware with small resources.</p>
<section id="example-of-peft-model-training-using-accelerates-deepspeed-integration">
<h3>Example of PEFT model training using 🤗 Accelerate’s DeepSpeed integration<a class="headerlink" href="#example-of-peft-model-training-using-accelerates-deepspeed-integration" title="Link to this heading">#</a></h3>
<p>DeepSpeed version required <code class="docutils literal notranslate"><span class="pre">v0.8.0</span></code>. An example is provided in <code class="docutils literal notranslate"><span class="pre">~examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py</span></code>.
a. First, run <code class="docutils literal notranslate"><span class="pre">accelerate</span> <span class="pre">config</span> <span class="pre">--config_file</span> <span class="pre">ds_zero3_cpu.yaml</span></code> and answer the questionnaire.
Below are the contents of the config file.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">compute_environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class="nt">deepspeed_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">gradient_accumulation_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">gradient_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">  </span><span class="nt">offload_optimizer_device</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cpu</span>
<span class="w">  </span><span class="nt">offload_param_device</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cpu</span>
<span class="w">  </span><span class="nt">zero3_init_flag</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">zero3_save_16bit_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">zero_stage</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="nt">distributed_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DEEPSPEED</span>
<span class="nt">downcast_bf16</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;no&quot;</span>
<span class="nt">dynamo_backend</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;NO&quot;</span>
<span class="nt">fsdp_config</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
<span class="nt">machine_rank</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">main_training_function</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="nt">megatron_lm_config</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
<span class="nt">mixed_precision</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;no&quot;</span>
<span class="nt">num_machines</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">num_processes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">rdzv_backend</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class="nt">same_network</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">use_cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
<p>b. run the below command to launch the example script</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>accelerate<span class="w"> </span>launch<span class="w"> </span>--config_file<span class="w"> </span>ds_zero3_cpu.yaml<span class="w"> </span>examples/peft_lora_seq2seq_accelerate_ds_zero3_offload.py
</pre></div>
</div>
<p>c. output logs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>GPU<span class="w"> </span>Memory<span class="w"> </span>before<span class="w"> </span>entering<span class="w"> </span>the<span class="w"> </span>train<span class="w"> </span>:<span class="w"> </span><span class="m">1916</span>
GPU<span class="w"> </span>Memory<span class="w"> </span>consumed<span class="w"> </span>at<span class="w"> </span>the<span class="w"> </span>end<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>train<span class="w"> </span><span class="o">(</span>end-begin<span class="o">)</span>:<span class="w"> </span><span class="m">66</span>
GPU<span class="w"> </span>Peak<span class="w"> </span>Memory<span class="w"> </span>consumed<span class="w"> </span>during<span class="w"> </span>the<span class="w"> </span>train<span class="w"> </span><span class="o">(</span>max-begin<span class="o">)</span>:<span class="w"> </span><span class="m">7488</span>
GPU<span class="w"> </span>Total<span class="w"> </span>Peak<span class="w"> </span>Memory<span class="w"> </span>consumed<span class="w"> </span>during<span class="w"> </span>the<span class="w"> </span>train<span class="w"> </span><span class="o">(</span>max<span class="o">)</span>:<span class="w"> </span><span class="m">9404</span>
CPU<span class="w"> </span>Memory<span class="w"> </span>before<span class="w"> </span>entering<span class="w"> </span>the<span class="w"> </span>train<span class="w"> </span>:<span class="w"> </span><span class="m">19411</span>
CPU<span class="w"> </span>Memory<span class="w"> </span>consumed<span class="w"> </span>at<span class="w"> </span>the<span class="w"> </span>end<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>train<span class="w"> </span><span class="o">(</span>end-begin<span class="o">)</span>:<span class="w"> </span><span class="m">0</span>
CPU<span class="w"> </span>Peak<span class="w"> </span>Memory<span class="w"> </span>consumed<span class="w"> </span>during<span class="w"> </span>the<span class="w"> </span>train<span class="w"> </span><span class="o">(</span>max-begin<span class="o">)</span>:<span class="w"> </span><span class="m">0</span>
CPU<span class="w"> </span>Total<span class="w"> </span>Peak<span class="w"> </span>Memory<span class="w"> </span>consumed<span class="w"> </span>during<span class="w"> </span>the<span class="w"> </span>train<span class="w"> </span><span class="o">(</span>max<span class="o">)</span>:<span class="w"> </span><span class="m">19411</span>
<span class="nv">epoch</span><span class="o">=</span><span class="m">4</span>:<span class="w"> </span><span class="nv">train_ppl</span><span class="o">=</span>tensor<span class="o">(</span><span class="m">1</span>.0705,<span class="w"> </span><span class="nv">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="o">)</span><span class="w"> </span><span class="nv">train_epoch_loss</span><span class="o">=</span>tensor<span class="o">(</span><span class="m">0</span>.0681,<span class="w"> </span><span class="nv">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="o">)</span>
<span class="m">100</span>%<span class="p">|</span>████████████████████████████████████████████████████████████████████████████████████████████<span class="p">|</span><span class="w"> </span><span class="m">7</span>/7<span class="w"> </span><span class="o">[</span><span class="m">00</span>:27&lt;<span class="m">00</span>:00,<span class="w">  </span><span class="m">3</span>.92s/it<span class="o">]</span>
GPU<span class="w"> </span>Memory<span class="w"> </span>before<span class="w"> </span>entering<span class="w"> </span>the<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>:<span class="w"> </span><span class="m">1982</span>
GPU<span class="w"> </span>Memory<span class="w"> </span>consumed<span class="w"> </span>at<span class="w"> </span>the<span class="w"> </span>end<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span><span class="nb">eval</span><span class="w"> </span><span class="o">(</span>end-begin<span class="o">)</span>:<span class="w"> </span>-66
GPU<span class="w"> </span>Peak<span class="w"> </span>Memory<span class="w"> </span>consumed<span class="w"> </span>during<span class="w"> </span>the<span class="w"> </span><span class="nb">eval</span><span class="w"> </span><span class="o">(</span>max-begin<span class="o">)</span>:<span class="w"> </span><span class="m">672</span>
GPU<span class="w"> </span>Total<span class="w"> </span>Peak<span class="w"> </span>Memory<span class="w"> </span>consumed<span class="w"> </span>during<span class="w"> </span>the<span class="w"> </span><span class="nb">eval</span><span class="w"> </span><span class="o">(</span>max<span class="o">)</span>:<span class="w"> </span><span class="m">2654</span>
CPU<span class="w"> </span>Memory<span class="w"> </span>before<span class="w"> </span>entering<span class="w"> </span>the<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>:<span class="w"> </span><span class="m">19411</span>
CPU<span class="w"> </span>Memory<span class="w"> </span>consumed<span class="w"> </span>at<span class="w"> </span>the<span class="w"> </span>end<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span><span class="nb">eval</span><span class="w"> </span><span class="o">(</span>end-begin<span class="o">)</span>:<span class="w"> </span><span class="m">0</span>
CPU<span class="w"> </span>Peak<span class="w"> </span>Memory<span class="w"> </span>consumed<span class="w"> </span>during<span class="w"> </span>the<span class="w"> </span><span class="nb">eval</span><span class="w"> </span><span class="o">(</span>max-begin<span class="o">)</span>:<span class="w"> </span><span class="m">0</span>
CPU<span class="w"> </span>Total<span class="w"> </span>Peak<span class="w"> </span>Memory<span class="w"> </span>consumed<span class="w"> </span>during<span class="w"> </span>the<span class="w"> </span><span class="nb">eval</span><span class="w"> </span><span class="o">(</span>max<span class="o">)</span>:<span class="w"> </span><span class="m">19411</span>
<span class="nv">accuracy</span><span class="o">=</span><span class="m">100</span>.0
eval_preds<span class="o">[</span>:10<span class="o">]=[</span><span class="s1">&#39;no complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;no complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;no complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;no complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;no complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;no complaint&#39;</span><span class="o">]</span>
dataset<span class="o">[</span><span class="s1">&#39;train&#39;</span><span class="o">][</span>label_column<span class="o">][</span>:10<span class="o">]=[</span><span class="s1">&#39;no complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;no complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;no complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;no complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;no complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;complaint&#39;</span>,<span class="w"> </span><span class="s1">&#39;no complaint&#39;</span><span class="o">]</span>
</pre></div>
</div>
</section>
<section id="example-of-peft-model-inference-using-accelerates-big-model-inferencing-capabilities">
<h3>Example of PEFT model inference using 🤗 Accelerate’s Big Model Inferencing capabilities<a class="headerlink" href="#example-of-peft-model-inference-using-accelerates-big-model-inferencing-capabilities" title="Link to this heading">#</a></h3>
<p>An example is provided in <a class="reference external" href="https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_big_model_inference.ipynb"><code class="docutils literal notranslate"><span class="pre">~examples/causal_language_modeling/peft_lora_clm_accelerate_big_model_inference.ipynb</span></code></a>.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>The compatibility of PEFT with tools like 🤗 Accelerate and DeepSpeed illustrates its practicality and adaptability in real-world applications. This compatibility ensures that PEFT can be easily integrated into existing workflows, allowing for seamless scaling and adaptation of models.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/llms/peft"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="peft-llms.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">PEFT for LLMs</p>
      </div>
    </a>
    <a class="right-next"
       href="../q-learning/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Q-Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-peft-methods">Supported PEFT Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started-with-peft">Getting Started with PEFT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-cases">Use Cases</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#get-comparable-performance-to-full-finetuning-by-adapting-llms-to-downstream-tasks-using-consumer-hardware">Get comparable performance to full finetuning by adapting LLMs to downstream tasks using consumer hardware</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-tuning-of-diffusion-models">Parameter Efficient Tuning of Diffusion Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-tuning-of-llms-for-rlhf-components-such-as-ranker-and-policy">Parameter Efficient Tuning of LLMs for RLHF components such as Ranker and Policy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#int8-training-of-large-models-in-colab-using-peft-lora-and-bits-and-bytes">INT8 training of large models in Colab using PEFT LoRA and bits_and_bytes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#save-compute-and-storage-even-for-medium-and-small-models">Save compute and storage even for medium and small models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#peft-accelerate">PEFT + 🤗 Accelerate</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-peft-model-training-using-accelerates-deepspeed-integration">Example of PEFT model training using 🤗 Accelerate’s DeepSpeed integration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-peft-model-inference-using-accelerates-big-model-inferencing-capabilities">Example of PEFT model inference using 🤗 Accelerate’s Big Model Inferencing capabilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://entelecheia.me" target="_blank">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>