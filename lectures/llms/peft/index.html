
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Parameter-Efficient Fine-Tuning (PEFT) &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/dataframe.css?v=574a5d82" />
    <link rel="stylesheet" type="text/css" href="../../../_static/slide.css?v=06ccce15" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../../_static/tabs.js?v=3ee01567"></script>
    <script src="../../../_static/js/hoverxref.js"></script>
    <script src="../../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/llms/peft/index';</script>
    <link rel="canonical" href="https://lectures.jeju.ai/lectures/llms/peft/index.html" />
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="PEFT for LLMs" href="peft-llms.html" />
    <link rel="prev" title="Retrieval Augmented Generation (RAG)" href="../rag/index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Courses</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_intro/index.html">Introduction to NLP</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/intro/index.html">Introduction</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/apps/index.html">NLP Applications</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/apps/research1.html">Research Part I</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/apps/research2.html">Research Part II</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/lm/index.html">Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/lm/ngram.html">N-gram Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/lm/usage.html">Usage of Language Models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/datasets/index.html">Datasets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/datasets/corpus.html">Text Data Collection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/datasets/lab-dart.html">Lab: Crawling DART Data</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/topic/index.html">Topic Modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/methods.html">Topic Modeling Methodologies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/lab-methods.html">Lab: Topic Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/lab-coherence.html">Lab: Topic Coherence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/tomotopy.html">Lab: Tomotopy</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/sentiments/index.html">Sentiment Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lexicon.html">Lexicon-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/ml.html">Machine Learning-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lab-lexicon.html">Lab: Lexicon-based Sentiment Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lab-ml.html">Lab: ML-based Sentiment Classification</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/tokenization/index.html">Tokenization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/tokenization.html">Understanding the Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/pos.html">Part-of-Speech Tagging and Parsing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/ngrams.html">N-grams for Tokenization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/korean.html">Tokenization in Korean</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/lab-tokenization.html">Lab: Tokenization and Pre-processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/lab-korean.html">Lab: Korean Text Processing</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/vectorization/index.html">Vector Representation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/semantics.html">Vector Semantics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/bow.html">Bags of Words Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/tf-idf.html">TF-IDF Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/similarity.html">Word Similarity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/lab-similarity.html">Lab: Word Similarity</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/embeddings/index.html">Word Embeddings</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/nlm.html">Neural Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/w2v.html">Word2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/glove.html">GloVe</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/fasttext.html">FastText</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_deep/index.html">Deep Learning for NLP</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/llms/index.html">Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/zeroshot.html">Zero Shot and Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/decoding.html">Decoding and Search Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/plms.html">Pretrained Language Models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/transformers/index.html">Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/bert.html">BERT: Bidirectional Encoder Representations from Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/bertviz.html">BERT: Visualizing Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/datasets/index.html">Datasets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/datasets/mc4.html">mC4 Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/datasets/lab-eda.html">Lab: Exploratory Data Analysis (EDA)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/tokenization/index.html">Tokenization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/subword.html">Subword Tokenization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/pipeline.html">Tokenization Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/bpe.html">BPE Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/wordpiece.html">WordPiece Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/unigram.html">Unigram Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/lab-train-tokenizers.html">Lab: Training Tokenizers</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/training/index.html">Training Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-pretrain-mlm.html">Lab: Pretraining LMs - MLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-pretrain-clm.html">Lab: Pretraining LMs - CLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-finetune-mlm.html">Lab: Finetuining a MLM</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/chatbots/index.html">Conversational AI and Chatbots</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/chatbots/rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/chatbots/detectGPT.html">How to Spot Machine-Written Texts</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_advances/index.html">Advances in AI and NLP</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/thesis.html">Writing a Thesis</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_advances/gpt/index.html">Generative Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/detectGPT.html">DetectGPT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/gpt4.html">GPT-4</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/camelids.html">Meet the Camelids: A Family of LLMs</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/sam/index.html">Segment Anything</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../aiart/index.html">AI Art (Generative AI)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/intro/index.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/brave/index.html">A Brave New World</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/aiart/index.html">Art and Music in Light of AI</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../aiart/text-to-image/index.html">Text-to-Image Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/imagen.html">Imagen</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/motion-capture-and-synthesis/index.html">Motion Capture and Motion Synthesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/robot/index.html">Robot Drawing System</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mlops/index.html">Machine Learning Systems Design</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/devops/index.html">DevOps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/gitops.html">GitOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/devsecops.html">DevSecOps</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/dotfiles/index.html">Dotfiles</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai">Dotfiles Project</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dotdrop/">Dotdrop Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/github/">GitHub Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/ssh-gpg-age/">SSH, GPG, and Age Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/sops/">SOPS Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/pass/">Pass and Passage Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/doppler/">Doppler Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dockerfiles/">Dockerfiles Usage</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/security/index.html">Security Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/auth-enc-sign.html">Authentication, Encryption, and Signing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/age-gpg-ssh.html">SSH, GPG, and AGE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/pass.html">Unix Password Managers</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/github/index.html">GitHub Workflow</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/fork-pull.html">Github’s Fork &amp; Pull Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/template.html">Project Templating Tools</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-python.entelecheia.ai/">Hyperfast Python Template</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-template.entelecheia.ai/">Hyperfast Template</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/containerization/index.html">Containerization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/docker.html">Docker</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/containerd.html"><code class="docutils literal notranslate"><span class="pre">containerd</span></code></a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/simple-pipeline/index.html">Simple MLOps Pipeline</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/simple-pipeline/server.html">Server Setup &amp; Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/simple-pipeline/vpn.html">VPN Connectivity</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/llmops/index.html">LLMOps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/llmops/bentoml.html">Introduction to BentoML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/llmops/bentochain.html">Deploy a Voice-Based Chatbot with BentoML, LangChain, and Gradio</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsecon/index.html">Data Science for Economics and Finance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/intro/index.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/intro/introduction.html">Data Science in Economics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/intro/challenges.html">Technical Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/intro/methods.html">Data Analytics Methods</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/cb/index.html">Central Banks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/cb/altdata.html">Alternative Data Sources for Central Banks</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/fomc/index.html">Textual Analysis of FOMC contents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/01_numerical_data.html">Preparing Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/02_textual_data.html">Preparing Textual Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/03_EDA_numericals1.html">EDA on Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/03_EDA_numericals2.html">EDA on Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/04_training_datasets.html">Create Training Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/05_features.html">Visualizing Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/06_AutoML.html">Checking Baseline with AutoML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/07_predict_sentiments.html">Predicting Sentiments of FOMC Corpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/08_EDA_sentiments1.html">EDA on Sentiments: Correlation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/08_EDA_sentiments2.html">EDA on Sentiment Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/09_visualize_features.html">Visualize Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/10_monetary_shocks.html">Monetary Policy Shocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/11_AutoML_with_tones.html">Predicting the next decisions with tones</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/esg-ratings/index.html">ESG Ratings</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/prepare_datasets.html">Preparing training datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/improve_datasets.html">Improving classification datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/train_classifiers.html">Training Classifiers for ESG Ratings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/build_news_corpus.html">Building <code class="docutils literal notranslate"><span class="pre">econ_news_kr</span></code> corpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/predict_esg_classes.html">Predicting ESG Categories and Polarities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/cross_validate_datasets.html">Cross validating datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/prepare_datasets_for_labeling.html">Preparing active learning data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/all_in_one_pipeline.html">Putting them together in a pipeline</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../softeng/index.html">Software Engineering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/intro/index.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/introduction.html">Software Engineering?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/processes.html">Software Processes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/sdlc.html">Software Development Life Cycle (SDLC)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/requirements.html">Requirements Engineering (RE)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/proposal/index.html">Project Proposal</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/proposal/steps.html">Steps in Software Engineering Projects</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/proposal/guidelines.html">Software Engineering Proposal Guideline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/proposal/template.html">Project Proposal Template</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/vcs/index.html">Version Control Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/00_introduction.html">0. Introduction to version control</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/01_solo_work_with_git.html">1. Solo work with git</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/02_fixing_mistakes.html">2. Fixing mistakes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/03_publishing.html">3. Publishing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/04_collaboration.html">4. Collaboration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/05_fork_and_pull.html">5. Fork and Pull</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/06_git_theory.html">6. Git Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/07_branches.html">7. Branches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/08_advanced_git_concepts.html">8. Advanced git concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/09_github_pages.html">9. Publishing from GitHub</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/10_rebasing.html">10. Rebasing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/11_debugging_with_git_bisect.html">11. Debugging With git bisect</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/12_multiple_remotes.html">12. Working with multiple remotes</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/spm/index.html">Software Process Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/spm/agile.html">Agile Software Development</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/devops/index.html">DevOps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/devops/gitops.html">GitOps</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Large Language Models</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../intro/index.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../intro/llms.html">Large Language Models?</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../stack/index.html">LLM Stacks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../stack/infra.html">Generative AI Infrastructure Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../stack/architecture.html">LLM Application Architectures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../stack/app.html">LLM App Ecosystem</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../agents/index.html">AI Agents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../agents/autogen.html">AutoGen</a></li>
<li class="toctree-l3"><a class="reference internal" href="../agents/autoscraper.html">AutoGen AutoScraper Agent</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../finetune/index.html">LLM Fine-tuning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetune/autotrain.html">Fine-Tuning LLMs with Hugging Face AutoTrain</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/index.html">Retrieval Augmented Generation (RAG)</a></li>
<li class="toctree-l2 current active has-children"><a class="current reference internal" href="#">Parameter-Efficient Fine-Tuning (PEFT)</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="peft-llms.html">PEFT for LLMs</a></li>
<li class="toctree-l3"><a class="reference internal" href="peft-hf.html">PEFT in HuggingFace Libraries</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../q-learning/index.html">Q-Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../q-learning/qstar.html">Q-Star (Q*)</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://youngjoon-lee.com">youngjoon-lee.com</a></li>
<li class="toctree-l1"><a class="reference external" href="https://courses.jeju.ai">courses.jeju.ai</a></li>
<li class="toctree-l1"><a class="reference external" href="https://research.jeju.ai">research.jeju.ai</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about/index.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/lectures/llms/peft/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Parameter-Efficient Fine-Tuning (PEFT)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#i-introduction-to-peft">I. Introduction to PEFT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-and-overview">Definition and Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-in-machine-learning-and-ai">Importance in Machine Learning and AI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#contrasting-with-traditional-fine-tuning-approaches">Contrasting with Traditional Fine-Tuning Approaches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ii-foundational-concepts">II. Foundational Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-basics-and-relevance">Transfer Learning: Basics and Relevance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-model-parameters-and-architectures">Understanding Model Parameters and Architectures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-significance-of-efficient-fine-tuning">The Significance of Efficient Fine-Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-key-concept-of-model-generalization">The Key Concept of Model Generalization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#iii-techniques-and-methods-in-peft">III. Techniques and Methods in PEFT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-peft-techniques">Overview of PEFT Techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-and-layer-wise-fine-tuning">Adaptive and Layer-wise Fine-Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#low-rank-adaptation-and-sparse-fine-tuning">Low-Rank Adaptation and Sparse Fine-Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-based-and-adapter-based-methods">Prompt-based and Adapter-based Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spotlight-on-key-techniques-bitfit-lora-and-more">Spotlight on Key Techniques: BitFit, LoRA, and More</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#iv-practical-applications-of-peft">IV. Practical Applications of PEFT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#case-studies-across-domains">Case Studies Across Domains</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#industry-specific-applications">Industry-Specific Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-into-existing-ml-pipelines">Integration into Existing ML Pipelines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#highlighting-applications-in-nlp-and-transformer-models">Highlighting Applications in NLP and Transformer Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#v-challenges-and-limitations">V. Challenges and Limitations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-pefts-limitations">Assessing PEFT’s Limitations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-and-underfitting-challenges">Overfitting and Underfitting Challenges</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-and-computational-efficiency">Scaling and Computational Efficiency</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#balancing-efficiency-with-performance">Balancing Efficiency with Performance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vi-current-trends-and-future-directions">VI. Current Trends and Future Directions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advancing-research-in-peft">Advancing Research in PEFT</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pefts-impact-on-large-scale-models">PEFT’s Impact on Large-Scale Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#projecting-the-future-of-peft-in-ai-and-ml">Projecting the Future of PEFT in AI and ML</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#peft-in-conjunction-with-emerging-ai-technologies">PEFT in Conjunction with Emerging AI Technologies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vii-conclusion">VII. Conclusion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#synthesizing-the-discussion">Synthesizing the Discussion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implications-for-ai-and-machine-learning">Implications for AI and Machine Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-contents">Next Contents</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="parameter-efficient-fine-tuning-peft">
<h1>Parameter-Efficient Fine-Tuning (PEFT)<a class="headerlink" href="#parameter-efficient-fine-tuning-peft" title="Link to this heading">#</a></h1>
<a class="reference internal image-reference" href="../../../_images/peft.png"><img alt="../../../_images/peft.png" class="align-center" src="../../../_images/peft.png" style="width: 100%;" />
</a>
<p>This lecture offers an in-depth exploration of Parameter-Efficient Fine-Tuning (PEFT) within the machine learning landscape. We will delve into its foundational concepts, diverse methodologies, practical implementations, and the evolving challenges and prospective developments in the field. The aim is to provide students with a comprehensive understanding of PEFT, equipping them with the knowledge to effectively apply these principles in both research and professional contexts.</p>
<section id="i-introduction-to-peft">
<h2>I. Introduction to PEFT<a class="headerlink" href="#i-introduction-to-peft" title="Link to this heading">#</a></h2>
<section id="definition-and-overview">
<h3>Definition and Overview<a class="headerlink" href="#definition-and-overview" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>PEFT Explained</strong>: In machine learning, PEFT represents a suite of techniques designed for the efficient fine-tuning of large-scale models. This approach stands in contrast to traditional fine-tuning methods, which typically involve the modification of a significant number or even all parameters in a pre-trained model to adapt it to new tasks or data sets.</p></li>
<li><p><strong>Focus on Efficiency</strong>: The cornerstone of PEFT is its emphasis on minimizing computational and resource demands. This is achieved by selectively adjusting a limited subset of the model’s parameters, thereby preserving the core structure and learned representations of the original model.</p></li>
</ul>
</section>
<section id="importance-in-machine-learning-and-ai">
<h3>Importance in Machine Learning and AI<a class="headerlink" href="#importance-in-machine-learning-and-ai" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Scaling Models</strong>: As machine learning models become increasingly large and complex, the computational expense of traditional fine-tuning methods becomes impractical. PEFT addresses this challenge by enabling more efficient model adaptations for specific tasks or data sets.</p></li>
<li><p><strong>Resource Utilization</strong>: PEFT renders advanced machine learning techniques more accessible, particularly for entities with constrained computational resources, by reducing the necessity for extensive resource allocation.</p></li>
<li><p><strong>Adaptability of Models</strong>: By maintaining the fundamental generalization capabilities of large models, PEFT provides a means to tailor these models to specific requirements without a significant overhaul.</p></li>
</ul>
</section>
<section id="contrasting-with-traditional-fine-tuning-approaches">
<h3>Contrasting with Traditional Fine-Tuning Approaches<a class="headerlink" href="#contrasting-with-traditional-fine-tuning-approaches" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Selective vs. Comprehensive Tuning</strong>: Traditional fine-tuning approaches often entail comprehensive parameter adjustments, leading to prolonged training durations and substantial computational costs. In contrast, PEFT adopts a more selective parameter tuning strategy, yielding more efficient training processes.</p></li>
<li><p><strong>Mitigating Overfitting Risks</strong>: Traditional fine-tuning can sometimes result in overfitting, particularly with smaller data sets. PEFT helps balance model adaptability against the risks of overfitting.</p></li>
<li><p><strong>Real-world Applicability</strong>: The efficiency inherent in PEFT makes it particularly suitable for practical applications where rapid deployment and adaptability are paramount.</p></li>
</ul>
</section>
<section id="core-concepts">
<h3>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Leveraging Transfer Learning</strong>: At the heart of PEFT is the principle of transfer learning, which involves repurposing a model developed for one task as a starting point for another task. PEFT refines this concept by focusing on the efficient transfer of knowledge.</p></li>
<li><p><strong>Model Adaptability Insights</strong>: A fundamental aspect of PEFT is understanding how models adapt to new data or tasks, which involves identifying the critical parameters for adaptation and efficiently tuning them to achieve optimal results.</p></li>
<li><p><strong>Navigating Resource Constraints</strong>: PEFT emerges as a vital solution in scenarios with limited resources, emphasizing model efficiency without significant performance compromises.</p></li>
</ul>
</section>
</section>
<section id="ii-foundational-concepts">
<h2>II. Foundational Concepts<a class="headerlink" href="#ii-foundational-concepts" title="Link to this heading">#</a></h2>
<section id="transfer-learning-basics-and-relevance">
<h3>Transfer Learning: Basics and Relevance<a class="headerlink" href="#transfer-learning-basics-and-relevance" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Defining Transfer Learning</strong>: Transfer learning is the process of applying knowledge acquired from solving one problem to a different, yet related, problem. In the context of machine learning, this often entails employing a pre-trained model for a new task.</p></li>
<li><p><strong>PEFT’s Role in Transfer Learning</strong>: PEFT harnesses transfer learning by fine-tuning a small subset of parameters within a pre-trained model, thereby enabling adaptability to new tasks with a reduced computational load.</p></li>
<li><p><strong>Widespread Applications</strong>: Transfer learning, especially when applied through PEFT, has gained extensive usage in fields such as natural language processing (NLP), computer vision, and other emerging domains.</p></li>
</ul>
</section>
<section id="understanding-model-parameters-and-architectures">
<h3>Understanding Model Parameters and Architectures<a class="headerlink" href="#understanding-model-parameters-and-architectures" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Model Parameters Explored</strong>: These are the elements of the model that are learned from the training data. In deep learning models, these parameters typically encompass weights and biases in neural networks.</p></li>
<li><p><strong>Exploring Model Architectures</strong>: The architecture of a model refers to its structural design, including the organization and interconnection of layers. Different architectures are better suited to specific task types.</p></li>
<li><p><strong>PEFT’s Approach to Parameters and Architectures</strong>: Strategies in PEFT often focus on identifying the most crucial parameters for a given task and selectively fine-tuning them while largely maintaining the rest of the model’s structure.</p></li>
</ul>
</section>
<section id="the-significance-of-efficient-fine-tuning">
<h3>The Significance of Efficient Fine-Tuning<a class="headerlink" href="#the-significance-of-efficient-fine-tuning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Managing Resources Effectively</strong>: Efficient fine-tuning is imperative for the effective management of computational resources, especially when dealing with sizable models such as transformers in NLP or convolutional networks in computer vision.</p></li>
<li><p><strong>Reducing Time and Costs</strong>: Methods that prioritize efficiency in fine-tuning significantly curtail the time and expenses associated with training and adapting models, thereby making machine learning applications more viable and scalable.</p></li>
<li><p><strong>Mitigating Environmental Impact</strong>: With the growing size of AI and ML models, concerns regarding their environmental impact due to energy consumption have arisen. Efficient fine-tuning can help alleviate these concerns by diminishing the computational requirements.</p></li>
</ul>
</section>
<section id="the-key-concept-of-model-generalization">
<h3>The Key Concept of Model Generalization<a class="headerlink" href="#the-key-concept-of-model-generalization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Defining Model Generalization</strong>: Model generalization refers to the capability of a model to perform effectively on new, unseen data. It gauges the extent to which the training data’s learnings are applicable to external data sets.</p></li>
<li><p><strong>Balancing Act</strong>: A pivotal challenge within PEFT is maintaining a balance between adapting the model for specific tasks and preserving its ability to generalize across various tasks.</p></li>
<li><p><strong>Enhancing Generalization through PEFT</strong>: By fine-tuning models efficiently, PEFT aids in preserving the generalization abilities acquired during the model’s initial training, while still permitting task-specific adaptability.</p></li>
</ul>
</section>
</section>
<section id="iii-techniques-and-methods-in-peft">
<h2>III. Techniques and Methods in PEFT<a class="headerlink" href="#iii-techniques-and-methods-in-peft" title="Link to this heading">#</a></h2>
<section id="overview-of-peft-techniques">
<h3>Overview of PEFT Techniques<a class="headerlink" href="#overview-of-peft-techniques" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Diverse Methodological Landscape</strong>: The array of techniques within PEFT is diverse, each providing a distinct method for modifying a small portion of a model’s parameters for effective adaptation.</p></li>
<li><p><strong>Unified Objective</strong>: Despite the variation in techniques, all PEFT methods strive to maintain the intrinsic strengths of the pre-trained model while efficiently adapting it to new tasks or data sets.</p></li>
</ul>
</section>
<section id="adaptive-and-layer-wise-fine-tuning">
<h3>Adaptive and Layer-wise Fine-Tuning<a class="headerlink" href="#adaptive-and-layer-wise-fine-tuning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Adaptive Fine-Tuning Defined</strong>: This approach entails selectively tuning certain parameters more intensively than others, based on their relevance to the specific task at hand.</p></li>
<li><p><strong>Layer-wise Fine-Tuning Explained</strong>: This method focuses on fine-tuning specific layers within a neural network, often targeting the latter layers as they are generally more specialized for particular tasks.</p></li>
<li><p><strong>Effectiveness in Application</strong>: These methods have shown particular efficacy in deep learning models, where different layers encapsulate various levels of abstractions.</p></li>
</ul>
</section>
<section id="low-rank-adaptation-and-sparse-fine-tuning">
<h3>Low-Rank Adaptation and Sparse Fine-Tuning<a class="headerlink" href="#low-rank-adaptation-and-sparse-fine-tuning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Low-Rank Adaptation Overview</strong>: This technique incorporates low-rank matrices to modify existing weights within a neural network, striking a balance between adaptability and parameter efficiency.</p></li>
<li><p><strong>Understanding Sparse Fine-Tuning</strong>: Sparse fine-tuning involves adjusting only a small, selected subset of weights, thereby keeping the majority of the model intact.</p></li>
<li><p><strong>Efficiency and Utility</strong>: Both of these methods are highly efficient in terms of computational resource usage and have demonstrated promising results in diverse ML tasks.</p></li>
</ul>
</section>
<section id="prompt-based-and-adapter-based-methods">
<h3>Prompt-based and Adapter-based Methods<a class="headerlink" href="#prompt-based-and-adapter-based-methods" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Exploring Prompt-based Methods</strong>: Commonly used in NLP, these methods entail appending prompts to input data, thereby guiding the model to adapt its responses without changing the underlying model weights.</p></li>
<li><p><strong>Adapter-based Methods Defined</strong>: These methods introduce small, trainable modules (adapters) within the layers of a model. The adapters are fine-tuned, while the original model weights remain unchanged.</p></li>
<li><p><strong>Innovative Flexibility</strong>: Representing a more recent development in PEFT, these techniques offer significant flexibility and adaptability for a variety of tasks.</p></li>
</ul>
</section>
<section id="spotlight-on-key-techniques-bitfit-lora-and-more">
<h3>Spotlight on Key Techniques: BitFit, LoRA, and More<a class="headerlink" href="#spotlight-on-key-techniques-bitfit-lora-and-more" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>BitFit Unpacked</strong>: This technique focuses on fine-tuning only the bias terms within a model. It is remarkably simple yet has proven effective for certain tasks.</p></li>
<li><p><strong>Layer-wise Relevance Propagation (LoRA) Explained</strong>: LoRA involves adjusting the weights in existing layers by adding learnable low-rank matrices, facilitating efficient adaptation with minimal parameter alterations.</p></li>
<li><p><strong>Exploring Additional Techniques</strong>: The field of PEFT is continuously evolving, with new techniques regularly emerging and contributing to the landscape of efficient model adaptation.</p></li>
</ul>
</section>
</section>
<section id="iv-practical-applications-of-peft">
<h2>IV. Practical Applications of PEFT<a class="headerlink" href="#iv-practical-applications-of-peft" title="Link to this heading">#</a></h2>
<section id="case-studies-across-domains">
<h3>Case Studies Across Domains<a class="headerlink" href="#case-studies-across-domains" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>PEFT in Natural Language Processing (NLP)</strong>: PEFT has brought about a transformative impact in NLP by enabling the efficient fine-tuning of large language models for specific tasks such as sentiment analysis, language translation, and text summarization.</p></li>
<li><p><strong>Computer Vision and PEFT</strong>: Within the realm of computer vision, PEFT facilitates the swift adaptation of pre-trained models to new tasks like image classification, object detection, and facial recognition, all while minimizing computational demands.</p></li>
<li><p><strong>Extending to Other Fields</strong>: The reach of PEFT extends to other areas, including audio processing, where it assists in tasks such as speech recognition and sound classification.</p></li>
</ul>
</section>
<section id="industry-specific-applications">
<h3>Industry-Specific Applications<a class="headerlink" href="#industry-specific-applications" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Healthcare</strong>: In the healthcare sector, PEFT enables the development of customized diagnostic tools and predictive models for patient care, improving accuracy and efficiency while keeping computational costs manageable.</p></li>
<li><p><strong>Finance</strong>: The finance industry benefits from PEFT through the creation of personalized trading algorithms and risk assessment models, allowing for rapid adaptation to fluctuating market conditions.</p></li>
<li><p><strong>Automotive Industry</strong>: The automotive sector employs PEFT to enhance autonomous driving systems, fine-tuning algorithms for improved decision-making and safety features.</p></li>
</ul>
</section>
<section id="integration-into-existing-ml-pipelines">
<h3>Integration into Existing ML Pipelines<a class="headerlink" href="#integration-into-existing-ml-pipelines" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Seamless Incorporation</strong>: PEFT techniques can be effortlessly integrated into existing ML pipelines, facilitating the rapid deployment of models across varied environments.</p></li>
<li><p><strong>Cost-Effectiveness in Integration</strong>: This integration is especially advantageous for organizations seeking to utilize advanced AI models without incurring substantial computational and financial costs.</p></li>
<li><p><strong>Performance Enhancement</strong>: Beyond resource reduction, PEFT often leads to improvements in model performance by concentrating on parameters that are most relevant to the specific application at hand.</p></li>
</ul>
</section>
<section id="highlighting-applications-in-nlp-and-transformer-models">
<h3>Highlighting Applications in NLP and Transformer Models<a class="headerlink" href="#highlighting-applications-in-nlp-and-transformer-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Transformers and PEFT Synergy</strong>: The emergence of transformer models in NLP marked a significant advancement in the field. PEFT plays a vital role in fine-tuning these models for specific tasks, while retaining their ability to generalize.</p></li>
<li><p><strong>Customization Efficiency</strong>: PEFT enables organizations to customize these potent models efficiently for a broad spectrum of language-related tasks, thereby making advanced NLP techniques more accessible and widely applicable.</p></li>
</ul>
</section>
</section>
<section id="v-challenges-and-limitations">
<h2>V. Challenges and Limitations<a class="headerlink" href="#v-challenges-and-limitations" title="Link to this heading">#</a></h2>
<section id="assessing-pefts-limitations">
<h3>Assessing PEFT’s Limitations<a class="headerlink" href="#assessing-pefts-limitations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Applicability Constraints</strong>: Although PEFT offers powerful capabilities, its applicability is not universal. Certain tasks or model architectures might not benefit substantially from PEFT, necessitating more traditional or comprehensive fine-tuning approaches.</p></li>
<li><p><strong>Accuracy vs. Efficiency Trade-offs</strong>: The efficiency gains in PEFT occasionally come with compromises in model accuracy or robustness, particularly when dealing with complex or diverse data sets.</p></li>
</ul>
</section>
<section id="overfitting-and-underfitting-challenges">
<h3>Overfitting and Underfitting Challenges<a class="headerlink" href="#overfitting-and-underfitting-challenges" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Overfitting Risks</strong>: Overfitting occurs when a model becomes overly tailored to the training data, losing its ability to generalize effectively. PEFT, by focusing on a limited set of parameters, can sometimes intensify this risk.</p></li>
<li><p><strong>Addressing Underfitting</strong>: On the other hand, underfitting happens when the model is insufficiently complex to capture the data’s underlying patterns. This can arise in PEFT if the fine-tuning is overly conservative.</p></li>
<li><p><strong>Achieving Balance</strong>: One of the critical challenges in PEFT is striking the right balance between overfitting and underfitting, ensuring that the model remains both adaptable and generalizable.</p></li>
</ul>
</section>
<section id="scaling-and-computational-efficiency">
<h3>Scaling and Computational Efficiency<a class="headerlink" href="#scaling-and-computational-efficiency" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Scalability Challenges</strong>: As models escalate in size and intricacy, the scalability of PEFT methods comes under scrutiny. Ensuring the effectiveness of these techniques for state-of-the-art models remains a focal area of research.</p></li>
<li><p><strong>Computational Demands</strong>: Although PEFT is designed for computational efficiency, there are instances where the reduction in parameter tuning does not significantly diminish the overall computational burden, particularly in extremely large models.</p></li>
</ul>
</section>
<section id="balancing-efficiency-with-performance">
<h3>Balancing Efficiency with Performance<a class="headerlink" href="#balancing-efficiency-with-performance" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Navigating Performance Trade-offs</strong>: The core dilemma in PEFT is optimizing the balance between resource efficiency and model efficacy. Identifying the optimal juncture where resource savings and model performance converge is critical.</p></li>
<li><p><strong>Contextualized Strategies</strong>: The effectiveness of PEFT methods can vary based on the task, model, and data set, necessitating a context-specific approach in applying these techniques.</p></li>
</ul>
</section>
</section>
<section id="vi-current-trends-and-future-directions">
<h2>VI. Current Trends and Future Directions<a class="headerlink" href="#vi-current-trends-and-future-directions" title="Link to this heading">#</a></h2>
<section id="advancing-research-in-peft">
<h3>Advancing Research in PEFT<a class="headerlink" href="#advancing-research-in-peft" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Technique Evolution</strong>: Continuous research is enhancing and innovating new PEFT techniques, aiming to improve their efficiency and broader applicability.</p></li>
<li><p><strong>Merging with Other ML Modalities</strong>: There is a noticeable trend of integrating PEFT with other machine learning methodologies, such as reinforcement learning and unsupervised learning, to broaden its application scope.</p></li>
<li><p><strong>Developing Customized PEFT Solutions</strong>: Customized PEFT methods for specific industries or applications are being crafted to address unique challenges and requirements.</p></li>
</ul>
</section>
<section id="pefts-impact-on-large-scale-models">
<h3>PEFT’s Impact on Large-Scale Models<a class="headerlink" href="#pefts-impact-on-large-scale-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Democratizing Advanced AI</strong>: PEFT’s capacity to efficiently fine-tune large models could democratize access to advanced AI technologies, making them available to organizations with limited computational resources.</p></li>
<li><p><strong>Augmenting Pre-Trained Models</strong>: PEFT techniques are increasingly vital in adapting pre-trained models for specialized tasks, enhancing their versatility and practical utility.</p></li>
<li><p><strong>Contributing to Sustainable Large Model Use</strong>: By reducing the computational demands of fine-tuning, PEFT contributes to the sustainability and environmental viability of utilizing large-scale models.</p></li>
</ul>
</section>
<section id="projecting-the-future-of-peft-in-ai-and-ml">
<h3>Projecting the Future of PEFT in AI and ML<a class="headerlink" href="#projecting-the-future-of-peft-in-ai-and-ml" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Expectations of Wider Adoption</strong>: As the efficacy and efficiency of PEFT continue to advance, its adoption is expected to increase across various domains of AI and ML.</p></li>
<li><p><strong>Convergence with Emerging Technologies</strong>: PEFT is anticipated to intersect with emerging technologies such as quantum computing and neuromorphic hardware, potentially revolutionizing its capabilities and applications.</p></li>
<li><p><strong>Emphasis on Ethical and Responsible AI</strong>: Future developments in PEFT should be aligned with frameworks prioritizing ethical considerations and responsible deployment of AI technologies.</p></li>
</ul>
</section>
<section id="peft-in-conjunction-with-emerging-ai-technologies">
<h3>PEFT in Conjunction with Emerging AI Technologies<a class="headerlink" href="#peft-in-conjunction-with-emerging-ai-technologies" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Optimizing AI for IoT</strong>: PEFT is poised to play a crucial role in enhancing AI models for Internet of Things (IoT) applications, where computational efficiency is of paramount importance.</p></li>
<li><p><strong>Enabling Advanced Edge Computing</strong>: The application of PEFT in edge computing scenarios can facilitate powerful on-device AI processing with reduced demands on resources.</p></li>
<li><p><strong>Elevating AR and VR Experiences</strong>: In augmented and virtual reality, PEFT can assist in creating more immersive and responsive experiences by efficiently fine-tuning AI models for real-time interaction and engagement.</p></li>
</ul>
</section>
</section>
<section id="vii-conclusion">
<h2>VII. Conclusion<a class="headerlink" href="#vii-conclusion" title="Link to this heading">#</a></h2>
<section id="synthesizing-the-discussion">
<h3>Synthesizing the Discussion<a class="headerlink" href="#synthesizing-the-discussion" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Revisiting PEFT’s Core</strong>: This lecture delved deep into the essence of PEFT, underscoring its pivotal role in fine-tuning large machine learning models efficiently while maintaining their performance.</p></li>
<li><p><strong>Techniques and Their Applications</strong>: We explored a variety of PEFT techniques, ranging from adaptive and layer-wise fine-tuning to innovative methods like prompt-based and adapter-based approaches. The practical applications of these techniques across diverse domains such as NLP, computer vision, and industry-specific scenarios were also highlighted.</p></li>
<li><p><strong>Navigating Challenges and Looking Ahead</strong>: The challenges and limitations inherent to PEFT, including the balancing act between efficiency and performance, and scalability concerns, were addressed. The discussion on current trends and future directions highlighted the dynamic and evolving nature of PEFT and its potential to revolutionize the fields of AI and ML.</p></li>
</ul>
</section>
<section id="implications-for-ai-and-machine-learning">
<h3>Implications for AI and Machine Learning<a class="headerlink" href="#implications-for-ai-and-machine-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Expanding Accessibility</strong>: The role of PEFT in broadening access to advanced AI models by minimizing computational demands is profound, offering significant implications for smaller organizations and independent researchers.</p></li>
<li><p><strong>Driving Innovation and Progress</strong>: The ongoing evolution of PEFT techniques is expected to catalyze further innovation in machine learning, leading to the development of more efficient and potent models.</p></li>
<li><p><strong>Advocating for Ethical Development</strong>: As PEFT continues to progress, it is crucial that these advancements are pursued within frameworks that emphasize ethical considerations and the responsible use of AI.</p></li>
</ul>
</section>
</section>
<section id="next-contents">
<h2>Next Contents<a class="headerlink" href="#next-contents" title="Link to this heading">#</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="peft-llms.html">PEFT for LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="peft-hf.html">PEFT in HuggingFace Libraries</a></li>
</ul>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/llms/peft"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../rag/index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Retrieval Augmented Generation (RAG)</p>
      </div>
    </a>
    <a class="right-next"
       href="peft-llms.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">PEFT for LLMs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#i-introduction-to-peft">I. Introduction to PEFT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-and-overview">Definition and Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-in-machine-learning-and-ai">Importance in Machine Learning and AI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#contrasting-with-traditional-fine-tuning-approaches">Contrasting with Traditional Fine-Tuning Approaches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ii-foundational-concepts">II. Foundational Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-basics-and-relevance">Transfer Learning: Basics and Relevance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-model-parameters-and-architectures">Understanding Model Parameters and Architectures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-significance-of-efficient-fine-tuning">The Significance of Efficient Fine-Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-key-concept-of-model-generalization">The Key Concept of Model Generalization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#iii-techniques-and-methods-in-peft">III. Techniques and Methods in PEFT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-peft-techniques">Overview of PEFT Techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-and-layer-wise-fine-tuning">Adaptive and Layer-wise Fine-Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#low-rank-adaptation-and-sparse-fine-tuning">Low-Rank Adaptation and Sparse Fine-Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-based-and-adapter-based-methods">Prompt-based and Adapter-based Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spotlight-on-key-techniques-bitfit-lora-and-more">Spotlight on Key Techniques: BitFit, LoRA, and More</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#iv-practical-applications-of-peft">IV. Practical Applications of PEFT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#case-studies-across-domains">Case Studies Across Domains</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#industry-specific-applications">Industry-Specific Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-into-existing-ml-pipelines">Integration into Existing ML Pipelines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#highlighting-applications-in-nlp-and-transformer-models">Highlighting Applications in NLP and Transformer Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#v-challenges-and-limitations">V. Challenges and Limitations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-pefts-limitations">Assessing PEFT’s Limitations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-and-underfitting-challenges">Overfitting and Underfitting Challenges</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-and-computational-efficiency">Scaling and Computational Efficiency</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#balancing-efficiency-with-performance">Balancing Efficiency with Performance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vi-current-trends-and-future-directions">VI. Current Trends and Future Directions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advancing-research-in-peft">Advancing Research in PEFT</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pefts-impact-on-large-scale-models">PEFT’s Impact on Large-Scale Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#projecting-the-future-of-peft-in-ai-and-ml">Projecting the Future of PEFT in AI and ML</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#peft-in-conjunction-with-emerging-ai-technologies">PEFT in Conjunction with Emerging AI Technologies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vii-conclusion">VII. Conclusion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#synthesizing-the-discussion">Synthesizing the Discussion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implications-for-ai-and-machine-learning">Implications for AI and Machine Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-contents">Next Contents</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://youngjoon-lee.com" target="_blank">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>