
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Q-Learning &#8212; ἐντελέχεια.άι</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/dataframe.css?v=574a5d82" />
    <link rel="stylesheet" type="text/css" href="../../../_static/slide.css?v=06ccce15" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../../_static/tabs.js?v=3ee01567"></script>
    <script src="../../../_static/js/hoverxref.js"></script>
    <script src="../../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/llms/q-learning/index';</script>
    <link rel="canonical" href="https://lectures.jeju.ai/lectures/llms/q-learning/index.html" />
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Q-Star (Q*)" href="qstar.html" />
    <link rel="prev" title="PEFT in HuggingFace Libraries" href="../peft/peft-hf.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">ἐντελέχεια.άι</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Courses</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_intro/index.html">Introduction to NLP</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_intro/intro/index.html">Introduction</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/apps/index.html">NLP Applications</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/apps/research1.html">Research Part I</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/apps/research2.html">Research Part II</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/lm/index.html">Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/lm/ngram.html">N-gram Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/lm/usage.html">Usage of Language Models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/datasets/index.html">Datasets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/datasets/corpus.html">Text Data Collection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/datasets/lab-dart.html">Lab: Crawling DART Data</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/topic/index.html">Topic Modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/methods.html">Topic Modeling Methodologies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/lab-methods.html">Lab: Topic Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/lab-coherence.html">Lab: Topic Coherence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/topic/tomotopy.html">Lab: Tomotopy</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/sentiments/index.html">Sentiment Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lexicon.html">Lexicon-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/ml.html">Machine Learning-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lab-lexicon.html">Lab: Lexicon-based Sentiment Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/sentiments/lab-ml.html">Lab: ML-based Sentiment Classification</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/tokenization/index.html">Tokenization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/tokenization.html">Understanding the Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/pos.html">Part-of-Speech Tagging and Parsing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/ngrams.html">N-grams for Tokenization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/korean.html">Tokenization in Korean</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/lab-tokenization.html">Lab: Tokenization and Pre-processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/tokenization/lab-korean.html">Lab: Korean Text Processing</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/vectorization/index.html">Vector Representation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/semantics.html">Vector Semantics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/bow.html">Bags of Words Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/tf-idf.html">TF-IDF Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/similarity.html">Word Similarity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/vectorization/lab-similarity.html">Lab: Word Similarity</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_intro/embeddings/index.html">Word Embeddings</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/nlm.html">Neural Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/w2v.html">Word2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/glove.html">GloVe</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_intro/embeddings/fasttext.html">FastText</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_deep/index.html">Deep Learning for NLP</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/llms/index.html">Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/zeroshot.html">Zero Shot and Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/decoding.html">Decoding and Search Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/llms/plms.html">Pretrained Language Models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/transformers/index.html">Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/bert.html">BERT: Bidirectional Encoder Representations from Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/bertviz.html">BERT: Visualizing Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/transformers/byt5.html">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/datasets/index.html">Datasets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/datasets/mc4.html">mC4 Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/datasets/lab-eda.html">Lab: Exploratory Data Analysis (EDA)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/tokenization/index.html">Tokenization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/subword.html">Subword Tokenization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/pipeline.html">Tokenization Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/bpe.html">BPE Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/wordpiece.html">WordPiece Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/unigram.html">Unigram Step-by-Step Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/tokenization/lab-train-tokenizers.html">Lab: Training Tokenizers</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/training/index.html">Training Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-pretrain-mlm.html">Lab: Pretraining LMs - MLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-pretrain-clm.html">Lab: Pretraining LMs - CLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/training/lab-finetune-mlm.html">Lab: Finetuining a MLM</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_deep/chatbots/index.html">Conversational AI and Chatbots</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/chatbots/rlhf.html">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_deep/chatbots/detectGPT.html">How to Spot Machine-Written Texts</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp_advances/index.html">Advances in AI and NLP</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/thesis.html">Writing a Thesis</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../nlp_advances/gpt/index.html">Generative Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/detectGPT.html">DetectGPT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/gpt4.html">GPT-4</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nlp_advances/gpt/camelids.html">Meet the Camelids: A Family of LLMs</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp_advances/sam/index.html">Segment Anything</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../aiart/index.html">AI Art (Generative AI)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/intro/index.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/brave/index.html">A Brave New World</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/aiart/index.html">Art and Music in Light of AI</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../aiart/text-to-image/index.html">Text-to-Image Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../aiart/text-to-image/imagen.html">Imagen</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/motion-capture-and-synthesis/index.html">Motion Capture and Motion Synthesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aiart/robot/index.html">Robot Drawing System</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mlops/index.html">Machine Learning Systems Design</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/intro.html">Introduction to MLOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mlops/project.html">MLOps Project</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/devops/index.html">DevOps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/gitops.html">GitOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/devops/devsecops.html">DevSecOps</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/dotfiles/index.html">Dotfiles</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai">Dotfiles Project</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dotdrop/">Dotdrop Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/github/">GitHub Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/ssh-gpg-age/">SSH, GPG, and Age Setup</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/sops/">SOPS Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/pass/">Pass and Passage Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/doppler/">Doppler Usage</a></li>
<li class="toctree-l3"><a class="reference external" href="https://dotfiles.entelecheia.ai/usage/dockerfiles/">Dockerfiles Usage</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/security/index.html">Security Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/auth-enc-sign.html">Authentication, Encryption, and Signing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/age-gpg-ssh.html">SSH, GPG, and AGE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/security/pass.html">Unix Password Managers</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/github/index.html">GitHub Workflow</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/fork-pull.html">Github’s Fork &amp; Pull Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/github/template.html">Project Templating Tools</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-python.entelecheia.ai/">Hyperfast Python Template</a></li>
<li class="toctree-l3"><a class="reference external" href="https://hyperfast-template.entelecheia.ai/">Hyperfast Template</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/containerization/index.html">Containerization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/docker.html">Docker</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/containerization/containerd.html"><code class="docutils literal notranslate"><span class="pre">containerd</span></code></a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/simple-pipeline/index.html">Simple MLOps Pipeline</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/simple-pipeline/server.html">Server Setup &amp; Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/simple-pipeline/vpn.html">VPN Connectivity</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mlops/llmops/index.html">LLMOps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/llmops/bentoml.html">Introduction to BentoML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mlops/llmops/bentochain.html">Deploy a Voice-Based Chatbot with BentoML, LangChain, and Gradio</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsecon/index.html">Data Science for Economics and Finance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/intro/index.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/intro/introduction.html">Data Science in Economics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/intro/challenges.html">Technical Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/intro/methods.html">Data Analytics Methods</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/cb/index.html">Central Banks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/cb/altdata.html">Alternative Data Sources for Central Banks</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/fomc/index.html">Textual Analysis of FOMC contents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/01_numerical_data.html">Preparing Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/02_textual_data.html">Preparing Textual Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/03_EDA_numericals1.html">EDA on Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/03_EDA_numericals2.html">EDA on Numerical Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/04_training_datasets.html">Create Training Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/05_features.html">Visualizing Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/06_AutoML.html">Checking Baseline with AutoML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/07_predict_sentiments.html">Predicting Sentiments of FOMC Corpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/08_EDA_sentiments1.html">EDA on Sentiments: Correlation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/08_EDA_sentiments2.html">EDA on Sentiment Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/09_visualize_features.html">Visualize Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/10_monetary_shocks.html">Monetary Policy Shocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/fomc/11_AutoML_with_tones.html">Predicting the next decisions with tones</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsecon/esg-ratings/index.html">ESG Ratings</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/prepare_datasets.html">Preparing training datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/improve_datasets.html">Improving classification datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/train_classifiers.html">Training Classifiers for ESG Ratings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/build_news_corpus.html">Building <code class="docutils literal notranslate"><span class="pre">econ_news_kr</span></code> corpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/predict_esg_classes.html">Predicting ESG Categories and Polarities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/cross_validate_datasets.html">Cross validating datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/prepare_datasets_for_labeling.html">Preparing active learning data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsecon/esg-ratings/all_in_one_pipeline.html">Putting them together in a pipeline</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../softeng/index.html">Software Engineering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/intro/index.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/introduction.html">Software Engineering?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/processes.html">Software Processes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/sdlc.html">Software Development Life Cycle (SDLC)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/intro/requirements.html">Requirements Engineering (RE)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/proposal/index.html">Project Proposal</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/proposal/steps.html">Steps in Software Engineering Projects</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/proposal/guidelines.html">Software Engineering Proposal Guideline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/proposal/template.html">Project Proposal Template</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/vcs/index.html">Version Control Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/00_introduction.html">0. Introduction to version control</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/01_solo_work_with_git.html">1. Solo work with git</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/02_fixing_mistakes.html">2. Fixing mistakes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/03_publishing.html">3. Publishing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/04_collaboration.html">4. Collaboration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/05_fork_and_pull.html">5. Fork and Pull</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/06_git_theory.html">6. Git Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/07_branches.html">7. Branches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/08_advanced_git_concepts.html">8. Advanced git concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/09_github_pages.html">9. Publishing from GitHub</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/10_rebasing.html">10. Rebasing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/11_debugging_with_git_bisect.html">11. Debugging With git bisect</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/vcs/12_multiple_remotes.html">12. Working with multiple remotes</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/spm/index.html">Software Process Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/spm/agile.html">Agile Software Development</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../softeng/devops/index.html">DevOps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../softeng/devops/gitops.html">GitOps</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Large Language Models</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../intro/index.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../intro/llms.html">Large Language Models?</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../stack/index.html">LLM Stacks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../stack/infra.html">Generative AI Infrastructure Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../stack/architecture.html">LLM Application Architectures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../stack/app.html">LLM App Ecosystem</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../agents/index.html">AI Agents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../agents/autogen.html">AutoGen</a></li>
<li class="toctree-l3"><a class="reference internal" href="../agents/autoscraper.html">AutoGen AutoScraper Agent</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../finetune/index.html">LLM Fine-tuning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetune/autotrain.html">Fine-Tuning LLMs with Hugging Face AutoTrain</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/index.html">Retrieval Augmented Generation (RAG)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../peft/index.html">Parameter-Efficient Fine-Tuning (PEFT)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../peft/peft-llms.html">PEFT for LLMs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../peft/peft-hf.html">PEFT in HuggingFace Libraries</a></li>
</ul>
</details></li>
<li class="toctree-l2 current active has-children"><a class="current reference internal" href="#">Q-Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="qstar.html">Q-Star (Q*)</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://youngjoon-lee.com">youngjoon-lee.com</a></li>
<li class="toctree-l1"><a class="reference external" href="https://courses.jeju.ai">courses.jeju.ai</a></li>
<li class="toctree-l1"><a class="reference external" href="https://research.jeju.ai">research.jeju.ai</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about/index.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/lectures/llms/q-learning/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Q-Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-large-language-models">Introduction to Large Language Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-and-overview">Definition and Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evolution">Evolution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architectural-insights">Architectural Insights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#capabilities-and-applications">Capabilities and Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ethical-and-technical-challenges">Ethical and Technical Challenges</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning-in-ai">Q-Learning in AI</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamentals-of-q-learning">Fundamentals of Q-Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-q-learning-works">How Q-Learning Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#application-in-ai">Application in AI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-limitations">Challenges and Limitations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advancements-and-variants">Advancements and Variants</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alphago-a-case-study-in-ai-mastery">AlphaGo: A Case Study in AI Mastery</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-alphago">Overview of AlphaGo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-components-of-alphago">Key Components of AlphaGo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-self-play">Training and Self-Play</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#technical-breakthroughs">Technical Breakthroughs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-achievements">Challenges and Achievements</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adapting-alphagos-strategies-to-large-language-models-llms">Adapting AlphaGo’s Strategies to Large Language Models (LLMs)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integrating-alphagos-principles-in-llms">Integrating AlphaGo’s Principles in LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-neural-network-adaptation">Policy Neural Network Adaptation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-neural-network-in-llm-context">Value Neural Network in LLM Context</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-search-techniques">Advanced Search Techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#groundtruth-signal-for-llms">Groundtruth Signal for LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-learning-and-self-improvement">Iterative Learning and Self-Improvement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-adaptation">Challenges in Adaptation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-of-thoughts-and-chain-of-thought-in-large-language-models-llms">Graph of Thoughts and Chain of Thought in Large Language Models (LLMs)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-of-thought-cot">Chain of Thought (CoT)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advancement-to-tree-of-thought-tot">Advancement to Tree of Thought (ToT)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-of-thoughts-got">Graph of Thoughts (GoT)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-with-llms">Integration with LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#potential-and-implications">Potential and Implications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#process-supervised-reward-models-prms-in-large-language-models">Process-Supervised Reward Models (PRMs) in Large Language Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-prms">Overview of PRMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#functioning-of-prms">Functioning of PRMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-in-llms">Implementation in LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-prms">Advantages of PRMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-considerations">Challenges and Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#integrating-q-learning-with-large-language-model-llm-training">Integrating Q-Learning with Large Language Model (LLM) Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conceptual-foundation">Conceptual Foundation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mechanism-and-adaptation">Mechanism and Adaptation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-improvement-loop">Training and Improvement Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-solutions">Challenges and Solutions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-implications">Applications and Implications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-future-prospects-in-advanced-llm-training">Challenges and Future Prospects in Advanced LLM Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#current-challenges">Current Challenges</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#future-prospects">Future Prospects</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-reading">References and Further Reading</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-contents">Next Contents</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="q-learning">
<h1>Q-Learning<a class="headerlink" href="#q-learning" title="Link to this heading">#</a></h1>
<a class="reference internal image-reference" href="../../../_images/q-learning.png"><img alt="../../../_images/q-learning.png" class="align-center" src="../../../_images/q-learning.png" style="width: 100%;" /></a>
<p><strong>Advanced Training Strategies for Large Language Models: A Q-Learning Approach</strong></p>
<section id="introduction-to-large-language-models">
<h2>Introduction to Large Language Models<a class="headerlink" href="#introduction-to-large-language-models" title="Link to this heading">#</a></h2>
<p>LLMs represent a significant leap in AI’s ability to interact with human language, opening up new possibilities across various sectors. However, their deployment must be managed carefully to address ethical, societal, and technical challenges.</p>
<section id="definition-and-overview">
<h3>Definition and Overview<a class="headerlink" href="#definition-and-overview" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Large Language Models (LLMs)</strong>: LLMs are advanced AI systems designed to understand, generate, and interact with human language. They are built using deep learning techniques, particularly neural networks, and are trained on vast amounts of text data.</p></li>
<li><p><strong>Characteristics</strong>: LLMs can perform a wide range of language-related tasks, including translation, summarization, question answering, and creative writing. Their ‘large’ aspect comes from the substantial size of their training datasets and the complexity of their neural network architectures.</p></li>
</ul>
</section>
<section id="evolution">
<h3>Evolution<a class="headerlink" href="#evolution" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Historical Development</strong>: The development of LLMs can be traced back to earlier forms of natural language processing (NLP) models. The evolution saw a shift from rule-based and statistical approaches to machine learning-based methods.</p></li>
<li><p><strong>Breakthroughs</strong>: Significant milestones include the introduction of transformer models like Google’s BERT and OpenAI’s GPT series. These models revolutionized the field with their ability to understand context and generate coherent and contextually relevant text.</p></li>
</ul>
</section>
<section id="architectural-insights">
<h3>Architectural Insights<a class="headerlink" href="#architectural-insights" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Neural Networks</strong>: LLMs primarily use a variant of neural networks called transformers. These are designed to process sequences of data (like text) and are particularly effective at handling long-range dependencies in language.</p></li>
<li><p><strong>Training Process</strong>: Training an LLM involves feeding it large volumes of text so that it learns language patterns, grammar, context, and nuances. The process requires significant computational resources and time.</p></li>
</ul>
</section>
<section id="capabilities-and-applications">
<h3>Capabilities and Applications<a class="headerlink" href="#capabilities-and-applications" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Language Understanding and Generation</strong>: LLMs excel at understanding the context and generating text that is contextually and grammatically coherent.</p></li>
<li><p><strong>Versatility</strong>: They are used in various applications like chatbots, writing assistants, content creation tools, and more.</p></li>
<li><p><strong>Customization and Fine-tuning</strong>: LLMs can be fine-tuned for specific tasks or industries, enhancing their applicability in specialized domains.</p></li>
</ul>
</section>
<section id="ethical-and-technical-challenges">
<h3>Ethical and Technical Challenges<a class="headerlink" href="#ethical-and-technical-challenges" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Bias and Fairness</strong>: Given that they are trained on human-generated text, LLMs can inherit and amplify biases present in the training data.</p></li>
<li><p><strong>Misinformation and Content Generation</strong>: The ability of LLMs to generate realistic text also poses challenges in terms of misinformation and the potential for misuse.</p></li>
<li><p><strong>Computational Requirements</strong>: Training and running LLMs demand substantial computational power, making them resource-intensive.</p></li>
</ul>
</section>
</section>
<section id="q-learning-in-ai">
<h2>Q-Learning in AI<a class="headerlink" href="#q-learning-in-ai" title="Link to this heading">#</a></h2>
<p>Q-learning represents a fundamental approach in reinforcement learning, offering a robust framework for decision-making in AI. Its effectiveness and adaptability have made it a staple in the AI toolkit, although it requires careful tuning and consideration of its limitations.</p>
<section id="fundamentals-of-q-learning">
<h3>Fundamentals of Q-Learning<a class="headerlink" href="#fundamentals-of-q-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Definition</strong>: Q-learning is a type of reinforcement learning (RL) algorithm used in AI to find the best action to take in a given state.</p></li>
<li><p><strong>Mechanism</strong>: It operates by learning a Q-value for each action in each state, which estimates the total reward that can be obtained from that state onward by taking that action.</p></li>
<li><p><strong>Q-Table</strong>: A key component of Q-learning is the Q-table, which stores Q-values for each state-action pair.</p></li>
</ul>
</section>
<section id="how-q-learning-works">
<h3>How Q-Learning Works<a class="headerlink" href="#how-q-learning-works" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Learning Process</strong>: The algorithm updates the Q-values based on the rewards received, using the Bellman equation. This process continues until the values converge, indicating that the model has learned the optimal action for each state.</p></li>
<li><p><strong>Exploration vs. Exploitation</strong>: Q-learning balances between exploring new actions (to discover better rewards) and exploiting known actions that give high rewards.</p></li>
</ul>
</section>
<section id="application-in-ai">
<h3>Application in AI<a class="headerlink" href="#application-in-ai" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Versatility</strong>: Q-learning is used in various domains like robotics, game playing, and autonomous vehicles, where decisions need to be made sequentially.</p></li>
<li><p><strong>Adaptability</strong>: It is particularly useful in environments with clear reward structures but can also adapt to more complex and subtle reward systems.</p></li>
</ul>
</section>
<section id="challenges-and-limitations">
<h3>Challenges and Limitations<a class="headerlink" href="#challenges-and-limitations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>State Space Size</strong>: Q-learning can become impractical in environments with very large state spaces because of the Q-table’s size.</p></li>
<li><p><strong>Convergence Time</strong>: The time it takes for the Q-values to converge can be long, especially in complex environments.</p></li>
<li><p><strong>Reward Dependency</strong>: The efficiency of Q-learning heavily depends on how well the reward system is designed.</p></li>
</ul>
</section>
<section id="advancements-and-variants">
<h3>Advancements and Variants<a class="headerlink" href="#advancements-and-variants" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Deep Q-Networks (DQN)</strong>: Integrating Q-learning with deep neural networks to handle large or continuous state spaces.</p></li>
<li><p><strong>Variants</strong>: Modifications like Double Q-learning and Dueling Q-networks have been developed to address specific challenges and improve performance.</p></li>
</ul>
</section>
</section>
<section id="alphago-a-case-study-in-ai-mastery">
<h2>AlphaGo: A Case Study in AI Mastery<a class="headerlink" href="#alphago-a-case-study-in-ai-mastery" title="Link to this heading">#</a></h2>
<p>AlphaGo’s success marked a milestone in AI, showing the potential of combining neural networks with advanced search techniques. It paved the way for further research in AI, inspiring new approaches and applications in various domains.</p>
<section id="overview-of-alphago">
<h3>Overview of AlphaGo<a class="headerlink" href="#overview-of-alphago" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Introduction</strong>: AlphaGo is a computer program developed by DeepMind that marked a significant achievement in AI by defeating a world champion Go player.</p></li>
<li><p><strong>Significance</strong>: Go is a complex board game with more possible positions than atoms in the universe, making it a formidable challenge for AI.</p></li>
</ul>
</section>
<section id="key-components-of-alphago">
<h3>Key Components of AlphaGo<a class="headerlink" href="#key-components-of-alphago" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Policy Neural Network (Policy NN)</strong></p>
<ul class="simple">
<li><p><strong>Function</strong>: Selects the next move during the Go game.</p></li>
<li><p><strong>Training</strong>: Initially trained on expert human games, then improved through self-play.</p></li>
<li><p><strong>Role</strong>: Narrows down the search space to feasible and promising moves.</p></li>
</ul>
</li>
<li><p><strong>Value Neural Network (Value NN)</strong></p>
<ul class="simple">
<li><p><strong>Purpose</strong>: Evaluates Go board positions and predicts the game’s winner.</p></li>
<li><p><strong>Training</strong>: Refined through self-play, learning to assess complex game positions.</p></li>
</ul>
</li>
<li><p><strong>Monte Carlo Tree Search (MCTS)</strong></p>
<ul class="simple">
<li><p><strong>Mechanism</strong>: Simulates game sequences, guided by Policy NN.</p></li>
<li><p><strong>Strategy</strong>: Balances exploration of new moves and exploitation of known good moves.</p></li>
<li><p><strong>Function</strong>: Selects the best move by aggregating results of many simulated games.</p></li>
</ul>
</li>
<li><p><strong>Groundtruth Signal</strong></p>
<ul class="simple">
<li><p><strong>Nature</strong>: Simple binary win/loss outcome in Go.</p></li>
<li><p><strong>Role</strong>: Provides a clear feedback mechanism for training.</p></li>
</ul>
</li>
</ol>
</section>
<section id="training-and-self-play">
<h3>Training and Self-Play<a class="headerlink" href="#training-and-self-play" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Bootstrapping</strong>: AlphaGo improved through playing against itself, allowing both Policy NN and Value NN to evolve.</p></li>
<li><p><strong>Iterative Learning</strong>: Continuous self-play led to an escalating level of play, with each version stronger than the last.</p></li>
</ul>
</section>
<section id="technical-breakthroughs">
<h3>Technical Breakthroughs<a class="headerlink" href="#technical-breakthroughs" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Handling Complexity</strong>: Demonstrated how AI could handle a high level of complexity and uncertainty.</p></li>
<li><p><strong>Integration of Techniques</strong>: Combined deep learning with Monte Carlo tree search effectively.</p></li>
</ul>
</section>
<section id="challenges-and-achievements">
<h3>Challenges and Achievements<a class="headerlink" href="#challenges-and-achievements" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Historic Matches</strong>: Defeated world champion Lee Sedol in a 4-1 victory, demonstrating super-human performance.</p></li>
<li><p><strong>Limitations</strong>: Despite its success, AlphaGo is specialized for Go and not directly applicable to broader AI challenges.</p></li>
</ul>
</section>
</section>
<section id="adapting-alphagos-strategies-to-large-language-models-llms">
<h2>Adapting AlphaGo’s Strategies to Large Language Models (LLMs)<a class="headerlink" href="#adapting-alphagos-strategies-to-large-language-models-llms" title="Link to this heading">#</a></h2>
<p>Integrating AlphaGo’s strategies into LLMs holds significant potential for advancing the field of natural language processing, offering a pathway to more intelligent, adaptive, and capable language models.</p>
<section id="integrating-alphagos-principles-in-llms">
<h3>Integrating AlphaGo’s Principles in LLMs<a class="headerlink" href="#integrating-alphagos-principles-in-llms" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Context</strong>: AlphaGo’s strategies revolutionized board game AI. Adapting these principles to LLMs involves leveraging the underlying concepts of learning and search to enhance language processing capabilities.</p></li>
</ul>
</section>
<section id="policy-neural-network-adaptation">
<h3>Policy Neural Network Adaptation<a class="headerlink" href="#policy-neural-network-adaptation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Role in LLMs</strong>: The Policy NN equivalent in LLMs selects the most appropriate language constructs, akin to choosing moves in Go.</p></li>
<li><p><strong>Training and Adaptation</strong>: It involves training on diverse language data and fine-tuning through tasks like question answering or text generation.</p></li>
</ul>
</section>
<section id="value-neural-network-in-llm-context">
<h3>Value Neural Network in LLM Context<a class="headerlink" href="#value-neural-network-in-llm-context" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Purpose</strong>: Evaluates the ‘value’ or appropriateness of generated text or intermediate steps in problem-solving.</p></li>
<li><p><strong>Enhancement</strong>: Incorporating feedback from diverse language tasks to refine the model’s understanding and generation capabilities.</p></li>
</ul>
</section>
<section id="advanced-search-techniques">
<h3>Advanced Search Techniques<a class="headerlink" href="#advanced-search-techniques" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Search in LLMs</strong>: Adapting the MCTS concept, LLMs can use advanced search algorithms to explore various language paths, enhancing creativity and problem-solving abilities.</p></li>
<li><p><strong>Application</strong>: Useful in tasks like constructing complex narratives or solving intricate problems where multiple reasoning paths can be explored.</p></li>
</ul>
</section>
<section id="groundtruth-signal-for-llms">
<h3>Groundtruth Signal for LLMs<a class="headerlink" href="#groundtruth-signal-for-llms" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Nature</strong>: Unlike the binary win/loss in Go, LLMs deal with more nuanced and subjective feedback in language tasks.</p></li>
<li><p><strong>Implementation</strong>: Involves using a combination of human feedback, automated evaluation metrics, and benchmarks for training and refining models.</p></li>
</ul>
</section>
<section id="iterative-learning-and-self-improvement">
<h3>Iterative Learning and Self-Improvement<a class="headerlink" href="#iterative-learning-and-self-improvement" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Self-Play Equivalent</strong>: In LLMs, this involves continuous self-evaluation and refinement on a variety of language tasks.</p></li>
<li><p><strong>Feedback Loop</strong>: The iterative process where the model learns from its outputs and the received evaluations, akin to AlphaGo’s self-play improvements.</p></li>
</ul>
</section>
<section id="challenges-in-adaptation">
<h3>Challenges in Adaptation<a class="headerlink" href="#challenges-in-adaptation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Complexity of Language</strong>: Unlike Go’s structured play, language is more nuanced and subjective, posing challenges in evaluation and improvement.</p></li>
<li><p><strong>Ethical Considerations</strong>: Ensuring the model’s outputs align with ethical guidelines and do not perpetuate biases.</p></li>
</ul>
</section>
</section>
<section id="graph-of-thoughts-and-chain-of-thought-in-large-language-models-llms">
<h2>Graph of Thoughts and Chain of Thought in Large Language Models (LLMs)<a class="headerlink" href="#graph-of-thoughts-and-chain-of-thought-in-large-language-models-llms" title="Link to this heading">#</a></h2>
<p>CoT, ToT, and GoT represent significant steps forward in making LLMs more capable and understandable in their problem-solving approaches.</p>
<section id="chain-of-thought-cot">
<h3>Chain of Thought (CoT)<a class="headerlink" href="#chain-of-thought-cot" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Introduction</strong>: Chain of Thought is a concept in LLMs that involves explicitly generating intermediate reasoning steps while solving problems.</p></li>
<li><p><strong>Functionality</strong>: CoT enhances the ability of LLMs to tackle complex tasks, such as mathematics or logic puzzles, by breaking down the solution process into understandable steps.</p></li>
<li><p><strong>Benefits</strong>: Improves transparency in how LLMs arrive at conclusions and aids in error analysis.</p></li>
</ul>
</section>
<section id="advancement-to-tree-of-thought-tot">
<h3>Advancement to Tree of Thought (ToT)<a class="headerlink" href="#advancement-to-tree-of-thought-tot" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Evolution</strong>: ToT extends CoT by introducing branching paths in the reasoning process.</p></li>
<li><p><strong>Mechanism</strong>: It allows LLMs to explore multiple reasoning pathways and backtrack if a certain path proves unfruitful, mimicking human problem-solving strategies.</p></li>
</ul>
</section>
<section id="graph-of-thoughts-got">
<h3>Graph of Thoughts (GoT)<a class="headerlink" href="#graph-of-thoughts-got" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Concept</strong>: GoT further expands on CoT and ToT by representing the reasoning process as a complex graph.</p></li>
<li><p><strong>Structure</strong>: In GoT, each node represents a reasoning step, and edges denote dependencies or relationships between these steps.</p></li>
<li><p><strong>Capabilities</strong>: This structure enables the merging of different reasoning pathways, allowing for more sophisticated problem-solving techniques.</p></li>
</ul>
</section>
<section id="integration-with-llms">
<h3>Integration with LLMs<a class="headerlink" href="#integration-with-llms" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Application</strong>: Implementing GoT in LLMs involves training models to generate and navigate these complex reasoning graphs.</p></li>
<li><p><strong>Challenges</strong>: Requires sophisticated training and fine-tuning to effectively leverage this approach in various language tasks.</p></li>
</ul>
</section>
<section id="potential-and-implications">
<h3>Potential and Implications<a class="headerlink" href="#potential-and-implications" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Enhanced Problem Solving</strong>: GoT can potentially elevate LLMs’ ability to handle more complex and nuanced tasks.</p></li>
<li><p><strong>Transparency and Interpretability</strong>: These advanced reasoning frameworks can make LLM outputs more interpretable and transparent.</p></li>
</ul>
</section>
</section>
<section id="process-supervised-reward-models-prms-in-large-language-models">
<h2>Process-Supervised Reward Models (PRMs) in Large Language Models<a class="headerlink" href="#process-supervised-reward-models-prms-in-large-language-models" title="Link to this heading">#</a></h2>
<p>PRMs represent a significant advancement in LLM training, offering a pathway to more intelligent, accurate, and transparent language models capable of complex problem-solving.</p>
<section id="overview-of-prms">
<h3>Overview of PRMs<a class="headerlink" href="#overview-of-prms" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Definition</strong>: Process-Supervised Reward Models (PRMs) are a reinforcement learning technique used in training LLMs, focusing on providing feedback for each intermediate step in a task.</p></li>
<li><p><strong>Contrast with ORMs</strong>: Unlike Outcome-supervised Reward Models (ORMs) that assess the final output, PRMs evaluate the correctness and logic of each step leading to the outcome.</p></li>
</ul>
</section>
<section id="functioning-of-prms">
<h3>Functioning of PRMs<a class="headerlink" href="#functioning-of-prms" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Detailed Feedback</strong>: PRMs provide step-by-step feedback, allowing models to understand precisely where errors occur in the reasoning process.</p></li>
<li><p><strong>Training Efficiency</strong>: This granularity improves the training efficiency as models learn not just what is wrong, but why it’s wrong.</p></li>
</ul>
</section>
<section id="implementation-in-llms">
<h3>Implementation in LLMs<a class="headerlink" href="#implementation-in-llms" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Application</strong>: PRMs are particularly useful in complex tasks involving multi-step reasoning, such as mathematical problem solving or logical reasoning.</p></li>
<li><p><strong>Integration</strong>: They require careful integration into the training pipeline to ensure that feedback is accurate and constructive.</p></li>
</ul>
</section>
<section id="advantages-of-prms">
<h3>Advantages of PRMs<a class="headerlink" href="#advantages-of-prms" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Improved Learning</strong>: By pinpointing errors, PRMs facilitate quicker and more effective learning.</p></li>
<li><p><strong>Greater Transparency</strong>: They enhance the transparency of how LLMs reach conclusions, making them more interpretable.</p></li>
</ul>
</section>
<section id="challenges-and-considerations">
<h3>Challenges and Considerations<a class="headerlink" href="#challenges-and-considerations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Complexity in Design</strong>: Designing effective PRMs can be challenging as it requires a nuanced understanding of the task at hand.</p></li>
<li><p><strong>Balancing Feedback</strong>: Ensuring that feedback is neither too sparse nor overwhelmingly detailed is crucial for optimal learning.</p></li>
</ul>
</section>
</section>
<section id="integrating-q-learning-with-large-language-model-llm-training">
<h2>Integrating Q-Learning with Large Language Model (LLM) Training<a class="headerlink" href="#integrating-q-learning-with-large-language-model-llm-training" title="Link to this heading">#</a></h2>
<p>Integrating Q-Learning into LLM training represents a significant step towards creating more sophisticated, adaptable, and intelligent language models.</p>
<section id="conceptual-foundation">
<h3>Conceptual Foundation<a class="headerlink" href="#conceptual-foundation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Q-Learning for LLMs</strong>: Adapting Q-learning for LLMs to enhance reasoning and decision-making, similar to AlphaGo’s approach in Go.</p></li>
<li><p><strong>Integration</strong>: Merging reinforcement learning principles with advanced language understanding, akin to combining AlphaGo’s Policy and Value NNs with LLMs’ capabilities.</p></li>
</ul>
</section>
<section id="mechanism-and-adaptation">
<h3>Mechanism and Adaptation<a class="headerlink" href="#mechanism-and-adaptation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Policy and Value Networks</strong>: Adaptation of AlphaGo’s Policy NN for selecting optimal language patterns and Value NN for evaluating reasoning steps in LLMs.</p></li>
<li><p><strong>Search Strategies</strong>: Incorporating AlphaGo’s Monte Carlo Tree Search (MCTS) principle, adapted to explore complex language and reasoning spaces in LLMs.</p></li>
</ul>
</section>
<section id="training-and-improvement-loop">
<h3>Training and Improvement Loop<a class="headerlink" href="#training-and-improvement-loop" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Iterative Enhancement</strong>: Leveraging the self-improvement loop of AlphaGo, where LLMs iteratively refine language generation and reasoning through continuous feedback.</p></li>
<li><p><strong>Feedback Mechanism</strong>: Detailed feedback from each step, mirroring AlphaGo’s integration of game outcomes and MCTS results to guide learning.</p></li>
</ul>
</section>
<section id="challenges-and-solutions">
<h3>Challenges and Solutions<a class="headerlink" href="#challenges-and-solutions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Complex Decision Space in Language</strong>: Addressing the nuanced decision-making in LLMs, inspired by AlphaGo’s handling of Go’s complexity.</p></li>
<li><p><strong>Effective Reward Shaping</strong>: Designing rewards that reflect language output quality, drawing parallels to AlphaGo’s groundtruth signal based on game outcomes.</p></li>
</ul>
</section>
<section id="applications-and-implications">
<h3>Applications and Implications<a class="headerlink" href="#applications-and-implications" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Enhanced Language Models</strong>: Aim to create LLMs with superior reasoning, analogous to AlphaGo’s mastery in Go.</p></li>
<li><p><strong>Broader Applicability</strong>: Adapting AlphaGo’s strategies to enable LLMs to tackle diverse, complex tasks, including creative and analytical challenges.</p></li>
</ul>
</section>
</section>
<section id="challenges-and-future-prospects-in-advanced-llm-training">
<h2>Challenges and Future Prospects in Advanced LLM Training<a class="headerlink" href="#challenges-and-future-prospects-in-advanced-llm-training" title="Link to this heading">#</a></h2>
<p>While there are significant challenges in advancing LLM training, the field also presents vast opportunities for growth and improvement. The future of LLMs lies in balancing these challenges with innovative solutions to unlock their full potential.</p>
<section id="current-challenges">
<h3>Current Challenges<a class="headerlink" href="#current-challenges" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Computational Resources</strong>: Advanced LLM training, especially with techniques like Q-Learning, requires immense computational power, making it resource-intensive.</p></li>
<li><p><strong>Data Quality and Bias</strong>: Ensuring the training data is diverse and unbiased remains a challenge, as LLMs can perpetuate existing biases in the data.</p></li>
<li><p><strong>Ethical Considerations</strong>: The potential misuse of advanced LLMs in generating misinformation or harmful content is a significant concern.</p></li>
<li><p><strong>Model Interpretability</strong>: As LLMs become more complex, ensuring their outputs are interpretable and explainable becomes increasingly challenging.</p></li>
</ol>
</section>
<section id="future-prospects">
<h3>Future Prospects<a class="headerlink" href="#future-prospects" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Energy-Efficient Models</strong>: Research into more efficient models and training methods to reduce computational demands.</p></li>
<li><p><strong>Addressing Biases</strong>: Continued efforts in developing techniques to identify and mitigate biases within LLMs.</p></li>
<li><p><strong>Robust Ethical Frameworks</strong>: Establishing stronger ethical guidelines and frameworks for the use and development of LLMs.</p></li>
<li><p><strong>Enhanced Interpretability</strong>: Innovations in making LLMs more transparent and understandable in their decision-making processes.</p></li>
</ol>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In summary, the development and refinement of Large Language Models (LLMs) through advanced training strategies, such as the incorporation of Q-Learning, signify a major leap in AI capabilities. These models, inspired by the mechanisms of AlphaGo and enhanced with sophisticated techniques like Process-Supervised Reward Models and Graph of Thoughts, are poised to revolutionize how AI understands and interacts with human language. However, this journey is not without its challenges, including ethical considerations, computational demands, and the need for bias mitigation. Looking forward, the potential of LLMs is vast, offering transformative possibilities in numerous domains, provided these challenges are thoughtfully addressed.</p>
</section>
<section id="references-and-further-reading">
<h2>References and Further Reading<a class="headerlink" href="#references-and-further-reading" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Silver, D., et al. (2016). “Mastering the game of Go with deep neural networks and tree search.” Nature, 529(7587), 484-489.</p></li>
<li><p>Brown, T. B., et al. (2020). “Language Models are Few-Shot Learners.” arXiv preprint arXiv:2005.14165.</p></li>
<li><p>Sutton, R. S., &amp; Barto, A. G. (2018). “Reinforcement Learning: An Introduction.” MIT press.</p></li>
<li><p>Vaswani, A., et al. (2017). “Attention is All You Need.” In Advances in Neural Information Processing Systems.</p></li>
<li><p>Bengio, Y., Lecun, Y., &amp; Hinton, G. (2021). “Deep Learning for AI.” Communications of the ACM, 64(7), 58-65.</p></li>
<li><p>DeepMind. AlphaGo. [Online]. Available: <a class="reference external" href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">https://deepmind.com/research/case-studies/alphago-the-story-so-far</a></p></li>
<li><p>OpenAI. (2023). “ChatGPT: Optimizing Language Models for Dialogue.” [Online]. Available: <a class="reference external" href="https://openai.com/blog/chatgpt">https://openai.com/blog/chatgpt</a></p></li>
<li><p>Wei, J., et al. (2022). “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” arXiv preprint arXiv:2201.11903.</p></li>
<li><p>Wu, E., et al. (2023). “Let’s Verify Step by Step.” arXiv preprint arXiv:2305.20050. [Online]. Available: <a class="reference external" href="https://arxiv.org/abs/2305.20050">https://arxiv.org/abs/2305.20050</a></p></li>
<li><p>Besta, M., et al. (2023). “Graph of Thoughts: Solving Elaborate Problems with Large Language Models.” arXiv preprint arXiv:2308.09687. [Online]. Available: <a class="reference external" href="https://arxiv.org/abs/2308.09687">https://arxiv.org/abs/2308.09687</a></p></li>
</ol>
</section>
<section id="next-contents">
<h2>Next Contents<a class="headerlink" href="#next-contents" title="Link to this heading">#</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="qstar.html">Q-Star (Q*)</a></li>
</ul>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/lecture",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/llms/q-learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../peft/peft-hf.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">PEFT in HuggingFace Libraries</p>
      </div>
    </a>
    <a class="right-next"
       href="qstar.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Q-Star (Q*)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-large-language-models">Introduction to Large Language Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-and-overview">Definition and Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evolution">Evolution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architectural-insights">Architectural Insights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#capabilities-and-applications">Capabilities and Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ethical-and-technical-challenges">Ethical and Technical Challenges</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning-in-ai">Q-Learning in AI</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamentals-of-q-learning">Fundamentals of Q-Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-q-learning-works">How Q-Learning Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#application-in-ai">Application in AI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-limitations">Challenges and Limitations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advancements-and-variants">Advancements and Variants</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alphago-a-case-study-in-ai-mastery">AlphaGo: A Case Study in AI Mastery</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-alphago">Overview of AlphaGo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-components-of-alphago">Key Components of AlphaGo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-self-play">Training and Self-Play</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#technical-breakthroughs">Technical Breakthroughs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-achievements">Challenges and Achievements</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adapting-alphagos-strategies-to-large-language-models-llms">Adapting AlphaGo’s Strategies to Large Language Models (LLMs)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integrating-alphagos-principles-in-llms">Integrating AlphaGo’s Principles in LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-neural-network-adaptation">Policy Neural Network Adaptation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-neural-network-in-llm-context">Value Neural Network in LLM Context</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-search-techniques">Advanced Search Techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#groundtruth-signal-for-llms">Groundtruth Signal for LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-learning-and-self-improvement">Iterative Learning and Self-Improvement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-adaptation">Challenges in Adaptation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-of-thoughts-and-chain-of-thought-in-large-language-models-llms">Graph of Thoughts and Chain of Thought in Large Language Models (LLMs)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-of-thought-cot">Chain of Thought (CoT)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advancement-to-tree-of-thought-tot">Advancement to Tree of Thought (ToT)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-of-thoughts-got">Graph of Thoughts (GoT)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-with-llms">Integration with LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#potential-and-implications">Potential and Implications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#process-supervised-reward-models-prms-in-large-language-models">Process-Supervised Reward Models (PRMs) in Large Language Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-prms">Overview of PRMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#functioning-of-prms">Functioning of PRMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-in-llms">Implementation in LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-prms">Advantages of PRMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-considerations">Challenges and Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#integrating-q-learning-with-large-language-model-llm-training">Integrating Q-Learning with Large Language Model (LLM) Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conceptual-foundation">Conceptual Foundation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mechanism-and-adaptation">Mechanism and Adaptation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-improvement-loop">Training and Improvement Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-solutions">Challenges and Solutions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-implications">Applications and Implications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-future-prospects-in-advanced-llm-training">Challenges and Future Prospects in Advanced LLM Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#current-challenges">Current Challenges</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#future-prospects">Future Prospects</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-reading">References and Further Reading</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-contents">Next Contents</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://youngjoon-lee.com" target="_blank">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>