{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfc871e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Zero Shot, Prompt, and Search Strategies\n",
    "\n",
    "```{image} ../figs/aiart/entelecheia_Zero_Shot.png\n",
    ":alt: Zero Shot, Prompt, and Search Strategies\n",
    ":class: bg-primary mb-1\n",
    ":width: 50%\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72c3e881",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Zero Shot and Few Shot Learners\n",
    "\n",
    "```{image} ../figs/aiart/entelecheia_a_robot.png\n",
    ":alt: Zero Shot and Few Shot Learners\n",
    ":class: bg-primary mb-1\n",
    ":width: 90%\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Zero-shot and few-shot learners are machine learning techniques that allow models to generalize and adapt to new tasks with little or no training data. These approaches are particularly useful in situations where obtaining labeled data is difficult or expensive. They are inspired by the human ability to learn new concepts and skills quickly, even from a few examples.\n",
    "\n",
    "Zero-shot learning (ZSL) refers to the ability of a model to perform a task without having seen any examples from that specific task during training. It relies on the model's capacity to generalize from related tasks or to leverage auxiliary information such as relationships between classes, attributes, or semantic information. For example, a zero-shot image classification model might recognize a never-seen-before object by leveraging its understanding of similar objects or associated attributes.\n",
    "\n",
    "Few-shot learning (FSL), on the other hand, involves training a model to perform a task with only a small number of examples (typically less than 10) from the target task. This is in contrast to traditional machine learning methods that often require large amounts of labeled data. Few-shot learning aims to quickly adapt to new tasks by leveraging prior knowledge and transfer learning. Meta-learning and memory-augmented neural networks are common approaches to few-shot learning, focusing on learning a model that can adapt rapidly to new tasks based on limited data.\n",
    "\n",
    "Both zero-shot and few-shot learners are essential components in the pursuit of artificial general intelligence (AGI), as they enable models to learn efficiently and generalize effectively across various tasks and domains with minimal data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6d22319",
   "metadata": {},
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "Prompt engineering is the process of designing and refining inputs (prompts) for language models like GPT-3 or GPT-4 to obtain better, more accurate, and contextually relevant outputs. Since large-scale language models are trained to generate text based on the input they receive, crafting effective prompts can significantly influence the quality of the generated responses.\n",
    "\n",
    "The main goal of prompt engineering is to maximize the usefulness and relevance of a language model's output by considering factors such as clarity, specificity, context, and constraints. It often involves an iterative process of experimentation and fine-tuning to arrive at the optimal prompt. Some strategies used in prompt engineering include:\n",
    "\n",
    "- Clarity: Ensure that the prompt is clear and unambiguous, which helps the language model understand the question or context better.\n",
    "- Specificity: Make the prompt more specific by including relevant details or asking for specific information. This can help narrow down the potential responses and avoid generic answers.\n",
    "- Context: Provide sufficient context to guide the model in generating responses that are relevant to the given situation or domain.\n",
    "- Constraints: Apply constraints on the response format or content, such as specifying the desired answer type (e.g., a list or a single word) or limiting the length of the response.\n",
    "- Redundancy: Ask the same question in multiple ways or incorporate different perspectives to increase the likelihood of obtaining accurate and comprehensive answers.\n",
    "- Instructiveness: Use explicit instructions or guiding questions to direct the model's attention to the relevant aspects of the problem or task.\n",
    "\n",
    "Prompt engineering is an essential skill in working with language models as it helps to bridge the gap between the model's training data and the desired output for specific use cases. By carefully crafting and refining prompts, users can improve the reliability and usefulness of generated responses, making the models more effective across a wide range of applications.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64a3564e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Prompting on LLMs\n",
    "\n",
    "Large Language Models (LLMs) like GPT-3 or GPT-4 can be used for various tasks, including zero-shot, one-shot, and few-shot learning. Here are some examples using text prompts for each case:\n",
    "\n",
    "### Zero-shot learning:\n",
    "\n",
    "Task: Sentiment analysis (positive or negative) for a movie review\n",
    "\n",
    "Prompt:\n",
    "\n",
    "```\n",
    "Determine the sentiment of the following movie review: \"I absolutely loved the movie! The storyline was captivating, and the acting was superb. I can't wait to watch it again!\"\n",
    "```\n",
    "\n",
    "Since the LLM has been pre-trained on a diverse range of texts, it should be able to classify the sentiment of the review without any additional examples.\n",
    "\n",
    "### One-shot learning:\n",
    "\n",
    "Task: Animal classification based on a brief description\n",
    "\n",
    "Prompt:\n",
    "\n",
    "```\n",
    "Based on the given description, classify the animal:\n",
    "Example: \"This animal has a long neck and long legs. It mainly eats leaves from trees.\": Giraffe\n",
    "\n",
    "Description: \"This small creature has a bushy tail, sharp claws, and climbs trees to collect nuts.\"\n",
    "```\n",
    "\n",
    "By providing an example in the prompt, the LLM can use this context to generate an appropriate classification for the given description.\n",
    "\n",
    "### Few-shot learning:\n",
    "\n",
    "Task: Convert a sentence from active to passive voice\n",
    "\n",
    "Prompt:\n",
    "\n",
    "```\n",
    "Transform the following sentences from active to passive voice:\n",
    "Example 1: \"John painted the house.\" -> \"The house was painted by John.\"\n",
    "Example 2: \"She baked a cake.\" -> \"A cake was baked by her.\"\n",
    "\n",
    "Sentence: \"The cat chased the mouse.\"\n",
    "```\n",
    "\n",
    "By providing multiple examples, the LLM can better understand the desired transformation and apply it to the input sentence.\n",
    "\n",
    "In each case, the prompt is designed to guide the LLM to perform the desired task with varying amounts of examples. The specific syntax for providing these prompts to an LLM like GPT-3 or GPT-4 depends on the API or library you are using, but the core idea remains the same: designing effective prompts to maximize the usefulness and relevance of the generated outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b604eb",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# If you run this notebook in Colab, set Hardware accelerator to GPU.\n",
    "# Then, install transformers\n",
    "%pip install -U transformers tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a89f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjlee/.cache/pypoetry/virtualenvs/lecture-_dERj_9R-py3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-30 00:19:57.858469: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.63G/1.63G [00:23<00:00, 68.1MB/s]\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.0/26.0 [00:00<00:00, 10.3kB/s]\n",
      "Downloading (â€¦)olve/main/vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:01<00:00, 874kB/s]\n",
      "Downloading (â€¦)olve/main/merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 553kB/s]\n",
      "Downloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:01<00:00, 1.10MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot Learning with Hugging Face's Transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "review = \"I absolutely loved the movie! The storyline was captivating, and the acting was superb. I can't wait to watch it again!\"\n",
    "\n",
    "# Initialize the zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Classify sentiment using the pipeline\n",
    "result = classifier(review, candidate_labels=[\"positive\", \"negative\"])\n",
    "print(result[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5501fdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the following geography-related question:\n",
      "Question: \"What is the capital city of France?\": \n",
      "Answer: Paris\n",
      "\n",
      "Question: \"What is the highest mountain in the world?\"\n",
      "\n",
      "Answer: Mount Everest\n",
      "\n",
      "Question\n"
     ]
    }
   ],
   "source": [
    "# One-shot Learning\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "text = \"\"\"\n",
    "Answer the following geography-related question:\n",
    "Question: \"What is the capital city of France?\": \n",
    "Answer: Paris\n",
    "\n",
    "Question: \"What is the highest mountain in the world?\"\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Generate a response using the model\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "response = tokenizer.decode(output[0])\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d4314f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transform the following sentences from active to passive voice:\n",
      "Example 1: \"John painted the house.\" -> \"The house was painted by John.\"\n",
      "Example 2: \"She baked a cake.\" -> \"A cake was baked by her.\"\n",
      "\n",
      "Example 3: \"The cat chased the mouse.\"\n",
      "\n",
      "Example 4: \"The cat was a cat.\" -> \"The cat was a cat.\"\n",
      "\n",
      "Example 5: \"The cat was a cat.\" -> \"The cat was a cat\n"
     ]
    }
   ],
   "source": [
    "# Few-shot Learning\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "text = \"\"\"\n",
    "Transform the following sentences from active to passive voice:\n",
    "Example 1: \"John painted the house.\" -> \"The house was painted by John.\"\n",
    "Example 2: \"She baked a cake.\" -> \"A cake was baked by her.\"\n",
    "\n",
    "Example 3: \"The cat chased the mouse.\"\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Generate a response using the model\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=100)\n",
    "response = tokenizer.decode(output[0])\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75a3c954",
   "metadata": {},
   "source": [
    "## Zero-Shot Reasoners and Chain-of-Thought Prompting\n",
    "\n",
    "The [University of Tokyo and Google Brain team](https://arxiv.org/abs/2205.11916) discovered that large language models (LLMs) possess inherent zero-shot abilities in high-level cognitive tasks. These abilities can be extracted using a method called Chain-of-Thought (CoT) prompting.\n",
    "\n",
    "Further research by the [Google Brain team](https://arxiv.org/abs/2201.11903) delved into CoT prompting and found that generating a series of intermediate reasoning steps, or a \"chain-of-thought,\" significantly improves LLMs' complex reasoning capabilities. Experiments conducted on three LLMs showed that CoT prompting enhances performance in arithmetic, common sense, and symbolic reasoning tasks.\n",
    "\n",
    "Here's an example to illustrate CoT prompting:\n",
    "\n",
    "- Q: Jane has 7 books on her shelf. She borrowed 4 books from the library. How many books does she have now?\n",
    "\n",
    "- A: Jane had 7 books initially. She borrowed 4 books from the library. So, 7 + 4 = 11. The answer is 11.\n",
    "\n",
    "And another example:\n",
    "\n",
    "- Q: A shop had 15 umbrellas. 8 umbrellas were sold, and they restocked 5 more. How many umbrellas do they have now?\n",
    "\n",
    "- A: The shop had 15 umbrellas originally. They sold 8 umbrellas, leaving 15 - 8 = 7. They restocked 5 more umbrellas, so they have 7 + 5 = 12. The answer is 12.\n",
    "\n",
    "\n",
    "In summary, chain-of-thought reasoning enables models to break down complex problems into smaller, manageable steps that can be solved individually. The language-based nature of CoT prompting makes it applicable to any task that can be solved through language. Empirical experiments have shown that CoT prompting improves performance across various reasoning tasks, and successful chain-of-thought reasoning emerges as models scale up."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb51ed20",
   "metadata": {},
   "source": [
    "Here's a Python example using Hugging Face Transformers to demonstrate Zero-Shot Reasoners and Chain-of-Thought Prompting with the GPT-2 model:\n",
    "\n",
    "**Import libraries and prepare the model**\n",
    "\n",
    "The following code imports necessary libraries, TensorFlow and Hugging Face Transformers, to work with the GPT-2 model. It initializes a GPT-2 tokenizer and loads the pre-trained GPT-2 model from Hugging Face's model hub. The EOS token is set as the PAD token to avoid warnings during tokenization and padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f045e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out the total number of books Jane has now, we can first count the number of books on her shelf, which is 7. Then we count the number of books she borrowed from the library, which is 4. Now, we add these two quantities together. What is 7 + 4? Well, it's the number of books she borrowed from the library. So, if you have a library of about 500 books, you have about 7 books on your shelf. So, if you have\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def generate_chain_of_thought(prompt, model, tokenizer, max_length=100):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "    generated_text = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Example problem\n",
    "problem = \"Jane has 7 books on her shelf. She borrowed 4 books from the library. How many books does she have now?\"\n",
    "\n",
    "# Chain-of-Thought Prompt\n",
    "cot_prompt = \"To find out the total number of books Jane has now, we can first count the number of books on her shelf, which is 7. Then we count the number of books she borrowed from the library, which is 4. Now, we add these two quantities together. What is 7 + 4?\"\n",
    "\n",
    "# Generate the answer\n",
    "answer = generate_chain_of_thought(cot_prompt, model, tokenizer)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51691090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to install the OpenAI Python library by running the following command:\n",
    "# pip install openai\n",
    "import openai\n",
    "import re\n",
    "\n",
    "openai.api_key = \"API_KEY\"\n",
    "\n",
    "def generate_chain_of_thought(prompt, model_engine=\"text-davinci-003\", max_tokens=100):\n",
    "    response = openai.Completion.create(\n",
    "        engine=model_engine,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Example problem\n",
    "problem = \"Jane has 7 books on her shelf. She borrowed 4 books from the library. How many books does she have now?\"\n",
    "\n",
    "# Chain-of-Thought Prompt\n",
    "cot_prompt = f\"Jane had 7 books initially. She borrowed 4 books from the library. What is the total number of books Jane has now? 7 + 4 =\"\n",
    "\n",
    "# Generate the answer\n",
    "answer_text = generate_chain_of_thought(cot_prompt)\n",
    "answer = re.findall(r\"\\d+\", answer_text)[0] if re.findall(r\"\\d+\", answer_text) else \"unknown\"\n",
    "print(answer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e21e5692",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Decoding and Search Strategies\n",
    "\n",
    "In recent years, there has been a surge of interest in open-ended language generation due to the development of large language models (LLMs) like GPT-2, XLNet, OpenAI-GPT, CTRL, TransfoXL, XLM, BART, T5, GPT-3, and BLOOM. These models have shown promising results in various generation tasks such as open-ended dialogue, summarization, and story generation. Improved decoding methods have played a significant role in the success of these models.\n",
    "\n",
    "Auto-regressive language generation assumes that the text being generated can be broken down into a sequence of subparts. Each part depends on the previous parts, allowing an auto-regressive decoder to generate text one token at a time based on its predecessors.\n",
    "\n",
    "The probability of generating a word sequence $w_{1:ð‘‡}$ given an initial context word sequence $W_0$ can be expressed as:\n",
    "\n",
    "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with }  w_{1: 0} = \\emptyset, $$\n",
    "\n",
    "Here, $W_0$ is the initial context word sequence. The length ð‘‡ of the word sequence is determined on-the-fly, which means it is decided as the sequence is generated. The sequence generation typically stops when an End-Of-Sequence (EOS) token is generated from the probability distribution $ð‘ƒ(w_t|w_{1:ð‘¡âˆ’1},W_0)$.\n",
    "\n",
    "This auto-regressive approach allows LLMs to generate coherent and contextually relevant text based on the initial context ð‘Š0. Different decoding strategies, such as greedy search, beam search, and sampling methods like top-k sampling, have been used to improve the generation quality and diversity.\n",
    "\n",
    "For example, consider a language model generating a story based on the context \"Once upon a time, in a small village\":\n",
    "1. Greedy search would select the word with the highest probability at each step, potentially leading to repetitive and less diverse text.\n",
    "2. Beam search would maintain a fixed number of partial sequences (the beam width) and extend them, selecting the most probable overall sequence. This can improve diversity but may still suffer from repetitiveness.\n",
    "3. Top-k sampling would sample the next word from the top-k most probable words, increasing diversity in the generated text.\n",
    "\n",
    "These strategies help LLMs generate meaningful and diverse text for various language generation tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1baa0b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8c0232d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Greedy Search\n",
    "\n",
    "```{image} ../figs/aiart/entelecheia_greedy_search.png\n",
    ":alt: Greedy Search\n",
    ":class: bg-primary mb-1\n",
    ":width: 50%\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Greedy search is a decoding strategy used in language generation tasks. It works by selecting the word with the highest probability as the next word in the generated sequence, given the previous words.\n",
    "\n",
    "In each step of the generation process, the model computes the probabilities of all possible words, given the context of the previously generated words. Greedy search then chooses the word with the highest probability and appends it to the output sequence. This process is repeated until a predefined stopping condition is met, such as reaching the maximum output length or generating an end-of-sentence (EOS) token.\n",
    "\n",
    "While greedy search is computationally efficient and straightforward to implement, it has some drawbacks. The main limitation is that it can generate suboptimal output sequences since it doesn't explore other possible word combinations. It always chooses the locally optimal word without considering the global context, which might lead to less coherent or less diverse generated text. Other search strategies, like beam search or nucleus sampling, can help overcome these limitations by exploring a larger space of possible output sequences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "988a31c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "```{image} ../figs/deep_nlp/zero/deepnlp_2_greedy_search.png\n",
    ":alt: Greedy Search Algorithm\n",
    ":class: bg-primary mb-1\n",
    ":width: 70%\n",
    ":align: center\n",
    "```\n",
    "\n",
    "The next word is chosen using the formula $w_t = \\operatorname{argmax}_{w}P(w | w_{1:t-1})$ at each timestep $t$, where $w_t$ is the next word and $w_{1:t-1}$ are the previous words in the sequence.\n",
    "\n",
    "For example, starting with the word \"The\", the algorithm evaluates the probabilities of all possible next words and greedily selects the one with the highest probability, such as \"nice\". The process is repeated to generate the subsequent words in the sequence. In this case, the final generated word sequence is (\"The\", \"nice\", \"woman\"). The overall probability of this sequence is calculated by multiplying the probabilities of each chosen word, which is $0.5 \\times 0.4 = 0.2$ in this example.\n",
    "\n",
    "While greedy search is computationally efficient and easy to implement, it has some limitations. The algorithm always chooses the locally optimal word without considering the global context, which might lead to less coherent or less diverse generated text. Other search strategies, like beam search or nucleus sampling, can help overcome these limitations by exploring a larger space of possible output sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0dabdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, but I'm not sure how to apply it to real-world applications.\n",
      "\n",
      "I'm not sure how to apply it to real-world applications. I'm not sure how to apply it to real-world applications. I'm not sure how to apply it to real-world applications. I'm not sure how to apply it to real-world applications. I'm not sure how to apply it to real-world applications. I'm not\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(\n",
    "    \"I enjoy studying deep learning for natural language processing\",\n",
    "    return_tensors=\"tf\",\n",
    ")\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=100)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * \"-\")\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40a03e74",
   "metadata": {},
   "source": [
    "-   Generating word sequences with GPT-2: To generate word sequences using GPT-2, you provide a context, such as (\"I\", \"enjoy\", \"studying\", \"deep\", \"learning\", \"for\", \"natural\", \"language\", \"processing\"), and the model predicts the most likely words to follow the given context.\n",
    "    \n",
    "-   Repetitive output: A common issue in language generation, especially when using greedy or beam search, is that the model often starts repeating itself. This occurs because these search strategies tend to get stuck in a loop of selecting locally optimal words without considering the broader context.\n",
    "    \n",
    "-   Drawbacks of greedy search:\n",
    "    \n",
    "    -   Misses high probability words: Greedy search can miss high probability words that are hidden behind a low probability word. Since the algorithm always chooses the word with the highest probability at each step, it doesn't explore other word combinations that could lead to better overall sequences.\n",
    "    -   Lack of diversity: Greedy search may generate less diverse and less coherent text, as it only focuses on the locally optimal choice. Other search strategies, like beam search or nucleus sampling, can help mitigate these issues by exploring a larger space of possible output sequences and considering global context."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91f06559",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Beam search\n",
    "\n",
    "```{image} ../figs/aiart/entelecheia_beam_search.png\n",
    ":alt: Beam Search\n",
    ":class: bg-primary mb-1\n",
    ":width: 50%\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Beam search is a decoding strategy used in language generation tasks to generate more diverse and coherent text compared to greedy search. It is a type of breadth-first search algorithm that maintains a fixed number of candidate sequences, called \"beams,\" at each step of the generation process.\n",
    "\n",
    "The main idea behind beam search is to explore multiple word options at each step, rather than choosing only the word with the highest probability, as in greedy search. The algorithm starts by selecting the top k words with the highest probabilities, where k is the beam size. It then extends each of these words with the top k words, given the context. This results in k * k new candidate sequences. The algorithm keeps only the top k sequences with the highest overall probabilities and discards the rest.\n",
    "\n",
    "The process is repeated at each timestep until a predefined stopping condition is met, such as reaching the maximum output length or generating an end-of-sentence (EOS) token. The candidate sequence with the highest overall probability is selected as the final output.\n",
    "\n",
    "Beam search offers a balance between computational complexity and the quality of generated text. A larger beam size increases the chances of finding better output sequences but also increases the computational cost. On the other hand, a smaller beam size is more computationally efficient but may generate less diverse and less coherent text.\n",
    "\n",
    "In summary, beam search is a decoding strategy that explores multiple word combinations during text generation, which can lead to more diverse and coherent output compared to greedy search. However, it comes at the cost of increased computational complexity, depending on the beam size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2a98286",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "```{image} ../figs/deep_nlp/zero/deepnlp_2_greedy_search.png\n",
    ":alt: Beam Search Algorithm\n",
    ":class: bg-primary mb-1\n",
    ":width: 60%\n",
    ":align: center\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff80c158-0eea-4f5b-99bf-21df2905efb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Beam search with `num_beams=2`: Beam search is a decoding strategy that considers multiple hypotheses during text generation. In this example, we set the beam size to 2, meaning the algorithm will keep track of the two most likely word sequences at each timestep.\n",
    "- At timestep 1: Instead of only considering the most likely hypothesis (\"The\", \"nice\"), as in greedy search, beam search also maintains the second most likely hypothesis (\"The\", \"dog\").\n",
    "- At timestep 2: Beam search evaluates the probabilities of extending both hypotheses with the top two words. It finds that the word sequence (\"The\", \"dog\", \"has\"), with a probability of 0.36, is more likely than (\"The\", \"nice\", \"woman\"), which has a probability of 0.2.\n",
    "- Optimal solution found: In this toy example, beam search successfully discovers the most likely word sequence, which was missed by the greedy search.\n",
    "- Comparison to greedy search: Beam search generally finds output sequences with higher probabilities than greedy search. However, it's not guaranteed to always find the most likely output, especially for large search spaces or small beam sizes. The quality of the generated text depends on the beam size, with larger beam sizes typically producing better results at the cost of increased computational complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163955d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Set `num_beams > 1` and `early_stopping=True` so that generation is finished when all beam hypotheses reached the EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4be3c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, and I'm excited to see how it can be applied to real-world applications.\n",
      "\n",
      "What is Deep Learning?\n",
      "\n",
      "Deep learning is a type of artificial intelligence (AI) that can be applied to real-world applications. Deep learning is a type of artificial intelligence (AI) that can be applied to real-world applications.\n",
      "\n",
      "Deep learning is a type of artificial intelligence (AI) that can be applied to real-world applications\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7f218",
   "metadata": {},
   "source": [
    "- While the result is arguably more fluent, the output still includes repetitions of the same word sequences.\n",
    "- A simple remedy is to introduce *n-grams* penalties as introduced by Paulus et al. (2017) and Klein et al. (2017). \n",
    "- The most common n-grams penalty makes sure that no *n-gram* appears twice by manually setting the probability of next words that could create an already seen *n-gram* to 0.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5693530b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Setting `no_repeat_ngram_size=2` to prevent 2-gram repetitions: By including this parameter in the generate function, we ensure that no 2-gram appears twice in the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3896798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, and I'm excited to see how it can be applied to real-world applications.\n",
      "\n",
      "In this post, I'll show you how you can use Deep Learning to build a neural network that can learn to read and write a sentence. In this article, you'll learn how to use the Deep Neural Network (DNN) to learn a language. You'll also learn about how the DNN works and what you need to do to get started\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d54cfb2",
   "metadata": {},
   "source": [
    "- Improved output: The generated text no longer contains repeated 2-grams, which results in a more coherent output. However, it's important to use n-gram penalties with caution, as they can prevent the repetition of important phrases. For example, a text about \"New York\" should not have a 2-gram penalty, as it would limit the number of times the city's name appears.\n",
    "- Comparing and selecting the best beam: To compare the top beams after generation and choose the best one, set the `num_return_sequences` parameter to the desired number of highest-scoring beams to return. Ensure that `num_return_sequences <= num_beams`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2db782d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy studying deep learning for natural language processing, and I'm excited to see how it can be applied to real-world applications.\n",
      "\n",
      "In this post, I'll show you how you can use Deep Learning to build a neural network that can learn to read and write a sentence. In this article, you'll learn how to use the Deep Neural Network (DNN) to learn a language. You'll also learn about how the DNN works and what you need to do to get started\n",
      "1: I enjoy studying deep learning for natural language processing, and I'm excited to see how it can be applied to real-world applications.\n",
      "\n",
      "In this post, I'll show you how you can use Deep Learning to build a neural network that can learn to read and write a sentence. In this article, you'll learn how to use the Deep Neural Network (DNN) to learn a language. You'll also learn about how the DNN works and what you need to know about it to\n",
      "2: I enjoy studying deep learning for natural language processing, and I'm excited to see how it can be applied to real-world applications.\n",
      "\n",
      "In this post, I'll show you how you can use Deep Learning to build a neural network that can learn to read and write a sentence. In this article, you'll learn how to use the Deep Neural Network (DNN) to learn a language. You'll also learn about how the DNN works and what you need to know to get started\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=3,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "    print(\"{}: {}\".format(\n",
    "        i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8805dd0e",
   "metadata": {},
   "source": [
    "- Analyzing the results: The output shows three different generated sequences, each being a top-scoring beam. The differences between these beams might be marginal, especially when using a small number of beams (e.g., 5). By examining these alternatives, you can select the one that best fits your requirements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7d59d1d",
   "metadata": {},
   "source": [
    "In open-ended text generation tasks, there are several reasons why beam search might not be the most suitable option:\n",
    "\n",
    "-   Predictable length vs. varying length: Beam search tends to perform well in tasks where the desired length of the generated output is more or less predictable, such as machine translation or summarization. However, in open-ended generation tasks like dialog and story generation, the desired output length can vary significantly, making beam search less optimal.\n",
    "    \n",
    "-   Repetitive generation: Beam search often results in repetitive text generation. This issue can be particularly challenging to control in tasks like story generation, where applying n-gram or other penalties to prevent repetition may require extensive fine-tuning to strike the right balance between avoiding repetitive phrases and forcing unnatural \"no-repetition\" constraints.\n",
    "    \n",
    "-   Human-like language generation: High-quality human language typically contains a mix of both predictable and surprising elements. In other words, as humans, we appreciate generated text that is not too predictable or boring. According to a study by [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751), beam search tends to favor high-probability words, which may lead to less engaging and less human-like text generation.\n",
    "    \n",
    "\n",
    "In summary, while beam search can be effective for certain text generation tasks, its limitations in handling varying output lengths, repetitiveness, and the need for more surprising and engaging text make it less suitable for open-ended generation tasks like dialog and story generation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "463b4eeb-9143-4677-8440-3b8a7f8a711b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "```{image} ../figs/deep_nlp/zero/deepnlp_2_beam_vs_human.png\n",
    ":alt: Beam Search vs Human\n",
    ":class: bg-primary mb-1\n",
    ":width: 60%\n",
    ":align: center\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b450fa1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Sampling\n",
    "\n",
    "Sampling means randomly picking the next word $w_t$ according to its conditional probability distribution:\n",
    "\n",
    "$$ w_t \\sim P(w|w_{1:t-1}) $$\n",
    "\n",
    "The following graphic visualizes language generation when sampling.\n",
    "\n",
    "![sampling](../figs/deep_nlp/zero/deepnlp_2_sampling_search.png)\n",
    "\n",
    "Language generation using sampling is not *deterministic* anymore. \n",
    "The word (\"car\") is sampled from the conditioned probability distribution $P(w | \\text{\"The\"})$, followed by sampling (\"drives\") from $P(w | \\text{\"The\"}, \\text{\"car\"})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd0ac2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Set `do_sample=True` and deactivate *Top-K* sampling via `top_k=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ef99e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, especially to see how it does useful things over evolutionary time once you shift the ideology of theoretical language. I also really like developing algorithmically tailored introductory programming books to teach my students, but I simply don't have the spare time to do all that I did before. HALAX fight3r also released their original issue warning the public that Gaming with Game Software may be detrimental to your solidified web skills and appearances, likely to undermine your beliefs and help\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8370c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- The text seems alright - but when taking a closer look, it is not very coherent. \n",
    "- Some words don't sound like they were written by a human. \n",
    "- That is the big problem when sampling word sequences: The models often generate incoherent gibberish.\n",
    "\n",
    "- A trick is to make the distribution $P(w|w_{1:t-1})$ sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the so-called `temperature` of the [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max).\n",
    "\n",
    "![temperature](../figs/deep_nlp/zero/deepnlp_2_sampling_search_with_temp.png)\n",
    "\n",
    "- The conditional next word distribution of step t=1 becomes much sharper leaving almost no chance for word (\"car\") to be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f82721",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Cool down the distribution in the library by setting `temperature=0.7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca49dff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, but this is a simplistic example of how the language can be applied to a complex problem.\n",
      "\n",
      "Using more complex architectures\n",
      "\n",
      "The most common approach is to look at a whole set of ML designs that are quite complex and require a lot of work to deploy. For example, the first ML project I'm learning is the human language learning project. Let's take a look at the human-language learning project.\n",
      "\n",
      "The current human-language\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=0,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3fa1d1",
   "metadata": {},
   "source": [
    "- There are less weird n-grams and the output is a bit more coherent now. \n",
    "- While applying temperature can make a distribution less random, in its limit, when setting `temperature` $\\to 0$, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8db85c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Top-K Sampling\n",
    "\n",
    "\n",
    "![top_k](../figs/deep_nlp/zero/deepnlp_2_top_k_sampling.png)\n",
    "\n",
    "In *Top-K* sampling, the *K* most likely next words are filtered and the probability mass is redistributed among only those *K* next words. GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e94719",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Having set $K = 6$, in both sampling steps we limit our sampling pool to 6 words. \n",
    "- While the 6 most likely words, defined as $V_{\\text{top-K}}$ encompass only two-thirds of the whole\n",
    "probability mass in the first step, it includes almost all of the probability mass in the second step. \n",
    "- Nevertheless, we see that it successfully eliminates the rather weird candidates (\"not\", \"the\", \"small\", \"told\") in the second sampling step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70658b2-fb8a-4ab1-8a97-81e3d4f58364",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's see how *Top-K* can be used in the library by setting `top_k=50`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e639a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, as well as in terms of learning from experience. As well as having my students who are learning an intermediate language and learn how to use it in a meaningful way or with a minimal effort, I can also focus on learning programming languages like C++ (though I wouldn't go so far as to call that \"advanced\"), Python (for those that aren't familiar with Python) or Java (I think I actually want to see what I can learn\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bea5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- The text is arguably the most *human-sounding* text so far. \n",
    "- One concern with *Top-K* sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution $P(w|w_{1:t-1})$. \n",
    "- This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above).\n",
    "\n",
    "\n",
    "- In step $t=1$, Top-K eliminates the possibility to sample (\"people\",\"big\",\"house\",\"cat\"), which seem like reasonable candidates. \n",
    "- On the other hand, in step $t=2$ the method includes the arguably ill-fitted words (\"down\",\"a\") in the sample pool of words. \n",
    "- Thus, limiting the sample pool to a fixed size $K$ could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution. \n",
    "- This intuition led Ari Holtzman et al. (2019) to create ***Top-p***- or ***nucleus***-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd5b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Top-p (nucleus) sampling\n",
    "\n",
    "![top_p](../figs/deep_nlp/zero/deepnlp_2_top_p_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3dae2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Instead of sampling only from the most likely *K* words, in *Top-p* sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability *p*. \n",
    "- The probability mass is then redistributed among this set of words. \n",
    "- This way, the size of the set of words (*a.k.a* the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. \n",
    "\n",
    "\n",
    "- Having set $p=0.92$, *Top-p* sampling picks the *minimum* number of words to exceed together $p=92$ of the probability mass, defined as $V_{\\text{top-p}}$. \n",
    "- In the first example, this included the 9 most likely words, whereas it only has to pick the top 3 words in the second example to exceed 92%. \n",
    "- It can be seen that it keeps a wide range of words where the next word is arguably less predictable, *e.g.* $P(w | \\text{\"The''})$, and only a few words when the next word seems more predictable, *e.g.* $P(w | \\text{\"The\"}, \\text{\"car\"})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549d760-3940-4349-822b-88874f096e4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Activate *Top-p* sampling by setting `0 < top_p < 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3b41fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing. Learn more at lecture.lightlink.com.\n",
      "\n",
      "Ciao â€” Social conscious \"synchronization\" between conscious and nonconscious participants at the Cognition Experience. Learn more at lecture.lightlink.com.\n",
      "\n",
      "Everett's Processio\n",
      "\n",
      "A video discussion\n",
      "\n",
      "The Evolution of Mindfulness in Today's Economy\n",
      "\n",
      "The Recent Advances in the Study of Mindfulness and Success in Society. The results from St Louis University\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_p=0.92,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e5ff5",
   "metadata": {},
   "source": [
    "Great, that sounds like it could have been written by a human. Well, maybe not quite yet.\n",
    "\n",
    "While in theory, *Top-p* seems more elegant than *Top-K*, both methods work well in practice. \n",
    "*Top-p* can also be used in combination with *Top-K*, which can avoid very low ranked words while allowing for some\n",
    "dynamic selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9bf73-78a9-44ee-9859-d45ba1977d55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Finally, to get multiple independently sampled outputs, we can *again* set the parameter `num_return_sequences > 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aafe1335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy studying deep learning for natural language processing, as I've learned it quickly enough for me to understand it. The idea of modeling learning comes from both of these worlds. The first is that of natural language processing and the second is a mathematical theory that allows you to draw meaningful conclusions. To understand what is in the world of Natural Language Processing, go to the book by Mike Biederman of the University of Toronto or check out his website at www.layers.com. You are also\n",
      "1: I enjoy studying deep learning for natural language processing. My favourite part, even though it is just about my main job, is learning to play a game. That is exactly what I am doing on my website. All this is part of my job as a developer and I love playing games.\n",
      "\n",
      "My job with this company was to create a website where my students could explore the world as they would enjoy learning about computer science, physics, or any other science.\n",
      "\n",
      "In doing so, I\n",
      "2: I enjoy studying deep learning for natural language processing. In my experience it doesn't help my training to teach my students how to do it, but it's an awesome source for training and has led to many exciting discoveries about Deep Learning and neural nets in general.\n",
      "\n",
      "Learning to code\n",
      "\n",
      "Another way to learn Deep Learning is by coding. For example, I often code to solve problem A and then use it for the solving of problem B. This is where the language really comes in. I\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\".format(\n",
    "        i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b7002",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Summary of decoding / search strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb9fb0b",
   "metadata": {},
   "source": [
    "As *ad-hoc* decoding methods, *top-p* and *top-K* sampling seem to produce more fluent text than traditional *greedy* - and *beam* search on open-ended language generation. Recently, there has been more evidence though that the apparent flaws of *greedy* and *beam* search - mainly generating repetitive word sequences - are caused by the model (especially the way the model is trained), rather than the decoding method, *cf.* [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). Also, as demonstrated in [Welleck et al. (2020)](https://arxiv.org/abs/2002.02492), it looks as *top-K* and *top-p* sampling also suffer from generating repetitive word sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010d4e7-3d9c-4980-80d3-95bea9cbba60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Greedy Search \n",
    "  - simply chooses the next word at each timestep t+1 that has the highest predicted probability of following the word at t. \n",
    "  - One of the main issues here is that greedy search will miss words with a high probability at t+1 if it is preceded by a word with a low probability at t.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb28a24-ea98-4a1d-a70a-f445648e3d08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Beam Search \n",
    "  - keeps track of the n-th (num_beams) most likely word sequences and outputs the most likely sequence. \n",
    "  - Sounds great, but this method breaks down when the output length can be highly variable â€” as in the case of open-ended text generation. \n",
    "  - Both greedy and beam search also produce outputs whose distribution does not align very well with the way humans might perform the same task (i.e. both are liable to produce fairly repetitive, boring text).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b20ed6-1e2e-4eae-abaa-47da7af1d4d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Sampling With Top-k + Top-p\n",
    "  - a combination of three methods. \n",
    "  - By sampling, we mean that the next word is chosen randomly based on its conditional probability distribution (von Platen, 2020). \n",
    "  - In Top-k, we choose the k most likely words, and then redistribute the probability mass amongst them before the next draw. \n",
    "  - Top-p adds an additional constraint to top-k, in that weâ€™re choosing from the smallest set of words whose cumulative probability exceed p.\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a12f491",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Prompt Engineering: The Career of Future\n",
    "\n",
    "```{image} ../figs/deep_nlp/zero/deepnlp_2_prompt.png\n",
    ":alt: Prompt Engineering\n",
    ":class: bg-primary mb-1\n",
    ":width: 50%\n",
    ":align: center\n",
    "```\n",
    "\n",
    "(source: https://twitter.com/karpathy/status/1273788774422441984/photo/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a132fd0-c774-49f6-b671-43f8187bc4d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "> With the No-Code revolution around the corner, and the coming of new-age technologies like GPT-3 we may see a stark difference between the career of today and the careers of tomorrowâ€¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4439c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "As a rule of thumb while designing the training prompt you should aim towards getting a zero-shot response from the model, if that isnâ€™t possible move forward with few examples rather than providing it with an entire corpus. The standard flow for training prompt design should look like: Zero-Shot â†’ Few Shots â†’ Corpus-based Priming.\n",
    "\n",
    "- Step 1: Define the problem you are trying to solve and bucket it into one of the possible natural language tasks classification, Q & A, text generation, creative writing, etc.\n",
    "- Step 2: Ask yourself if there is a way to get a solution with zero-shot (i.e. without priming the GPT-3 model with any external training examples)\n",
    "- Step 3: If you think that you need external examples to prime the model for your use case, go back to step-2 and think really hard.\n",
    "- Step 4: Now think of how you might encounter the problem in a textual fashion given the â€œtext-in, text-outâ€ interface of GPT-3. Think about all the possible scenarios to represent your problem in textual form.\n",
    "- Step 5: If you end up using the external examples, use as few as possible and try to include variety in your examples without essentially overfitting the model or skewing the predictions."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f869af7787e6a1c49e09e367fc6e1b81d93d1c6583b43249c80edc047bd13cb2"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
